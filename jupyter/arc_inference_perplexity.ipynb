{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85995f60-1839-4667-ac9e-d8c7d5ee3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig, Gemma3ForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19bf1bc0-cc18-4e59-9677-9ac0acdf582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "530c6411-4583-4cf8-94a0-3791fe7711b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(\n",
    "    model_id=\"google/gemma-3-1b-it\",\n",
    "    dtype=torch.float16,\n",
    "    load_in_8bit=True,\n",
    "):\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=load_in_8bit)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = Gemma3ForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=dtype,\n",
    "        quantization_config=quantization_config if load_in_8bit else None,\n",
    "    )\n",
    "\n",
    "    # <end_of_turn> 토큰을 eos_token으로 등록 (토크나이저에 없으면 추가)\n",
    "    if '<end_of_turn>' not in tokenizer.get_vocab():\n",
    "        tokenizer.add_special_tokens({'eos_token': '<end_of_turn>'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    else:\n",
    "        # 이미 있으면 eos_token으로 지정만 해줌\n",
    "        tokenizer.eos_token = '<end_of_turn>'\n",
    "\n",
    "    model.eval()\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13c0c08d-9b7e-49eb-9361-ddd3911270f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_model()\n",
    "model.tokenizer = tokenizer  # decode에 사용하기 위해 tokenizer 주입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "837da4c7-d664-40a9-b3e7-0b6978c87139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_ansi(r, g, b):\n",
    "    return f\"\\033[38;2;{r};{g};{b}m\"\n",
    "\n",
    "def bg_rgb_ansi(r, g, b):\n",
    "    return f\"\\033[48;2;{r};{g};{b}m\"  # 배경색\n",
    "\n",
    "def color_by_prob(text, prob):\n",
    "    \"\"\"\n",
    "    확률(prob) 값에 따라 색상을 입힌 문자열 반환\n",
    "    높은 확률일수록 초록색, 낮을수록 빨간색 계열\n",
    "    \"\"\"\n",
    "    if prob >= 0.5:\n",
    "        color = \"\\033[92m\"  # 밝은 초록 (high prob)\n",
    "    elif prob >= 0.2:\n",
    "        color = \"\\033[93m\"  # 밝은 노랑 (medium prob)\n",
    "    else:\n",
    "        color = \"\\033[91m\"  # 밝은 빨강 (low prob)\n",
    "\n",
    "    reset = \"\\033[0m\"\n",
    "    return f\"{color}{text}{reset}\"\n",
    "\n",
    "def get_yellow_to_green_ansi(prob):\n",
    "    \"\"\"\n",
    "    확률에 따라 진한 노랑(255,255,0) → 초록(0,255,0) 색상 매핑\n",
    "    \"\"\"\n",
    "    steps = 10\n",
    "    index = min(int(prob * steps), steps - 1)\n",
    "\n",
    "    r_start, g, b = 255, 255, 0  # 시작 색 (노랑)\n",
    "    r_end = 0                   # 끝 색 (초록)\n",
    "    \n",
    "    # R을 선형적으로 감소\n",
    "    r = int(r_start - (r_start - r_end) * (index / (steps - 1)))\n",
    "    \n",
    "    return rgb_ansi(r, g, b)\n",
    "\n",
    "def get_yellow_to_green_bg_ansi(prob):\n",
    "    steps = 10\n",
    "    index = min(int(prob * steps), steps - 1)\n",
    "    r_start, g, b = 255, 255, 0\n",
    "    r = int(r_start - (r_start * index / (steps - 1)))\n",
    "    return bg_rgb_ansi(r, g, b)\n",
    "\n",
    "def get_yellow_to_green_bg_ansi(prob):\n",
    "    steps = 10\n",
    "    index = min(int(prob * steps), steps - 1)\n",
    "    r_start, g, b = 255, 255, 0\n",
    "    r = int(r_start - (r_start * index / (steps - 1)))\n",
    "    return bg_rgb_ansi(r, g, b)\n",
    "\n",
    "def get_yellow_to_brightblue_bg_ansi(prob):\n",
    "    \"\"\"\n",
    "    prob: 0.0(노랑) → 1.0(밝은 파랑) 배경색 10단계 반환\n",
    "    밝은 파랑: #6699FF (R=102,G=153,B=255)\n",
    "    노랑: #FFFF00 (R=255,G=255,B=0)\n",
    "    \"\"\"\n",
    "    steps = 10\n",
    "    index = min(int(prob * steps), steps - 1)\n",
    "    \n",
    "    r_start, g_start, b_start = 255, 255, 0       # 노랑\n",
    "    r_end, g_end, b_end = 102, 153, 255           # 밝은 파랑\n",
    "    \n",
    "    r = int(r_start + (r_end - r_start) * index / (steps - 1))\n",
    "    g = int(g_start + (g_end - g_start) * index / (steps - 1))\n",
    "    b = int(b_start + (b_end - b_start) * index / (steps - 1))\n",
    "    \n",
    "    return bg_rgb_ansi(r, g, b)\n",
    "\n",
    "def get_yellow_to_lightblue_bg_ansi(prob):\n",
    "    \"\"\"\n",
    "    prob: 0.0 (#E5E200 노랑) → 1.0 (#008FE6 파랑) 배경색 10단계 선형 보간\n",
    "    \"\"\"\n",
    "    steps = 10\n",
    "    index = min(int(prob * steps), steps - 1)\n",
    "\n",
    "    r_start, g_start, b_start = 229, 226, 0     # #E5E200 노랑\n",
    "    r_end, g_end, b_end = 0, 143, 230           # #008FE6 파랑\n",
    "\n",
    "    r = int(r_start + (r_end - r_start) * index / (steps - 1))\n",
    "    g = int(g_start + (g_end - g_start) * index / (steps - 1))\n",
    "    b = int(b_start + (b_end - b_start) * index / (steps - 1))\n",
    "\n",
    "    return bg_rgb_ansi(r, g, b)\n",
    "\n",
    "\n",
    "def bg_rgb_ansi(r, g, b):\n",
    "    return f\"\\033[48;2;{r};{g};{b}m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e99b176-8288-400f-b82d-f70789f36d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_tokens(top_tokens, field_width=22):\n",
    "    \"\"\"Top-5 토큰 정보를 가운데 정렬 + 배경색 노랑→초록, 글자색 기본 유지\"\"\"\n",
    "\n",
    "    formatted = []\n",
    "    for _, token_str, prob in top_tokens:\n",
    "        token_display = f\"{repr(token_str)} ({prob:.4f})\"\n",
    "        padded = f\"{token_display:^{field_width}}\"   # 먼저 순수 텍스트로 정렬\n",
    "        bg_color = get_yellow_to_lightblue_bg_ansi(prob) # 외부 정의 함수 호출\n",
    "        colored = f\"{bg_color}{padded}\\033[0m\"       # 색상 코드 감싸기\n",
    "        formatted.append(colored)\n",
    "\n",
    "    print(\"== Top-5 Tokens ==\")\n",
    "    print(\" | \".join(formatted))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50203565-f3f4-4dbc-9941-bd4679102e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token(model, input_ids):\n",
    "    \"\"\"다음 토큰과 top-5 후보 반환\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    next_token_logits = logits[0, -1, :]\n",
    "    probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "    # Top-5 토큰 추출\n",
    "    top_probs, top_ids = torch.topk(probs, k=5)\n",
    "    top_tokens = [(token_id.item(), token_str, top_probs[i].item())\n",
    "                  for i, token_id in enumerate(top_ids)\n",
    "                  for token_str in [model.tokenizer.decode(token_id)]]\n",
    "\n",
    "    return top_tokens\n",
    "\n",
    "def run_inference_loop(tokenizer, model, input_text, max_new_tokens=20):\n",
    "    \"\"\"전체 생성 루프 실행\"\"\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    generated = input_ids\n",
    "\n",
    "    print(f\"Input: {input_text}\\n\")\n",
    "\n",
    "    for step in range(max_new_tokens):\n",
    "        top_tokens = generate_next_token(model, generated)\n",
    "        print_top_tokens(top_tokens)\n",
    "\n",
    "        # 다음 토큰 선택 (top-1 기반, greedy)\n",
    "        next_token_id = torch.tensor([[top_tokens[0][0]]], device=model.device)\n",
    "        generated = torch.cat([generated, next_token_id], dim=1)\n",
    "\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    output_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    print(\"== Final Output ==\")\n",
    "    print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8727721-5b42-4eb5-ac17-ded8ee6791a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_from_logits(tokenizer, model, input_text):\n",
    "    \"\"\"logits를 사용하여 perplexity 직접 계산\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "        input_ids = inputs.input_ids.to(model.device)\n",
    "\n",
    "        # 모델 예측 logits\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        # 정답 label은 input_ids를 한 칸 오른쪽으로 이동\n",
    "        shift_logits = logits[:, :-1, :]         # [batch, seq-1, vocab]\n",
    "        shift_labels = input_ids[:, 1:]          # [batch, seq-1]\n",
    "\n",
    "        # CrossEntropy 계산: log_softmax 후 정답 토큰의 log prob 추출\n",
    "        log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "        shift_labels = shift_labels.unsqueeze(-1)  # [batch, seq-1, 1]\n",
    "\n",
    "        # gather로 정답 토큰의 log prob 추출\n",
    "        token_log_probs = log_probs.gather(dim=-1, index=shift_labels).squeeze(-1)  # [batch, seq-1]\n",
    "\n",
    "        # 평균 negative log likelihood\n",
    "        nll = -token_log_probs.mean()\n",
    "        perplexity = torch.exp(nll)\n",
    "\n",
    "        print(f\"NLL: {nll.item():.4f}\")\n",
    "        print(f\"Perplexity: {perplexity.item():.4f}\")\n",
    "        return perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4db778d-4fe2-4726-8e9d-3b902ea5f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_inference_loop(tokenizer, model, \"What is 2 + 2? Answer briefly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54674bf9-d390-44cc-ba10-b67bbfa97bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_self_confidence(tokenizer, model, input_text, max_new_tokens=20, field_width=20):\n",
    "    import math\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    generated = input_ids\n",
    "    log_probs = []\n",
    "    history = []\n",
    "\n",
    "    print(f\"Input: {input_text}\\n\")\n",
    "\n",
    "    for step in range(max_new_tokens):\n",
    "        outputs = model(generated)\n",
    "        logits = outputs.logits[:, -1, :]  # 마지막 토큰 logits\n",
    "\n",
    "        log_softmax = F.log_softmax(logits, dim=-1)\n",
    "        next_token_id = torch.argmax(log_softmax, dim=-1)\n",
    "        next_log_prob = log_softmax[0, next_token_id]\n",
    "        prob = next_log_prob.exp().item()\n",
    "\n",
    "        log_probs.append(next_log_prob.item())\n",
    "        avg_nll = -sum(log_probs) / len(log_probs)\n",
    "        avg_prob = math.exp(-avg_nll)\n",
    "\n",
    "        token_str = repr(tokenizer.decode([next_token_id.item()]))\n",
    "\n",
    "        # 배경색 적용\n",
    "        bg_color = get_yellow_to_lightblue_bg_ansi(avg_prob)\n",
    "        padded = f\"{token_str:^{field_width}}\"\n",
    "        colored_token = f\"{bg_color}{padded}\\033[0m\"\n",
    "\n",
    "        print(\n",
    "            f\"[{step:02d}] Token: {colored_token} | \"\n",
    "            f\"LogProb: {next_log_prob.item():>8.4f}  Prob: {prob:>7.4f}  | \"\n",
    "            f\"AvgNLL: {avg_nll:.4f}  PPL_Prob: {avg_prob:.4f}\"\n",
    "        )\n",
    "\n",
    "        history.append({\n",
    "            \"step\": step,\n",
    "            \"token_id\": next_token_id.item(),\n",
    "            \"token_str\": token_str,\n",
    "            \"log_prob\": next_log_prob.item(),\n",
    "            \"prob\": prob,\n",
    "            \"avg_nll\": avg_nll,\n",
    "            \"ppl_prob\": avg_prob,\n",
    "            \"colored_token\": colored_token,  # 기록도 색칠 토큰 포함\n",
    "        })\n",
    "\n",
    "        generated = torch.cat([generated, next_token_id.unsqueeze(0)], dim=1)\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    print(\"\\n== Self Perplexity ==\")\n",
    "    print(f\"Avg NLL: {avg_nll:.4f}\")\n",
    "    print(f\"Perplexity (self): {avg_prob:.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def print_generation_history(history, field_width=20):\n",
    "    print(\"\\n=== Generation History ===\")\n",
    "    for entry in history:\n",
    "        # 이미 색칠된 토큰을 기록에서 재사용 (있으면)\n",
    "        colored_token = entry.get(\"colored_token\")\n",
    "        if not colored_token:\n",
    "            # 없으면 새로 색칠 (fallback)\n",
    "            bg_color = get_yellow_to_lightblue_bg_ansi(entry['ppl_prob'])\n",
    "            padded = f\"{entry['token_str'] :^{field_width}}\"\n",
    "            colored_token = f\"{bg_color}{padded}\\033[0m\"\n",
    "\n",
    "        print(\n",
    "            f\"[{entry['step']:02d}] Token: {colored_token} | \"\n",
    "            f\"LogProb: {entry['log_prob']:8.4f}  Prob: {entry['prob']:7.4f}  | \"\n",
    "            f\"AvgNLL: {entry['avg_nll']:.4f}  PPL_Prob: {entry['ppl_prob']:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47ccaec4-bc19-4bd1-9bb3-14ee2198b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_with_self_confidence(tokenizer, model, \n",
    "#                               \"What is 4 + 4? Answer briefly.\", \n",
    "#                               max_new_tokens=50,\n",
    "#                               field_width=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d283f86-a8bd-4fb0-ae09-a74336e2c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = generate_with_self_confidence(tokenizer, model, \n",
    "#                                         \"What is 2 + 2?\", \n",
    "#                                         max_new_tokens=20,\n",
    "#                                         field_width=15)\n",
    "# print_generation_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "abce5f9c-d9e3-4ca1-9f05-cdf4bd6b5d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = \"../arc-prize-2025/arc-agi_training_challenges.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e1e9edd-46a3-4012-a5f5-6589bfaa89e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_file, \"r\") as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dce7634f-8eb8-444c-a764-379270e2d555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00576224 {'train': [{'input': [[7, 9], [4, 3]], 'output': [[7, 9, 7, 9, 7, 9], [4, 3, 4, 3, 4, 3], [9, 7, 9, 7, 9, 7], [3, 4, 3, 4, 3, 4], [7, 9, 7, 9, 7, 9], [4, 3, 4, 3, 4, 3]]}, {'input': [[8, 6], [6, 4]], 'output': [[8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4], [6, 8, 6, 8, 6, 8], [4, 6, 4, 6, 4, 6], [8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4]]}], 'test': [{'input': [[3, 2], [7, 8]]}]}\n"
     ]
    }
   ],
   "source": [
    "for key in json_data:\n",
    "    print(key, json_data[key])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f0b76f1-235e-4adc-afe5-969db0ab56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_from_example(key: str, example: dict) -> str:\n",
    "    prompt = f\"The following examples show 2D input arrays and their corresponding output arrays. Each output is generated by applying a specific pattern or rule to the input.\\n\\n\"\n",
    "    prompt += \"- Please study the relationship between each `input` and its corresponding `output` in the `train` examples.\\n\"\n",
    "    prompt += \"- Then, based on these patterns, predict the `output` for the given `test` input.\\n\\n\"\n",
    "\n",
    "    for i, sample in enumerate(example.get(\"train\", []), 1):\n",
    "        prompt += f\"### Example {i}\\nInput:\\n{sample['input']}\\n\\nOutput:\\n{sample['output']}\\n\\n\"\n",
    "\n",
    "    for i, test_case in enumerate(example.get(\"test\", []), 1):\n",
    "        prompt += f\"---\\n\\n### Test Input {i}:\\n{test_case['input']}\\n\\n**Task:**\\nBased on the `train` examples above, generate the expected `output` array for this test input.\\n\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97ce74b9-b276-465f-9b0e-4de4e9dd9915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Prompt for key 833966f4 (length=675) ===\n",
      "\n",
      "The following examples show 2D input arrays and their corresponding output arrays. Each output is generated by applying a specific pattern or rule to the input.\n",
      "\n",
      "- Please study the relationship between each `input` and its corresponding `output` in the `train` examples.\n",
      "- Then, based on these patterns, predict the `output` for the given `test` input.\n",
      "\n",
      "### Example 1\n",
      "Input:\n",
      "[[9], [0], [1], [6], [8]]\n",
      "\n",
      "Output:\n",
      "[[0], [9], [1], [8], [6]]\n",
      "\n",
      "### Example 2\n",
      "Input:\n",
      "[[4], [3], [6], [2], [8]]\n",
      "\n",
      "Output:\n",
      "[[3], [4], [6], [8], [2]]\n",
      "\n",
      "---\n",
      "\n",
      "### Test Input 1:\n",
      "[[4], [5], [6], [7], [2]]\n",
      "\n",
      "**Task:**\n",
      "Based on the `train` examples above, generate the expected `output` array for this test input.\n",
      "\n",
      "\n",
      "=== Prompt for key 6150a2bd (length=715) ===\n",
      "\n",
      "The following examples show 2D input arrays and their corresponding output arrays. Each output is generated by applying a specific pattern or rule to the input.\n",
      "\n",
      "- Please study the relationship between each `input` and its corresponding `output` in the `train` examples.\n",
      "- Then, based on these patterns, predict the `output` for the given `test` input.\n",
      "\n",
      "### Example 1\n",
      "Input:\n",
      "[[5, 5, 2], [1, 0, 0], [0, 0, 0]]\n",
      "\n",
      "Output:\n",
      "[[0, 0, 0], [0, 0, 1], [2, 5, 5]]\n",
      "\n",
      "### Example 2\n",
      "Input:\n",
      "[[3, 3, 8], [3, 7, 0], [5, 0, 0]]\n",
      "\n",
      "Output:\n",
      "[[0, 0, 5], [0, 7, 3], [8, 3, 3]]\n",
      "\n",
      "---\n",
      "\n",
      "### Test Input 1:\n",
      "[[6, 3, 5], [6, 8, 0], [4, 0, 0]]\n",
      "\n",
      "**Task:**\n",
      "Based on the `train` examples above, generate the expected `output` array for this test input.\n",
      "\n",
      "\n",
      "=== Prompt for key e9afcf9a (length=750) ===\n",
      "\n",
      "The following examples show 2D input arrays and their corresponding output arrays. Each output is generated by applying a specific pattern or rule to the input.\n",
      "\n",
      "- Please study the relationship between each `input` and its corresponding `output` in the `train` examples.\n",
      "- Then, based on these patterns, predict the `output` for the given `test` input.\n",
      "\n",
      "### Example 1\n",
      "Input:\n",
      "[[4, 4, 4, 4, 4, 4], [8, 8, 8, 8, 8, 8]]\n",
      "\n",
      "Output:\n",
      "[[4, 8, 4, 8, 4, 8], [8, 4, 8, 4, 8, 4]]\n",
      "\n",
      "### Example 2\n",
      "Input:\n",
      "[[3, 3, 3, 3, 3, 3], [9, 9, 9, 9, 9, 9]]\n",
      "\n",
      "Output:\n",
      "[[3, 9, 3, 9, 3, 9], [9, 3, 9, 3, 9, 3]]\n",
      "\n",
      "---\n",
      "\n",
      "### Test Input 1:\n",
      "[[6, 6, 6, 6, 6, 6], [2, 2, 2, 2, 2, 2]]\n",
      "\n",
      "**Task:**\n",
      "Based on the `train` examples above, generate the expected `output` array for this test input.\n",
      "\n",
      "\n",
      "=== Prompt for key 5582e5ca (length=814) ===\n",
      "\n",
      "The following examples show 2D input arrays and their corresponding output arrays. Each output is generated by applying a specific pattern or rule to the input.\n",
      "\n",
      "- Please study the relationship between each `input` and its corresponding `output` in the `train` examples.\n",
      "- Then, based on these patterns, predict the `output` for the given `test` input.\n",
      "\n",
      "### Example 1\n",
      "Input:\n",
      "[[6, 8, 9], [1, 8, 1], [9, 4, 9]]\n",
      "\n",
      "Output:\n",
      "[[9, 9, 9], [9, 9, 9], [9, 9, 9]]\n",
      "\n",
      "### Example 2\n",
      "Input:\n",
      "[[4, 4, 8], [6, 4, 3], [6, 3, 0]]\n",
      "\n",
      "Output:\n",
      "[[4, 4, 4], [4, 4, 4], [4, 4, 4]]\n",
      "\n",
      "### Example 3\n",
      "Input:\n",
      "[[4, 6, 9], [6, 4, 1], [8, 8, 6]]\n",
      "\n",
      "Output:\n",
      "[[6, 6, 6], [6, 6, 6], [6, 6, 6]]\n",
      "\n",
      "---\n",
      "\n",
      "### Test Input 1:\n",
      "[[8, 8, 6], [4, 6, 9], [8, 3, 0]]\n",
      "\n",
      "**Task:**\n",
      "Based on the `train` examples above, generate the expected `output` array for this test input.\n",
      "\n",
      "\n",
      "=== Prompt for key d037b0a7 (length=814) ===\n",
      "\n",
      "The following examples show 2D input arrays and their corresponding output arrays. Each output is generated by applying a specific pattern or rule to the input.\n",
      "\n",
      "- Please study the relationship between each `input` and its corresponding `output` in the `train` examples.\n",
      "- Then, based on these patterns, predict the `output` for the given `test` input.\n",
      "\n",
      "### Example 1\n",
      "Input:\n",
      "[[0, 2, 0], [7, 0, 8], [0, 0, 0]]\n",
      "\n",
      "Output:\n",
      "[[0, 2, 0], [7, 2, 8], [7, 2, 8]]\n",
      "\n",
      "### Example 2\n",
      "Input:\n",
      "[[0, 0, 6], [0, 4, 0], [3, 0, 0]]\n",
      "\n",
      "Output:\n",
      "[[0, 0, 6], [0, 4, 6], [3, 4, 6]]\n",
      "\n",
      "### Example 3\n",
      "Input:\n",
      "[[4, 0, 0], [0, 2, 0], [0, 0, 0]]\n",
      "\n",
      "Output:\n",
      "[[4, 0, 0], [4, 2, 0], [4, 2, 0]]\n",
      "\n",
      "---\n",
      "\n",
      "### Test Input 1:\n",
      "[[4, 0, 8], [0, 0, 0], [0, 7, 0]]\n",
      "\n",
      "**Task:**\n",
      "Based on the `train` examples above, generate the expected `output` array for this test input.\n",
      "\n",
      "\n",
      "=== Prompt for key d631b094 (length=819) ===\n",
      "\n",
      "The following examples show 2D input arrays and their corresponding output arrays. Each output is generated by applying a specific pattern or rule to the input.\n",
      "\n",
      "- Please study the relationship between each `input` and its corresponding `output` in the `train` examples.\n",
      "- Then, based on these patterns, predict the `output` for the given `test` input.\n",
      "\n",
      "### Example 1\n",
      "Input:\n",
      "[[0, 7, 0], [0, 0, 0], [0, 0, 0]]\n",
      "\n",
      "Output:\n",
      "[[7]]\n",
      "\n",
      "### Example 2\n",
      "Input:\n",
      "[[0, 2, 0], [2, 0, 0], [0, 2, 0]]\n",
      "\n",
      "Output:\n",
      "[[2, 2, 2]]\n",
      "\n",
      "### Example 3\n",
      "Input:\n",
      "[[0, 8, 0], [8, 8, 0], [8, 0, 0]]\n",
      "\n",
      "Output:\n",
      "[[8, 8, 8, 8]]\n",
      "\n",
      "### Example 4\n",
      "Input:\n",
      "[[0, 0, 0], [1, 0, 0], [0, 1, 0]]\n",
      "\n",
      "Output:\n",
      "[[1, 1]]\n",
      "\n",
      "---\n",
      "\n",
      "### Test Input 1:\n",
      "[[4, 4, 0], [4, 0, 4], [0, 0, 4]]\n",
      "\n",
      "**Task:**\n",
      "Based on the `train` examples above, generate the expected `output` array for this test input.\n",
      "\n",
      "\n",
      "=== Prompt for key 66e6c45b (length=830) ===\n",
      "\n",
      "The following examples show 2D input arrays and their corresponding output arrays. Each output is generated by applying a specific pattern or rule to the input.\n",
      "\n",
      "- Please study the relationship between each `input` and its corresponding `output` in the `train` examples.\n",
      "- Then, based on these patterns, predict the `output` for the given `test` input.\n",
      "\n",
      "### Example 1\n",
      "Input:\n",
      "[[0, 0, 0, 0], [0, 5, 6, 0], [0, 8, 3, 0], [0, 0, 0, 0]]\n",
      "\n",
      "Output:\n",
      "[[5, 0, 0, 6], [0, 0, 0, 0], [0, 0, 0, 0], [8, 0, 0, 3]]\n",
      "\n",
      "### Example 2\n",
      "Input:\n",
      "[[0, 0, 0, 0], [0, 3, 4, 0], [0, 7, 6, 0], [0, 0, 0, 0]]\n",
      "\n",
      "Output:\n",
      "[[3, 0, 0, 4], [0, 0, 0, 0], [0, 0, 0, 0], [7, 0, 0, 6]]\n",
      "\n",
      "---\n",
      "\n",
      "### Test Input 1:\n",
      "[[0, 0, 0, 0], [0, 2, 3, 0], [0, 4, 9, 0], [0, 0, 0, 0]]\n",
      "\n",
      "**Task:**\n",
      "Based on the `train` examples above, generate the expected `output` array for this test input.\n",
      "\n",
      "\n",
      "=== Prompt for key 00576224 (length=838) ===\n",
      "\n",
      "The following examples show 2D input arrays and their corresponding output arrays. Each output is generated by applying a specific pattern or rule to the input.\n",
      "\n",
      "- Please study the relationship between each `input` and its corresponding `output` in the `train` examples.\n",
      "- Then, based on these patterns, predict the `output` for the given `test` input.\n",
      "\n",
      "### Example 1\n",
      "Input:\n",
      "[[7, 9], [4, 3]]\n",
      "\n",
      "Output:\n",
      "[[7, 9, 7, 9, 7, 9], [4, 3, 4, 3, 4, 3], [9, 7, 9, 7, 9, 7], [3, 4, 3, 4, 3, 4], [7, 9, 7, 9, 7, 9], [4, 3, 4, 3, 4, 3]]\n",
      "\n",
      "### Example 2\n",
      "Input:\n",
      "[[8, 6], [6, 4]]\n",
      "\n",
      "Output:\n",
      "[[8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4], [6, 8, 6, 8, 6, 8], [4, 6, 4, 6, 4, 6], [8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4]]\n",
      "\n",
      "---\n",
      "\n",
      "### Test Input 1:\n",
      "[[3, 2], [7, 8]]\n",
      "\n",
      "**Task:**\n",
      "Based on the `train` examples above, generate the expected `output` array for this test input.\n",
      "\n",
      "\n",
      "=== Prompt for key 49d1d64f (length=887) ===\n",
      "\n",
      "The following examples show 2D input arrays and their corresponding output arrays. Each output is generated by applying a specific pattern or rule to the input.\n",
      "\n",
      "- Please study the relationship between each `input` and its corresponding `output` in the `train` examples.\n",
      "- Then, based on these patterns, predict the `output` for the given `test` input.\n",
      "\n",
      "### Example 1\n",
      "Input:\n",
      "[[1, 8, 4], [8, 3, 8]]\n",
      "\n",
      "Output:\n",
      "[[0, 1, 8, 4, 0], [1, 1, 8, 4, 4], [8, 8, 3, 8, 8], [0, 8, 3, 8, 0]]\n",
      "\n",
      "### Example 2\n",
      "Input:\n",
      "[[1, 2], [3, 8]]\n",
      "\n",
      "Output:\n",
      "[[0, 1, 2, 0], [1, 1, 2, 2], [3, 3, 8, 8], [0, 3, 8, 0]]\n",
      "\n",
      "### Example 3\n",
      "Input:\n",
      "[[2, 1, 4], [8, 0, 2], [3, 2, 8]]\n",
      "\n",
      "Output:\n",
      "[[0, 2, 1, 4, 0], [2, 2, 1, 4, 4], [8, 8, 0, 2, 2], [3, 3, 2, 8, 8], [0, 3, 2, 8, 0]]\n",
      "\n",
      "---\n",
      "\n",
      "### Test Input 1:\n",
      "[[2, 8], [1, 4], [3, 4]]\n",
      "\n",
      "**Task:**\n",
      "Based on the `train` examples above, generate the expected `output` array for this test input.\n",
      "\n",
      "\n",
      "=== Prompt for key c9e6f938 (length=895) ===\n",
      "\n",
      "The following examples show 2D input arrays and their corresponding output arrays. Each output is generated by applying a specific pattern or rule to the input.\n",
      "\n",
      "- Please study the relationship between each `input` and its corresponding `output` in the `train` examples.\n",
      "- Then, based on these patterns, predict the `output` for the given `test` input.\n",
      "\n",
      "### Example 1\n",
      "Input:\n",
      "[[0, 0, 0], [0, 7, 7], [0, 0, 0]]\n",
      "\n",
      "Output:\n",
      "[[0, 0, 0, 0, 0, 0], [0, 7, 7, 7, 7, 0], [0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "### Example 2\n",
      "Input:\n",
      "[[0, 7, 0], [0, 0, 7], [0, 7, 7]]\n",
      "\n",
      "Output:\n",
      "[[0, 7, 0, 0, 7, 0], [0, 0, 7, 7, 0, 0], [0, 7, 7, 7, 7, 0]]\n",
      "\n",
      "### Example 3\n",
      "Input:\n",
      "[[0, 0, 0], [7, 0, 0], [0, 0, 0]]\n",
      "\n",
      "Output:\n",
      "[[0, 0, 0, 0, 0, 0], [7, 0, 0, 0, 0, 7], [0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "---\n",
      "\n",
      "### Test Input 1:\n",
      "[[7, 7, 0], [0, 7, 0], [0, 0, 7]]\n",
      "\n",
      "**Task:**\n",
      "Based on the `train` examples above, generate the expected `output` array for this test input.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_list = []\n",
    "\n",
    "for i, (key, example) in enumerate(json_data.items()):\n",
    "    prompt = generate_prompt_from_example(key, example)\n",
    "    prompt_list.append((key, prompt))\n",
    "\n",
    "# 길이 기준 오름차순 정렬\n",
    "prompt_list.sort(key=lambda x: len(x[1]))\n",
    "\n",
    "# 출력\n",
    "for key, prompt in prompt_list[:10]:\n",
    "    print(f\"\\n=== Prompt for key {key} (length={len(prompt)}) ===\\n\")\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d1cd34e-39ca-413d-b8c9-901b86c11eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prompt 1 (key: 833966f4, length: 675) ---\n",
      "\n",
      "Input: The following examples show 2D input arrays and their corresponding output arrays. Each output is generated by applying a specific pattern or rule to the input.\n",
      "\n",
      "- Please study the relationship between each `input` and its corresponding `output` in the `train` examples.\n",
      "- Then, based on these patterns, predict the `output` for the given `test` input.\n",
      "\n",
      "### Example 1\n",
      "Input:\n",
      "[[9], [0], [1], [6], [8]]\n",
      "\n",
      "Output:\n",
      "[[0], [9], [1], [8], [6]]\n",
      "\n",
      "### Example 2\n",
      "Input:\n",
      "[[4], [3], [6], [2], [8]]\n",
      "\n",
      "Output:\n",
      "[[3], [4], [6], [8], [2]]\n",
      "\n",
      "---\n",
      "\n",
      "### Test Input 1:\n",
      "[[4], [5], [6], [7], [2]]\n",
      "\n",
      "**Task:**\n",
      "Based on the `train` examples above, generate the expected `output` array for this test input.\n",
      "\n",
      "\n",
      "[00] Token: \u001b[48;2;25;152;204m   'Output'    \u001b[0m | LogProb:  -0.1804  Prob:  0.8350  | AvgNLL: 0.1804  PPL_Prob: 0.8349\n",
      "[01] Token: \u001b[48;2;0;143;230m      ':'      \u001b[0m | LogProb:  -0.0014  Prob:  0.9985  | AvgNLL: 0.0909  PPL_Prob: 0.9131\n",
      "[02] Token: \u001b[48;2;0;143;230m     '\\n'      \u001b[0m | LogProb:  -0.0306  Prob:  0.9697  | AvgNLL: 0.0708  PPL_Prob: 0.9316\n",
      "[03] Token: \u001b[48;2;0;143;230m     '[['      \u001b[0m | LogProb:  -0.1011  Prob:  0.9038  | AvgNLL: 0.0784  PPL_Prob: 0.9246\n",
      "[04] Token: \u001b[48;2;25;152;204m      '2'      \u001b[0m | LogProb:  -0.3796  Prob:  0.6841  | AvgNLL: 0.1386  PPL_Prob: 0.8705\n",
      "[05] Token: \u001b[48;2;25;152;204m     '],'      \u001b[0m | LogProb:  -0.0358  Prob:  0.9648  | AvgNLL: 0.1215  PPL_Prob: 0.8856\n",
      "[06] Token: \u001b[48;2;0;143;230m     ' ['      \u001b[0m | LogProb:  -0.0006  Prob:  0.9995  | AvgNLL: 0.1042  PPL_Prob: 0.9010\n",
      "[07] Token: \u001b[48;2;0;143;230m      '4'      \u001b[0m | LogProb:  -0.0321  Prob:  0.9683  | AvgNLL: 0.0952  PPL_Prob: 0.9092\n",
      "[08] Token: \u001b[48;2;0;143;230m     '],'      \u001b[0m | LogProb:  -0.0015  Prob:  0.9985  | AvgNLL: 0.0848  PPL_Prob: 0.9187\n",
      "[09] Token: \u001b[48;2;0;143;230m     ' ['      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0763  PPL_Prob: 0.9265\n",
      "[10] Token: \u001b[48;2;0;143;230m      '5'      \u001b[0m | LogProb:  -0.0095  Prob:  0.9907  | AvgNLL: 0.0702  PPL_Prob: 0.9322\n",
      "[11] Token: \u001b[48;2;0;143;230m     '],'      \u001b[0m | LogProb:  -0.0003  Prob:  0.9995  | AvgNLL: 0.0644  PPL_Prob: 0.9376\n",
      "[12] Token: \u001b[48;2;0;143;230m     ' ['      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0595  PPL_Prob: 0.9423\n",
      "[13] Token: \u001b[48;2;0;143;230m      '6'      \u001b[0m | LogProb:  -0.0076  Prob:  0.9922  | AvgNLL: 0.0557  PPL_Prob: 0.9458\n",
      "[14] Token: \u001b[48;2;0;143;230m     '],'      \u001b[0m | LogProb:  -0.0015  Prob:  0.9985  | AvgNLL: 0.0521  PPL_Prob: 0.9492\n",
      "[15] Token: \u001b[48;2;0;143;230m     ' ['      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0489  PPL_Prob: 0.9523\n",
      "[16] Token: \u001b[48;2;0;143;230m      '7'      \u001b[0m | LogProb:  -0.0008  Prob:  0.9990  | AvgNLL: 0.0461  PPL_Prob: 0.9550\n",
      "[17] Token: \u001b[48;2;0;143;230m     ']]'      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.0435  PPL_Prob: 0.9574\n",
      "[18] Token: \u001b[48;2;0;143;230m    '\\n\\n'     \u001b[0m | LogProb:  -0.0587  Prob:  0.9429  | AvgNLL: 0.0443  PPL_Prob: 0.9567\n",
      "[19] Token: \u001b[48;2;0;143;230m     '###'     \u001b[0m | LogProb:  -0.6436  Prob:  0.5254  | AvgNLL: 0.0743  PPL_Prob: 0.9284\n",
      "[20] Token: \u001b[48;2;0;143;230m    ' Test'    \u001b[0m | LogProb:  -0.2233  Prob:  0.7998  | AvgNLL: 0.0814  PPL_Prob: 0.9219\n",
      "[21] Token: \u001b[48;2;0;143;230m   ' Input'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0777  PPL_Prob: 0.9253\n",
      "[22] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0743  PPL_Prob: 0.9284\n",
      "[23] Token: \u001b[48;2;0;143;230m      '2'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0712  PPL_Prob: 0.9313\n",
      "[24] Token: \u001b[48;2;0;143;230m      ':'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0684  PPL_Prob: 0.9339\n",
      "[25] Token: \u001b[48;2;0;143;230m     '\\n'      \u001b[0m | LogProb:  -0.0019  Prob:  0.9980  | AvgNLL: 0.0658  PPL_Prob: 0.9363\n",
      "[26] Token: \u001b[48;2;0;143;230m    'Input'    \u001b[0m | LogProb:  -0.0040  Prob:  0.9961  | AvgNLL: 0.0635  PPL_Prob: 0.9385\n",
      "[27] Token: \u001b[48;2;0;143;230m      ':'      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0612  PPL_Prob: 0.9406\n",
      "[28] Token: \u001b[48;2;0;143;230m     '\\n'      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.0591  PPL_Prob: 0.9426\n",
      "[29] Token: \u001b[48;2;0;143;230m     '[['      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0572  PPL_Prob: 0.9444\n",
      "[30] Token: \u001b[48;2;0;143;230m      '1'      \u001b[0m | LogProb:  -0.7515  Prob:  0.4717  | AvgNLL: 0.0796  PPL_Prob: 0.9235\n",
      "[31] Token: \u001b[48;2;0;143;230m     '],'      \u001b[0m | LogProb:  -0.0135  Prob:  0.9868  | AvgNLL: 0.0775  PPL_Prob: 0.9254\n",
      "[32] Token: \u001b[48;2;0;143;230m     ' ['      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0752  PPL_Prob: 0.9276\n",
      "[33] Token: \u001b[48;2;0;143;230m      '2'      \u001b[0m | LogProb:  -0.0370  Prob:  0.9639  | AvgNLL: 0.0740  PPL_Prob: 0.9286\n",
      "[34] Token: \u001b[48;2;0;143;230m     '],'      \u001b[0m | LogProb:  -0.0085  Prob:  0.9917  | AvgNLL: 0.0722  PPL_Prob: 0.9304\n",
      "[35] Token: \u001b[48;2;0;143;230m     ' ['      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0702  PPL_Prob: 0.9322\n",
      "[36] Token: \u001b[48;2;0;143;230m      '3'      \u001b[0m | LogProb:  -0.0075  Prob:  0.9927  | AvgNLL: 0.0685  PPL_Prob: 0.9338\n",
      "[37] Token: \u001b[48;2;0;143;230m     '],'      \u001b[0m | LogProb:  -0.0298  Prob:  0.9707  | AvgNLL: 0.0674  PPL_Prob: 0.9348\n",
      "[38] Token: \u001b[48;2;0;143;230m     ' ['      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0657  PPL_Prob: 0.9364\n",
      "[39] Token: \u001b[48;2;0;143;230m      '4'      \u001b[0m | LogProb:  -0.0108  Prob:  0.9893  | AvgNLL: 0.0643  PPL_Prob: 0.9377\n",
      "[40] Token: \u001b[48;2;0;143;230m     '],'      \u001b[0m | LogProb:  -0.0145  Prob:  0.9854  | AvgNLL: 0.0631  PPL_Prob: 0.9388\n",
      "[41] Token: \u001b[48;2;0;143;230m     ' ['      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0616  PPL_Prob: 0.9402\n",
      "[42] Token: \u001b[48;2;0;143;230m      '5'      \u001b[0m | LogProb:  -0.0116  Prob:  0.9883  | AvgNLL: 0.0605  PPL_Prob: 0.9413\n",
      "[43] Token: \u001b[48;2;0;143;230m     ']]'      \u001b[0m | LogProb:  -0.0004  Prob:  0.9995  | AvgNLL: 0.0591  PPL_Prob: 0.9426\n",
      "[44] Token: \u001b[48;2;0;143;230m    '\\n\\n'     \u001b[0m | LogProb:  -0.0008  Prob:  0.9990  | AvgNLL: 0.0578  PPL_Prob: 0.9438\n",
      "[45] Token: \u001b[48;2;0;143;230m     '**'      \u001b[0m | LogProb:  -0.0009  Prob:  0.9990  | AvgNLL: 0.0566  PPL_Prob: 0.9450\n",
      "[46] Token: \u001b[48;2;0;143;230m    'Task'     \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0554  PPL_Prob: 0.9461\n",
      "[47] Token: \u001b[48;2;0;143;230m     ':**'     \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0542  PPL_Prob: 0.9472\n",
      "[48] Token: \u001b[48;2;0;143;230m     '\\n'      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0531  PPL_Prob: 0.9483\n",
      "[49] Token: \u001b[48;2;0;143;230m    'Based'    \u001b[0m | LogProb:  -0.0008  Prob:  0.9990  | AvgNLL: 0.0521  PPL_Prob: 0.9493\n",
      "[50] Token: \u001b[48;2;0;143;230m     ' on'     \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0510  PPL_Prob: 0.9502\n",
      "[51] Token: \u001b[48;2;0;143;230m    ' the'     \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0501  PPL_Prob: 0.9512\n",
      "[52] Token: \u001b[48;2;0;143;230m     ' `'      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.0491  PPL_Prob: 0.9521\n",
      "[53] Token: \u001b[48;2;0;143;230m    'train'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0482  PPL_Prob: 0.9529\n",
      "[54] Token: \u001b[48;2;0;143;230m      '`'      \u001b[0m | LogProb:   0.0000  Prob:  1.0000  | AvgNLL: 0.0473  PPL_Prob: 0.9538\n",
      "[55] Token: \u001b[48;2;0;143;230m  ' examples'  \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0465  PPL_Prob: 0.9546\n",
      "[56] Token: \u001b[48;2;0;143;230m   ' above'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0457  PPL_Prob: 0.9554\n",
      "[57] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0449  PPL_Prob: 0.9561\n",
      "[58] Token: \u001b[48;2;0;143;230m  ' generate'  \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0441  PPL_Prob: 0.9568\n",
      "[59] Token: \u001b[48;2;0;143;230m    ' the'     \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0434  PPL_Prob: 0.9575\n",
      "[60] Token: \u001b[48;2;0;143;230m  ' expected'  \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0427  PPL_Prob: 0.9582\n",
      "[61] Token: \u001b[48;2;0;143;230m     ' `'      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.0420  PPL_Prob: 0.9589\n",
      "[62] Token: \u001b[48;2;0;143;230m   'output'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0413  PPL_Prob: 0.9595\n",
      "[63] Token: \u001b[48;2;0;143;230m      '`'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0407  PPL_Prob: 0.9601\n",
      "[64] Token: \u001b[48;2;0;143;230m   ' array'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0401  PPL_Prob: 0.9607\n",
      "[65] Token: \u001b[48;2;0;143;230m    ' for'     \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0395  PPL_Prob: 0.9613\n",
      "[66] Token: \u001b[48;2;0;143;230m    ' this'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0389  PPL_Prob: 0.9619\n",
      "[67] Token: \u001b[48;2;0;143;230m    ' test'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0383  PPL_Prob: 0.9624\n",
      "[68] Token: \u001b[48;2;0;143;230m   ' input'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0377  PPL_Prob: 0.9630\n",
      "[69] Token: \u001b[48;2;0;143;230m      '.'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0372  PPL_Prob: 0.9635\n",
      "[70] Token: \u001b[48;2;0;143;230m     '\\n'      \u001b[0m | LogProb:  -0.0031  Prob:  0.9971  | AvgNLL: 0.0367  PPL_Prob: 0.9639\n",
      "[71] Token: \u001b[48;2;0;143;230m   'Output'    \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0362  PPL_Prob: 0.9644\n",
      "[72] Token: \u001b[48;2;0;143;230m      ':'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0357  PPL_Prob: 0.9649\n",
      "[73] Token: \u001b[48;2;0;143;230m     '\\n'      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0352  PPL_Prob: 0.9654\n",
      "[74] Token: \u001b[48;2;0;143;230m     '[['      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0348  PPL_Prob: 0.9658\n",
      "[75] Token: \u001b[48;2;0;143;230m      '1'      \u001b[0m | LogProb:  -0.0054  Prob:  0.9946  | AvgNLL: 0.0344  PPL_Prob: 0.9662\n",
      "[76] Token: \u001b[48;2;0;143;230m     '],'      \u001b[0m | LogProb:  -0.0016  Prob:  0.9985  | AvgNLL: 0.0340  PPL_Prob: 0.9666\n",
      "[77] Token: \u001b[48;2;0;143;230m     ' ['      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0335  PPL_Prob: 0.9670\n",
      "[78] Token: \u001b[48;2;0;143;230m      '2'      \u001b[0m | LogProb:  -0.0051  Prob:  0.9951  | AvgNLL: 0.0332  PPL_Prob: 0.9674\n",
      "[79] Token: \u001b[48;2;0;143;230m     '],'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0327  PPL_Prob: 0.9678\n",
      "[80] Token: \u001b[48;2;0;143;230m     ' ['      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0323  PPL_Prob: 0.9682\n",
      "[81] Token: \u001b[48;2;0;143;230m      '3'      \u001b[0m | LogProb:  -0.0005  Prob:  0.9995  | AvgNLL: 0.0320  PPL_Prob: 0.9686\n",
      "[82] Token: \u001b[48;2;0;143;230m     '],'      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0316  PPL_Prob: 0.9689\n",
      "[83] Token: \u001b[48;2;0;143;230m     ' ['      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0312  PPL_Prob: 0.9693\n",
      "[84] Token: \u001b[48;2;0;143;230m      '4'      \u001b[0m | LogProb:  -0.0011  Prob:  0.9990  | AvgNLL: 0.0308  PPL_Prob: 0.9696\n",
      "[85] Token: \u001b[48;2;0;143;230m     '],'      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.0305  PPL_Prob: 0.9700\n",
      "[86] Token: \u001b[48;2;0;143;230m     ' ['      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0301  PPL_Prob: 0.9703\n",
      "[87] Token: \u001b[48;2;0;143;230m      '5'      \u001b[0m | LogProb:  -0.0005  Prob:  0.9995  | AvgNLL: 0.0298  PPL_Prob: 0.9706\n",
      "[88] Token: \u001b[48;2;0;143;230m     ']]'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0295  PPL_Prob: 0.9710\n",
      "[89] Token: \u001b[48;2;0;143;230m    '\\n\\n'     \u001b[0m | LogProb:  -0.0140  Prob:  0.9858  | AvgNLL: 0.0293  PPL_Prob: 0.9711\n",
      "[90] Token: \u001b[48;2;0;143;230m     '---'     \u001b[0m | LogProb:  -0.2151  Prob:  0.8066  | AvgNLL: 0.0313  PPL_Prob: 0.9692\n",
      "[91] Token: \u001b[48;2;0;143;230m    '\\n\\n'     \u001b[0m | LogProb:  -0.0313  Prob:  0.9692  | AvgNLL: 0.0313  PPL_Prob: 0.9692\n",
      "[92] Token: \u001b[48;2;0;143;230m     '###'     \u001b[0m | LogProb:  -0.0065  Prob:  0.9937  | AvgNLL: 0.0311  PPL_Prob: 0.9694\n",
      "[93] Token: \u001b[48;2;0;143;230m    ' Test'    \u001b[0m | LogProb:  -0.0137  Prob:  0.9863  | AvgNLL: 0.0309  PPL_Prob: 0.9696\n",
      "[94] Token: \u001b[48;2;0;143;230m   ' Input'    \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0306  PPL_Prob: 0.9699\n",
      "[95] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0302  PPL_Prob: 0.9702\n",
      "[96] Token: \u001b[48;2;0;143;230m      '3'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0299  PPL_Prob: 0.9705\n",
      "[97] Token: \u001b[48;2;0;143;230m      ':'      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.0296  PPL_Prob: 0.9708\n",
      "[98] Token: \u001b[48;2;0;143;230m     '\\n'      \u001b[0m | LogProb:  -0.0040  Prob:  0.9961  | AvgNLL: 0.0294  PPL_Prob: 0.9711\n",
      "[99] Token: \u001b[48;2;0;143;230m    'Input'    \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0291  PPL_Prob: 0.9713\n",
      "\n",
      "== Self Perplexity ==\n",
      "Avg NLL: 0.0291\n",
      "Perplexity (self): 0.9713\n",
      "\n",
      "--- Prompt 2 (key: 6150a2bd, length: 715) ---\n",
      "\n",
      "Input: The following examples show 2D input arrays and their corresponding output arrays. Each output is generated by applying a specific pattern or rule to the input.\n",
      "\n",
      "- Please study the relationship between each `input` and its corresponding `output` in the `train` examples.\n",
      "- Then, based on these patterns, predict the `output` for the given `test` input.\n",
      "\n",
      "### Example 1\n",
      "Input:\n",
      "[[5, 5, 2], [1, 0, 0], [0, 0, 0]]\n",
      "\n",
      "Output:\n",
      "[[0, 0, 0], [0, 0, 1], [2, 5, 5]]\n",
      "\n",
      "### Example 2\n",
      "Input:\n",
      "[[3, 3, 8], [3, 7, 0], [5, 0, 0]]\n",
      "\n",
      "Output:\n",
      "[[0, 0, 5], [0, 7, 3], [8, 3, 3]]\n",
      "\n",
      "---\n",
      "\n",
      "### Test Input 1:\n",
      "[[6, 3, 5], [6, 8, 0], [4, 0, 0]]\n",
      "\n",
      "**Task:**\n",
      "Based on the `train` examples above, generate the expected `output` array for this test input.\n",
      "\n",
      "\n",
      "[00] Token: \u001b[48;2;101;179;127m   'Output'    \u001b[0m | LogProb:  -0.5356  Prob:  0.5854  | AvgNLL: 0.5356  PPL_Prob: 0.5853\n",
      "[01] Token: \u001b[48;2;50;161;178m      ':'      \u001b[0m | LogProb:  -0.0019  Prob:  0.9980  | AvgNLL: 0.2687  PPL_Prob: 0.7643\n",
      "[02] Token: \u001b[48;2;25;152;204m     '\\n'      \u001b[0m | LogProb:  -0.0970  Prob:  0.9077  | AvgNLL: 0.2115  PPL_Prob: 0.8094\n",
      "[03] Token: \u001b[48;2;25;152;204m     '[['      \u001b[0m | LogProb:  -0.1967  Prob:  0.8213  | AvgNLL: 0.2078  PPL_Prob: 0.8124\n",
      "[04] Token: \u001b[48;2;50;161;178m      '0'      \u001b[0m | LogProb:  -0.3279  Prob:  0.7202  | AvgNLL: 0.2318  PPL_Prob: 0.7931\n",
      "[05] Token: \u001b[48;2;25;152;204m      ','      \u001b[0m | LogProb:  -0.0005  Prob:  0.9995  | AvgNLL: 0.1933  PPL_Prob: 0.8243\n",
      "[06] Token: \u001b[48;2;25;152;204m      ' '      \u001b[0m | LogProb:  -0.0004  Prob:  0.9995  | AvgNLL: 0.1657  PPL_Prob: 0.8473\n",
      "[07] Token: \u001b[48;2;25;152;204m      '0'      \u001b[0m | LogProb:  -0.0587  Prob:  0.9429  | AvgNLL: 0.1523  PPL_Prob: 0.8587\n",
      "[08] Token: \u001b[48;2;25;152;204m      ','      \u001b[0m | LogProb:  -0.0003  Prob:  0.9995  | AvgNLL: 0.1355  PPL_Prob: 0.8733\n",
      "[09] Token: \u001b[48;2;25;152;204m      ' '      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.1219  PPL_Prob: 0.8852\n",
      "[10] Token: \u001b[48;2;25;152;204m      '0'      \u001b[0m | LogProb:  -0.0770  Prob:  0.9258  | AvgNLL: 0.1178  PPL_Prob: 0.8888\n",
      "[11] Token: \u001b[48;2;25;152;204m     '],'      \u001b[0m | LogProb:  -0.0020  Prob:  0.9980  | AvgNLL: 0.1082  PPL_Prob: 0.8975\n",
      "[12] Token: \u001b[48;2;0;143;230m     ' ['      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0999  PPL_Prob: 0.9050\n",
      "[13] Token: \u001b[48;2;0;143;230m      '0'      \u001b[0m | LogProb:  -0.0296  Prob:  0.9707  | AvgNLL: 0.0949  PPL_Prob: 0.9095\n",
      "[14] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0885  PPL_Prob: 0.9153\n",
      "[15] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.0830  PPL_Prob: 0.9203\n",
      "[16] Token: \u001b[48;2;0;143;230m      '0'      \u001b[0m | LogProb:  -0.0649  Prob:  0.9370  | AvgNLL: 0.0819  PPL_Prob: 0.9213\n",
      "[17] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0774  PPL_Prob: 0.9255\n",
      "[18] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.0733  PPL_Prob: 0.9293\n",
      "[19] Token: \u001b[48;2;0;143;230m      '1'      \u001b[0m | LogProb:  -0.1240  Prob:  0.8833  | AvgNLL: 0.0759  PPL_Prob: 0.9269\n",
      "[20] Token: \u001b[48;2;0;143;230m     '],'      \u001b[0m | LogProb:  -0.0003  Prob:  0.9995  | AvgNLL: 0.0723  PPL_Prob: 0.9303\n",
      "[21] Token: \u001b[48;2;0;143;230m     ' ['      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0690  PPL_Prob: 0.9333\n",
      "[22] Token: \u001b[48;2;0;143;230m      '2'      \u001b[0m | LogProb:  -0.2455  Prob:  0.7822  | AvgNLL: 0.0767  PPL_Prob: 0.9262\n",
      "[23] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.0735  PPL_Prob: 0.9292\n",
      "[24] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.0705  PPL_Prob: 0.9319\n",
      "[25] Token: \u001b[48;2;0;143;230m      '5'      \u001b[0m | LogProb:  -0.1299  Prob:  0.8784  | AvgNLL: 0.0728  PPL_Prob: 0.9298\n",
      "[26] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0701  PPL_Prob: 0.9323\n",
      "[27] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0676  PPL_Prob: 0.9346\n",
      "[28] Token: \u001b[48;2;0;143;230m      '5'      \u001b[0m | LogProb:  -0.0107  Prob:  0.9893  | AvgNLL: 0.0657  PPL_Prob: 0.9364\n",
      "[29] Token: \u001b[48;2;0;143;230m     ']]'      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.0635  PPL_Prob: 0.9385\n",
      "[30] Token: \u001b[48;2;0;143;230m    '\\n\\n'     \u001b[0m | LogProb:  -0.0749  Prob:  0.9277  | AvgNLL: 0.0639  PPL_Prob: 0.9381\n",
      "[31] Token: \u001b[48;2;0;143;230m     '**'      \u001b[0m | LogProb:  -0.8535  Prob:  0.4260  | AvgNLL: 0.0885  PPL_Prob: 0.9153\n",
      "[32] Token: \u001b[48;2;0;143;230m 'Explanation' \u001b[0m | LogProb:  -0.1223  Prob:  0.8848  | AvgNLL: 0.0896  PPL_Prob: 0.9143\n",
      "[33] Token: \u001b[48;2;0;143;230m     ':**'     \u001b[0m | LogProb:  -0.0034  Prob:  0.9966  | AvgNLL: 0.0870  PPL_Prob: 0.9167\n",
      "[34] Token: \u001b[48;2;0;143;230m     '\\n'      \u001b[0m | LogProb:  -0.4668  Prob:  0.6270  | AvgNLL: 0.0979  PPL_Prob: 0.9068\n",
      "[35] Token: \u001b[48;2;0;143;230m     'The'     \u001b[0m | LogProb:  -0.0592  Prob:  0.9424  | AvgNLL: 0.0968  PPL_Prob: 0.9077\n",
      "[36] Token: \u001b[48;2;25;152;204m  ' pattern'   \u001b[0m | LogProb:  -1.0586  Prob:  0.3469  | AvgNLL: 0.1228  PPL_Prob: 0.8844\n",
      "[37] Token: \u001b[48;2;25;152;204m     ' is'     \u001b[0m | LogProb:  -0.5010  Prob:  0.6060  | AvgNLL: 0.1327  PPL_Prob: 0.8757\n",
      "[38] Token: \u001b[48;2;25;152;204m     ' to'     \u001b[0m | LogProb:  -0.1290  Prob:  0.8789  | AvgNLL: 0.1326  PPL_Prob: 0.8758\n",
      "[39] Token: \u001b[48;2;25;152;204m   ' apply'    \u001b[0m | LogProb:  -2.2246  Prob:  0.1081  | AvgNLL: 0.1849  PPL_Prob: 0.8311\n",
      "[40] Token: \u001b[48;2;25;152;204m     ' a'      \u001b[0m | LogProb:  -0.7109  Prob:  0.4912  | AvgNLL: 0.1978  PPL_Prob: 0.8206\n",
      "[41] Token: \u001b[48;2;25;152;204m' transformation'\u001b[0m | LogProb:  -1.2383  Prob:  0.2898  | AvgNLL: 0.2226  PPL_Prob: 0.8005\n",
      "[42] Token: \u001b[48;2;25;152;204m     ' to'     \u001b[0m | LogProb:  -0.1982  Prob:  0.8203  | AvgNLL: 0.2220  PPL_Prob: 0.8009\n",
      "[43] Token: \u001b[48;2;25;152;204m    ' the'     \u001b[0m | LogProb:  -0.2720  Prob:  0.7617  | AvgNLL: 0.2231  PPL_Prob: 0.8000\n",
      "[44] Token: \u001b[48;2;25;152;204m   ' input'    \u001b[0m | LogProb:  -0.0625  Prob:  0.9395  | AvgNLL: 0.2196  PPL_Prob: 0.8029\n",
      "[45] Token: \u001b[48;2;50;161;178m   ' array'    \u001b[0m | LogProb:  -1.2607  Prob:  0.2834  | AvgNLL: 0.2422  PPL_Prob: 0.7849\n",
      "[46] Token: \u001b[48;2;50;161;178m      '.'      \u001b[0m | LogProb:  -1.1084  Prob:  0.3301  | AvgNLL: 0.2606  PPL_Prob: 0.7706\n",
      "[47] Token: \u001b[48;2;50;161;178m    ' The'     \u001b[0m | LogProb:  -1.2930  Prob:  0.2744  | AvgNLL: 0.2821  PPL_Prob: 0.7542\n",
      "[48] Token: \u001b[48;2;50;161;178m   ' first'    \u001b[0m | LogProb:  -1.3428  Prob:  0.2612  | AvgNLL: 0.3038  PPL_Prob: 0.7380\n",
      "[49] Token: \u001b[48;2;50;161;178m    ' row'     \u001b[0m | LogProb:  -0.9287  Prob:  0.3950  | AvgNLL: 0.3163  PPL_Prob: 0.7289\n",
      "[50] Token: \u001b[48;2;50;161;178m     ' is'     \u001b[0m | LogProb:  -0.4768  Prob:  0.6206  | AvgNLL: 0.3194  PPL_Prob: 0.7266\n",
      "[51] Token: \u001b[48;2;50;161;178m' transformed' \u001b[0m | LogProb:  -1.5791  Prob:  0.2062  | AvgNLL: 0.3436  PPL_Prob: 0.7092\n",
      "[52] Token: \u001b[48;2;50;161;178m     ' to'     \u001b[0m | LogProb:  -0.9814  Prob:  0.3748  | AvgNLL: 0.3557  PPL_Prob: 0.7007\n",
      "[53] Token: \u001b[48;2;76;170;153m    ' the'     \u001b[0m | LogProb:  -1.9668  Prob:  0.1399  | AvgNLL: 0.3855  PPL_Prob: 0.6801\n",
      "[54] Token: \u001b[48;2;76;170;153m   ' first'    \u001b[0m | LogProb:  -0.5044  Prob:  0.6040  | AvgNLL: 0.3877  PPL_Prob: 0.6786\n",
      "[55] Token: \u001b[48;2;76;170;153m   ' column'   \u001b[0m | LogProb:  -0.5229  Prob:  0.5928  | AvgNLL: 0.3901  PPL_Prob: 0.6770\n",
      "[56] Token: \u001b[48;2;76;170;153m      ','      \u001b[0m | LogProb:  -0.6685  Prob:  0.5127  | AvgNLL: 0.3950  PPL_Prob: 0.6737\n",
      "[57] Token: \u001b[48;2;76;170;153m    ' the'     \u001b[0m | LogProb:  -0.2769  Prob:  0.7583  | AvgNLL: 0.3929  PPL_Prob: 0.6751\n",
      "[58] Token: \u001b[48;2;76;170;153m   ' second'   \u001b[0m | LogProb:  -0.0048  Prob:  0.9951  | AvgNLL: 0.3864  PPL_Prob: 0.6795\n",
      "[59] Token: \u001b[48;2;76;170;153m    ' row'     \u001b[0m | LogProb:  -0.0133  Prob:  0.9868  | AvgNLL: 0.3801  PPL_Prob: 0.6838\n",
      "[60] Token: \u001b[48;2;76;170;153m     ' is'     \u001b[0m | LogProb:  -0.4194  Prob:  0.6572  | AvgNLL: 0.3808  PPL_Prob: 0.6833\n",
      "[61] Token: \u001b[48;2;76;170;153m' transformed' \u001b[0m | LogProb:  -0.0126  Prob:  0.9873  | AvgNLL: 0.3748  PPL_Prob: 0.6874\n",
      "[62] Token: \u001b[48;2;76;170;153m     ' to'     \u001b[0m | LogProb:  -0.0083  Prob:  0.9917  | AvgNLL: 0.3690  PPL_Prob: 0.6914\n",
      "[63] Token: \u001b[48;2;76;170;153m    ' the'     \u001b[0m | LogProb:  -0.0130  Prob:  0.9873  | AvgNLL: 0.3635  PPL_Prob: 0.6953\n",
      "[64] Token: \u001b[48;2;76;170;153m   ' second'   \u001b[0m | LogProb:  -0.0238  Prob:  0.9766  | AvgNLL: 0.3582  PPL_Prob: 0.6989\n",
      "[65] Token: \u001b[48;2;50;161;178m   ' column'   \u001b[0m | LogProb:  -0.0024  Prob:  0.9976  | AvgNLL: 0.3528  PPL_Prob: 0.7027\n",
      "[66] Token: \u001b[48;2;50;161;178m      ','      \u001b[0m | LogProb:  -0.0341  Prob:  0.9663  | AvgNLL: 0.3481  PPL_Prob: 0.7060\n",
      "[67] Token: \u001b[48;2;50;161;178m    ' and'     \u001b[0m | LogProb:  -0.0094  Prob:  0.9907  | AvgNLL: 0.3431  PPL_Prob: 0.7096\n",
      "[68] Token: \u001b[48;2;50;161;178m    ' the'     \u001b[0m | LogProb:  -0.1113  Prob:  0.8945  | AvgNLL: 0.3397  PPL_Prob: 0.7119\n",
      "[69] Token: \u001b[48;2;50;161;178m   ' third'    \u001b[0m | LogProb:  -0.0477  Prob:  0.9536  | AvgNLL: 0.3356  PPL_Prob: 0.7149\n",
      "[70] Token: \u001b[48;2;50;161;178m    ' row'     \u001b[0m | LogProb:  -0.0029  Prob:  0.9971  | AvgNLL: 0.3309  PPL_Prob: 0.7183\n",
      "[71] Token: \u001b[48;2;50;161;178m     ' is'     \u001b[0m | LogProb:  -0.0471  Prob:  0.9541  | AvgNLL: 0.3270  PPL_Prob: 0.7211\n",
      "[72] Token: \u001b[48;2;50;161;178m' transformed' \u001b[0m | LogProb:  -0.0137  Prob:  0.9863  | AvgNLL: 0.3227  PPL_Prob: 0.7242\n",
      "[73] Token: \u001b[48;2;50;161;178m     ' to'     \u001b[0m | LogProb:  -0.0089  Prob:  0.9912  | AvgNLL: 0.3184  PPL_Prob: 0.7273\n",
      "[74] Token: \u001b[48;2;50;161;178m    ' the'     \u001b[0m | LogProb:  -0.0029  Prob:  0.9971  | AvgNLL: 0.3142  PPL_Prob: 0.7304\n",
      "[75] Token: \u001b[48;2;50;161;178m   ' third'    \u001b[0m | LogProb:  -0.0099  Prob:  0.9902  | AvgNLL: 0.3102  PPL_Prob: 0.7333\n",
      "[76] Token: \u001b[48;2;50;161;178m   ' column'   \u001b[0m | LogProb:  -0.0013  Prob:  0.9985  | AvgNLL: 0.3062  PPL_Prob: 0.7362\n",
      "[77] Token: \u001b[48;2;50;161;178m      '.'      \u001b[0m | LogProb:  -0.0275  Prob:  0.9727  | AvgNLL: 0.3026  PPL_Prob: 0.7389\n",
      "[78] Token: \u001b[48;2;50;161;178m    '\\n\\n'     \u001b[0m | LogProb:  -1.0791  Prob:  0.3398  | AvgNLL: 0.3125  PPL_Prob: 0.7317\n",
      "[79] Token: \u001b[48;2;50;161;178m     '---'     \u001b[0m | LogProb:  -0.5449  Prob:  0.5801  | AvgNLL: 0.3154  PPL_Prob: 0.7295\n",
      "[80] Token: \u001b[48;2;50;161;178m    '\\n\\n'     \u001b[0m | LogProb:  -0.0486  Prob:  0.9526  | AvgNLL: 0.3121  PPL_Prob: 0.7319\n",
      "[81] Token: \u001b[48;2;50;161;178m     '###'     \u001b[0m | LogProb:  -0.0282  Prob:  0.9722  | AvgNLL: 0.3086  PPL_Prob: 0.7345\n",
      "[82] Token: \u001b[48;2;50;161;178m    ' Test'    \u001b[0m | LogProb:  -0.5137  Prob:  0.5981  | AvgNLL: 0.3111  PPL_Prob: 0.7327\n",
      "[83] Token: \u001b[48;2;50;161;178m   ' Input'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.3074  PPL_Prob: 0.7354\n",
      "[84] Token: \u001b[48;2;50;161;178m      ' '      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.3038  PPL_Prob: 0.7380\n",
      "[85] Token: \u001b[48;2;50;161;178m      '2'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.3002  PPL_Prob: 0.7407\n",
      "[86] Token: \u001b[48;2;50;161;178m      ':'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2968  PPL_Prob: 0.7432\n",
      "[87] Token: \u001b[48;2;50;161;178m     '\\n'      \u001b[0m | LogProb:  -0.0174  Prob:  0.9829  | AvgNLL: 0.2936  PPL_Prob: 0.7456\n",
      "[88] Token: \u001b[48;2;50;161;178m    'Input'    \u001b[0m | LogProb:  -0.0337  Prob:  0.9668  | AvgNLL: 0.2907  PPL_Prob: 0.7478\n",
      "[89] Token: \u001b[48;2;50;161;178m      ':'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2874  PPL_Prob: 0.7502\n",
      "[90] Token: \u001b[48;2;50;161;178m     '\\n'      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.2843  PPL_Prob: 0.7525\n",
      "[91] Token: \u001b[48;2;50;161;178m     '[['      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2812  PPL_Prob: 0.7549\n",
      "[92] Token: \u001b[48;2;50;161;178m      '1'      \u001b[0m | LogProb:  -0.6909  Prob:  0.5010  | AvgNLL: 0.2856  PPL_Prob: 0.7516\n",
      "[93] Token: \u001b[48;2;50;161;178m      ','      \u001b[0m | LogProb:  -0.0261  Prob:  0.9741  | AvgNLL: 0.2828  PPL_Prob: 0.7536\n",
      "[94] Token: \u001b[48;2;50;161;178m      ' '      \u001b[0m | LogProb:  -0.0011  Prob:  0.9990  | AvgNLL: 0.2799  PPL_Prob: 0.7559\n",
      "[95] Token: \u001b[48;2;50;161;178m      '2'      \u001b[0m | LogProb:  -0.3542  Prob:  0.7017  | AvgNLL: 0.2807  PPL_Prob: 0.7553\n",
      "[96] Token: \u001b[48;2;50;161;178m      ','      \u001b[0m | LogProb:  -0.0007  Prob:  0.9990  | AvgNLL: 0.2778  PPL_Prob: 0.7575\n",
      "[97] Token: \u001b[48;2;50;161;178m      ' '      \u001b[0m | LogProb:  -0.0003  Prob:  0.9995  | AvgNLL: 0.2749  PPL_Prob: 0.7596\n",
      "[98] Token: \u001b[48;2;50;161;178m      '3'      \u001b[0m | LogProb:  -0.0462  Prob:  0.9551  | AvgNLL: 0.2726  PPL_Prob: 0.7614\n",
      "[99] Token: \u001b[48;2;50;161;178m     '],'      \u001b[0m | LogProb:  -0.0094  Prob:  0.9907  | AvgNLL: 0.2700  PPL_Prob: 0.7634\n",
      "\n",
      "== Self Perplexity ==\n",
      "Avg NLL: 0.2700\n",
      "Perplexity (self): 0.7634\n",
      "\n",
      "--- Prompt 3 (key: e9afcf9a, length: 750) ---\n",
      "\n",
      "Input: The following examples show 2D input arrays and their corresponding output arrays. Each output is generated by applying a specific pattern or rule to the input.\n",
      "\n",
      "- Please study the relationship between each `input` and its corresponding `output` in the `train` examples.\n",
      "- Then, based on these patterns, predict the `output` for the given `test` input.\n",
      "\n",
      "### Example 1\n",
      "Input:\n",
      "[[4, 4, 4, 4, 4, 4], [8, 8, 8, 8, 8, 8]]\n",
      "\n",
      "Output:\n",
      "[[4, 8, 4, 8, 4, 8], [8, 4, 8, 4, 8, 4]]\n",
      "\n",
      "### Example 2\n",
      "Input:\n",
      "[[3, 3, 3, 3, 3, 3], [9, 9, 9, 9, 9, 9]]\n",
      "\n",
      "Output:\n",
      "[[3, 9, 3, 9, 3, 9], [9, 3, 9, 3, 9, 3]]\n",
      "\n",
      "---\n",
      "\n",
      "### Test Input 1:\n",
      "[[6, 6, 6, 6, 6, 6], [2, 2, 2, 2, 2, 2]]\n",
      "\n",
      "**Task:**\n",
      "Based on the `train` examples above, generate the expected `output` array for this test input.\n",
      "\n",
      "\n",
      "[00] Token: \u001b[48;2;50;161;178m   'Output'    \u001b[0m | LogProb:  -0.3425  Prob:  0.7100  | AvgNLL: 0.3425  PPL_Prob: 0.7100\n",
      "[01] Token: \u001b[48;2;25;152;204m      ':'      \u001b[0m | LogProb:  -0.0018  Prob:  0.9980  | AvgNLL: 0.1721  PPL_Prob: 0.8419\n",
      "[02] Token: \u001b[48;2;25;152;204m     '\\n'      \u001b[0m | LogProb:  -0.0414  Prob:  0.9595  | AvgNLL: 0.1286  PPL_Prob: 0.8794\n",
      "[03] Token: \u001b[48;2;25;152;204m     '[['      \u001b[0m | LogProb:  -0.1285  Prob:  0.8794  | AvgNLL: 0.1286  PPL_Prob: 0.8794\n",
      "[04] Token: \u001b[48;2;0;143;230m      '6'      \u001b[0m | LogProb:  -0.0069  Prob:  0.9932  | AvgNLL: 0.1042  PPL_Prob: 0.9010\n",
      "[05] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0006  Prob:  0.9995  | AvgNLL: 0.0870  PPL_Prob: 0.9167\n",
      "[06] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0010  Prob:  0.9990  | AvgNLL: 0.0747  PPL_Prob: 0.9280\n",
      "[07] Token: \u001b[48;2;0;143;230m      '6'      \u001b[0m | LogProb:  -0.0036  Prob:  0.9966  | AvgNLL: 0.0658  PPL_Prob: 0.9363\n",
      "[08] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.0585  PPL_Prob: 0.9432\n",
      "[09] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0526  PPL_Prob: 0.9487\n",
      "[10] Token: \u001b[48;2;0;143;230m      '6'      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0479  PPL_Prob: 0.9533\n",
      "[11] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0005  Prob:  0.9995  | AvgNLL: 0.0439  PPL_Prob: 0.9570\n",
      "[12] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0405  PPL_Prob: 0.9603\n",
      "[13] Token: \u001b[48;2;0;143;230m      '6'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0377  PPL_Prob: 0.9630\n",
      "[14] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0351  PPL_Prob: 0.9655\n",
      "[15] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0329  PPL_Prob: 0.9676\n",
      "[16] Token: \u001b[48;2;0;143;230m      '6'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0310  PPL_Prob: 0.9695\n",
      "[17] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0293  PPL_Prob: 0.9711\n",
      "[18] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0277  PPL_Prob: 0.9726\n",
      "[19] Token: \u001b[48;2;0;143;230m      '6'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0264  PPL_Prob: 0.9740\n",
      "[20] Token: \u001b[48;2;0;143;230m     '],'      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.0251  PPL_Prob: 0.9752\n",
      "[21] Token: \u001b[48;2;0;143;230m     ' ['      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0240  PPL_Prob: 0.9763\n",
      "[22] Token: \u001b[48;2;0;143;230m      '2'      \u001b[0m | LogProb:  -0.0006  Prob:  0.9995  | AvgNLL: 0.0230  PPL_Prob: 0.9773\n",
      "[23] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0220  PPL_Prob: 0.9782\n",
      "[24] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0211  PPL_Prob: 0.9791\n",
      "[25] Token: \u001b[48;2;0;143;230m      '2'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0203  PPL_Prob: 0.9799\n",
      "[26] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0196  PPL_Prob: 0.9806\n",
      "[27] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0189  PPL_Prob: 0.9813\n",
      "[28] Token: \u001b[48;2;0;143;230m      '2'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0182  PPL_Prob: 0.9820\n",
      "[29] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0176  PPL_Prob: 0.9825\n",
      "[30] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0170  PPL_Prob: 0.9831\n",
      "[31] Token: \u001b[48;2;0;143;230m      '2'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0165  PPL_Prob: 0.9836\n",
      "[32] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0160  PPL_Prob: 0.9841\n",
      "[33] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:   0.0000  Prob:  1.0000  | AvgNLL: 0.0155  PPL_Prob: 0.9846\n",
      "[34] Token: \u001b[48;2;0;143;230m      '2'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0151  PPL_Prob: 0.9850\n",
      "[35] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0147  PPL_Prob: 0.9854\n",
      "[36] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0143  PPL_Prob: 0.9858\n",
      "[37] Token: \u001b[48;2;0;143;230m      '2'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0139  PPL_Prob: 0.9862\n",
      "[38] Token: \u001b[48;2;0;143;230m     ']]'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0135  PPL_Prob: 0.9865\n",
      "[39] Token: \u001b[48;2;0;143;230m    '\\n\\n'     \u001b[0m | LogProb:  -0.1407  Prob:  0.8687  | AvgNLL: 0.0167  PPL_Prob: 0.9834\n",
      "[40] Token: \u001b[48;2;0;143;230m     '---'     \u001b[0m | LogProb:  -1.0059  Prob:  0.3657  | AvgNLL: 0.0408  PPL_Prob: 0.9600\n",
      "[41] Token: \u001b[48;2;0;143;230m    '\\n\\n'     \u001b[0m | LogProb:  -0.0584  Prob:  0.9434  | AvgNLL: 0.0413  PPL_Prob: 0.9596\n",
      "[42] Token: \u001b[48;2;0;143;230m     '###'     \u001b[0m | LogProb:  -0.0134  Prob:  0.9868  | AvgNLL: 0.0406  PPL_Prob: 0.9602\n",
      "[43] Token: \u001b[48;2;0;143;230m    ' Test'    \u001b[0m | LogProb:  -0.0550  Prob:  0.9463  | AvgNLL: 0.0409  PPL_Prob: 0.9599\n",
      "[44] Token: \u001b[48;2;0;143;230m   ' Input'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0400  PPL_Prob: 0.9608\n",
      "[45] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0392  PPL_Prob: 0.9616\n",
      "[46] Token: \u001b[48;2;0;143;230m      '2'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0383  PPL_Prob: 0.9624\n",
      "[47] Token: \u001b[48;2;0;143;230m      ':'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0375  PPL_Prob: 0.9632\n",
      "[48] Token: \u001b[48;2;0;143;230m     '\\n'      \u001b[0m | LogProb:  -0.0094  Prob:  0.9907  | AvgNLL: 0.0370  PPL_Prob: 0.9637\n",
      "[49] Token: \u001b[48;2;0;143;230m    'Input'    \u001b[0m | LogProb:  -0.2776  Prob:  0.7578  | AvgNLL: 0.0418  PPL_Prob: 0.9591\n",
      "[50] Token: \u001b[48;2;0;143;230m      ':'      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0410  PPL_Prob: 0.9599\n",
      "[51] Token: \u001b[48;2;0;143;230m     '\\n'      \u001b[0m | LogProb:  -0.0003  Prob:  0.9995  | AvgNLL: 0.0402  PPL_Prob: 0.9606\n",
      "[52] Token: \u001b[48;2;0;143;230m     '[['      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0394  PPL_Prob: 0.9613\n",
      "[53] Token: \u001b[48;2;0;143;230m      '1'      \u001b[0m | LogProb:  -0.1687  Prob:  0.8447  | AvgNLL: 0.0418  PPL_Prob: 0.9591\n",
      "[54] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0076  Prob:  0.9922  | AvgNLL: 0.0412  PPL_Prob: 0.9596\n",
      "[55] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0005  Prob:  0.9995  | AvgNLL: 0.0405  PPL_Prob: 0.9603\n",
      "[56] Token: \u001b[48;2;0;143;230m      '2'      \u001b[0m | LogProb:  -0.2620  Prob:  0.7695  | AvgNLL: 0.0443  PPL_Prob: 0.9566\n",
      "[57] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0006  Prob:  0.9995  | AvgNLL: 0.0436  PPL_Prob: 0.9573\n",
      "[58] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0429  PPL_Prob: 0.9581\n",
      "[59] Token: \u001b[48;2;0;143;230m      '3'      \u001b[0m | LogProb:  -0.0068  Prob:  0.9932  | AvgNLL: 0.0423  PPL_Prob: 0.9586\n",
      "[60] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0012  Prob:  0.9985  | AvgNLL: 0.0416  PPL_Prob: 0.9593\n",
      "[61] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0409  PPL_Prob: 0.9599\n",
      "[62] Token: \u001b[48;2;0;143;230m      '4'      \u001b[0m | LogProb:  -0.0046  Prob:  0.9956  | AvgNLL: 0.0403  PPL_Prob: 0.9605\n",
      "[63] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0012  Prob:  0.9985  | AvgNLL: 0.0397  PPL_Prob: 0.9611\n",
      "[64] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0391  PPL_Prob: 0.9616\n",
      "[65] Token: \u001b[48;2;0;143;230m      '5'      \u001b[0m | LogProb:  -0.0003  Prob:  0.9995  | AvgNLL: 0.0385  PPL_Prob: 0.9622\n",
      "[66] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0898  Prob:  0.9141  | AvgNLL: 0.0393  PPL_Prob: 0.9615\n",
      "[67] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0387  PPL_Prob: 0.9620\n",
      "[68] Token: \u001b[48;2;0;143;230m      '6'      \u001b[0m | LogProb:  -0.0010  Prob:  0.9990  | AvgNLL: 0.0382  PPL_Prob: 0.9626\n",
      "[69] Token: \u001b[48;2;0;143;230m     ']]'      \u001b[0m | LogProb:  -0.3240  Prob:  0.7231  | AvgNLL: 0.0423  PPL_Prob: 0.9586\n",
      "[70] Token: \u001b[48;2;0;143;230m    '\\n\\n'     \u001b[0m | LogProb:  -0.0047  Prob:  0.9951  | AvgNLL: 0.0417  PPL_Prob: 0.9591\n",
      "[71] Token: \u001b[48;2;0;143;230m     '**'      \u001b[0m | LogProb:  -0.2703  Prob:  0.7632  | AvgNLL: 0.0449  PPL_Prob: 0.9561\n",
      "[72] Token: \u001b[48;2;0;143;230m    'Task'     \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0443  PPL_Prob: 0.9567\n",
      "[73] Token: \u001b[48;2;0;143;230m     ':**'     \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0437  PPL_Prob: 0.9573\n",
      "[74] Token: \u001b[48;2;0;143;230m     '\\n'      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.0431  PPL_Prob: 0.9578\n",
      "[75] Token: \u001b[48;2;0;143;230m    'Based'    \u001b[0m | LogProb:  -0.0012  Prob:  0.9990  | AvgNLL: 0.0426  PPL_Prob: 0.9583\n",
      "[76] Token: \u001b[48;2;0;143;230m     ' on'     \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0420  PPL_Prob: 0.9589\n",
      "[77] Token: \u001b[48;2;0;143;230m    ' the'     \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0415  PPL_Prob: 0.9594\n",
      "[78] Token: \u001b[48;2;0;143;230m     ' `'      \u001b[0m | LogProb:  -0.0003  Prob:  0.9995  | AvgNLL: 0.0409  PPL_Prob: 0.9599\n",
      "[79] Token: \u001b[48;2;0;143;230m    'train'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0404  PPL_Prob: 0.9604\n",
      "[80] Token: \u001b[48;2;0;143;230m      '`'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0399  PPL_Prob: 0.9609\n",
      "[81] Token: \u001b[48;2;0;143;230m  ' examples'  \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0394  PPL_Prob: 0.9613\n",
      "[82] Token: \u001b[48;2;0;143;230m   ' above'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0390  PPL_Prob: 0.9618\n",
      "[83] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0385  PPL_Prob: 0.9622\n",
      "[84] Token: \u001b[48;2;0;143;230m  ' generate'  \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0381  PPL_Prob: 0.9627\n",
      "[85] Token: \u001b[48;2;0;143;230m    ' the'     \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0376  PPL_Prob: 0.9631\n",
      "[86] Token: \u001b[48;2;0;143;230m  ' expected'  \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0372  PPL_Prob: 0.9635\n",
      "[87] Token: \u001b[48;2;0;143;230m     ' `'      \u001b[0m | LogProb:  -0.0003  Prob:  0.9995  | AvgNLL: 0.0368  PPL_Prob: 0.9639\n",
      "[88] Token: \u001b[48;2;0;143;230m   'output'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0363  PPL_Prob: 0.9643\n",
      "[89] Token: \u001b[48;2;0;143;230m      '`'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0359  PPL_Prob: 0.9647\n",
      "[90] Token: \u001b[48;2;0;143;230m   ' array'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0355  PPL_Prob: 0.9651\n",
      "[91] Token: \u001b[48;2;0;143;230m    ' for'     \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0352  PPL_Prob: 0.9654\n",
      "[92] Token: \u001b[48;2;0;143;230m    ' this'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0348  PPL_Prob: 0.9658\n",
      "[93] Token: \u001b[48;2;0;143;230m    ' test'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0344  PPL_Prob: 0.9662\n",
      "[94] Token: \u001b[48;2;0;143;230m   ' input'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0341  PPL_Prob: 0.9665\n",
      "[95] Token: \u001b[48;2;0;143;230m      '.'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0337  PPL_Prob: 0.9669\n",
      "[96] Token: \u001b[48;2;0;143;230m     '\\n'      \u001b[0m | LogProb:  -0.0077  Prob:  0.9922  | AvgNLL: 0.0334  PPL_Prob: 0.9671\n",
      "[97] Token: \u001b[48;2;0;143;230m   'Output'    \u001b[0m | LogProb:  -0.0003  Prob:  0.9995  | AvgNLL: 0.0331  PPL_Prob: 0.9675\n",
      "[98] Token: \u001b[48;2;0;143;230m      ':'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0328  PPL_Prob: 0.9678\n",
      "[99] Token: \u001b[48;2;0;143;230m     '\\n'      \u001b[0m | LogProb:  -0.0003  Prob:  0.9995  | AvgNLL: 0.0324  PPL_Prob: 0.9681\n",
      "\n",
      "== Self Perplexity ==\n",
      "Avg NLL: 0.0324\n",
      "Perplexity (self): 0.9681\n",
      "\n",
      "--- Prompt 4 (key: 5582e5ca, length: 814) ---\n",
      "\n",
      "Input: The following examples show 2D input arrays and their corresponding output arrays. Each output is generated by applying a specific pattern or rule to the input.\n",
      "\n",
      "- Please study the relationship between each `input` and its corresponding `output` in the `train` examples.\n",
      "- Then, based on these patterns, predict the `output` for the given `test` input.\n",
      "\n",
      "### Example 1\n",
      "Input:\n",
      "[[6, 8, 9], [1, 8, 1], [9, 4, 9]]\n",
      "\n",
      "Output:\n",
      "[[9, 9, 9], [9, 9, 9], [9, 9, 9]]\n",
      "\n",
      "### Example 2\n",
      "Input:\n",
      "[[4, 4, 8], [6, 4, 3], [6, 3, 0]]\n",
      "\n",
      "Output:\n",
      "[[4, 4, 4], [4, 4, 4], [4, 4, 4]]\n",
      "\n",
      "### Example 3\n",
      "Input:\n",
      "[[4, 6, 9], [6, 4, 1], [8, 8, 6]]\n",
      "\n",
      "Output:\n",
      "[[6, 6, 6], [6, 6, 6], [6, 6, 6]]\n",
      "\n",
      "---\n",
      "\n",
      "### Test Input 1:\n",
      "[[8, 8, 6], [4, 6, 9], [8, 3, 0]]\n",
      "\n",
      "**Task:**\n",
      "Based on the `train` examples above, generate the expected `output` array for this test input.\n",
      "\n",
      "\n",
      "[00] Token: \u001b[48;2;50;161;178m   'Output'    \u001b[0m | LogProb:  -0.3284  Prob:  0.7202  | AvgNLL: 0.3284  PPL_Prob: 0.7201\n",
      "[01] Token: \u001b[48;2;25;152;204m      ':'      \u001b[0m | LogProb:  -0.0014  Prob:  0.9985  | AvgNLL: 0.1649  PPL_Prob: 0.8480\n",
      "[02] Token: \u001b[48;2;25;152;204m     '\\n'      \u001b[0m | LogProb:  -0.0445  Prob:  0.9565  | AvgNLL: 0.1247  PPL_Prob: 0.8827\n",
      "[03] Token: \u001b[48;2;25;152;204m     '[['      \u001b[0m | LogProb:  -0.0712  Prob:  0.9312  | AvgNLL: 0.1114  PPL_Prob: 0.8946\n",
      "[04] Token: \u001b[48;2;0;143;230m      '8'      \u001b[0m | LogProb:  -0.0350  Prob:  0.9658  | AvgNLL: 0.0961  PPL_Prob: 0.9084\n",
      "[05] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0003  Prob:  0.9995  | AvgNLL: 0.0801  PPL_Prob: 0.9230\n",
      "[06] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0004  Prob:  0.9995  | AvgNLL: 0.0687  PPL_Prob: 0.9336\n",
      "[07] Token: \u001b[48;2;0;143;230m      '8'      \u001b[0m | LogProb:  -0.0011  Prob:  0.9990  | AvgNLL: 0.0603  PPL_Prob: 0.9415\n",
      "[08] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.0536  PPL_Prob: 0.9478\n",
      "[09] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.0483  PPL_Prob: 0.9529\n",
      "[10] Token: \u001b[48;2;0;143;230m      '6'      \u001b[0m | LogProb:  -0.0033  Prob:  0.9966  | AvgNLL: 0.0442  PPL_Prob: 0.9568\n",
      "[11] Token: \u001b[48;2;0;143;230m     '],'      \u001b[0m | LogProb:  -0.0005  Prob:  0.9995  | AvgNLL: 0.0405  PPL_Prob: 0.9603\n",
      "[12] Token: \u001b[48;2;0;143;230m     ' ['      \u001b[0m | LogProb:  -0.0005  Prob:  0.9995  | AvgNLL: 0.0374  PPL_Prob: 0.9632\n",
      "[13] Token: \u001b[48;2;0;143;230m      '4'      \u001b[0m | LogProb:  -0.0204  Prob:  0.9800  | AvgNLL: 0.0362  PPL_Prob: 0.9644\n",
      "[14] Token: \u001b[48;2;0;143;230m      ','      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0338  PPL_Prob: 0.9668\n",
      "[15] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0317  PPL_Prob: 0.9688\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Prompt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) ---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 여기에 모델 호출 함수 넣기 (예시로 history 사용)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_with_self_confidence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 결과 저장\u001b[39;00m\n\u001b[0;32m     16\u001b[0m results[key] \u001b[38;5;241m=\u001b[39m history\n",
      "Cell \u001b[1;32mIn[29], line 13\u001b[0m, in \u001b[0;36mgenerate_with_self_confidence\u001b[1;34m(tokenizer, model, input_text, max_new_tokens, field_width)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[1;32m---> 13\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# 마지막 토큰 logits\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     log_softmax \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\utils\\generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\models\\gemma3\\modeling_gemma3.py:682\u001b[0m, in \u001b[0;36mGemma3ForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[0;32m    678\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    679\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m    680\u001b[0m )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 682\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    695\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    696\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\utils\\generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\models\\gemma3\\modeling_gemma3.py:566\u001b[0m, in \u001b[0;36mGemma3TextModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    564\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m--> 566\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings_global\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings_global\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\models\\gemma3\\modeling_gemma3.py:385\u001b[0m, in \u001b[0;36mGemma3DecoderLayer.forward\u001b[1;34m(self, hidden_states, position_embeddings_global, position_embeddings_local, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    383\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m position_embeddings_global\n\u001b[1;32m--> 385\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    396\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m    397\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\transformers\\models\\gemma3\\modeling_gemma3.py:311\u001b[0m, in \u001b[0;36mGemma3Attention.forward\u001b[1;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    309\u001b[0m hidden_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m--> 311\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    312\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    313\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:1004\u001b[0m, in \u001b[0;36mLinear8bitLt.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[0;32m   1002\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m-> 1004\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mhas_fp16_weights \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1007\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCB\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:369\u001b[0m, in \u001b[0;36mmatmul\u001b[1;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threshold \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    368\u001b[0m     state\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m=\u001b[39m threshold\n\u001b[1;32m--> 369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul8bitLt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\torch\\autograd\\function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    583\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:196\u001b[0m, in \u001b[0;36mMatMul8bitLt.forward\u001b[1;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[0;32m    193\u001b[0m     CA, CAt, SCA, SCAt, outlier_cols \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mint8_double_quant(A\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat16), threshold\u001b[38;5;241m=\u001b[39mstate\u001b[38;5;241m.\u001b[39mthreshold)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# Fast path\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     CA, SCA, outlier_cols \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8_vectorwise_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     CAt \u001b[38;5;241m=\u001b[39m SCAt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    199\u001b[0m has_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\bitsandbytes\\functional.py:2245\u001b[0m, in \u001b[0;36mint8_vectorwise_quant\u001b[1;34m(A, threshold)\u001b[0m\n\u001b[0;32m   2227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mint8_vectorwise_quant\u001b[39m(A: torch\u001b[38;5;241m.\u001b[39mTensor, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m):\n\u001b[0;32m   2228\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Quantizes a tensor with dtype `torch.float16` to `torch.int8` in accordance to the `LLM.int8()` algorithm.\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m \n\u001b[0;32m   2230\u001b[0m \u001b[38;5;124;03m    For more information, see the [LLM.int8() paper](https://arxiv.org/abs/2208.07339).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2243\u001b[0m \u001b[38;5;124;03m        - `torch.Tensor` with dtype `torch.int32`, *optional*: A list of column indices which contain outlier features.\u001b[39;00m\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbitsandbytes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8_vectorwise_quant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\torch\\_ops.py:716\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\torch\\_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[0;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:634\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m--> 634\u001b[0m     \u001b[43m_maybe_set_eval_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprior\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch_env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:101\u001b[0m, in \u001b[0;36m_maybe_set_eval_frame\u001b[1;34m(callback)\u001b[0m\n\u001b[0;32m     96\u001b[0m cached_backends: Dict[\u001b[38;5;28mint\u001b[39m, CompilerFn] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     98\u001b[0m unset \u001b[38;5;241m=\u001b[39m Unset\u001b[38;5;241m.\u001b[39mtoken\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_set_eval_frame\u001b[39m(callback: DynamoCallback):\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;66;03m# A wrapper on set_eval_frame that is guarded by a Justknob.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# Users can disable torchDynamo by setting the JK to False.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval_frame\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_eval_frame\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m justknobs_check(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch/compiler:enable_compiler_set_eval_frame\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for i, (key, prompt) in enumerate(prompt_list[:10]):\n",
    "    print(f\"\\n--- Prompt {i+1} (key: {key}, length: {len(prompt)}) ---\\n\")\n",
    "    \n",
    "    # 여기에 모델 호출 함수 넣기 (예시로 history 사용)\n",
    "    history = generate_with_self_confidence(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        input_text=prompt,\n",
    "        max_new_tokens=100,\n",
    "        field_width=15,\n",
    "    )\n",
    "    \n",
    "    # 결과 저장\n",
    "    results[key] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4254b48-5ec1-4662-b717-c99d7a9ac9bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "questions = {\n",
    "    \"Easy\": [\n",
    "        \"Is the Moon Earth's satellite?\",\n",
    "        \"What is 3 + 4?\",\n",
    "        \"Is water H2O?\",\n",
    "        \"Is the Sun a star?\",\n",
    "        \"Are cats mammals?\"\n",
    "    ],\n",
    "    \"Tricky\": [\n",
    "        \"If you say “I am lying,” are you telling the truth?\",\n",
    "        # \"How many months have 28 days?\",\n",
    "        \"If you are your brother's sibling, who are you?\",\n",
    "        # \"If a clock is one hour fast after one hour, what time is it now?\",\n",
    "        \"In a dark room with a candle, a lantern, and a match, which do you light first?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for difficulty, qs in questions.items():\n",
    "    print(f\"\\n--- {difficulty} Questions ---\\n\")\n",
    "    results[difficulty] = []\n",
    "    for q in qs:\n",
    "        history = generate_with_self_confidence(tokenizer, model, \n",
    "                                                q, \n",
    "                                                max_new_tokens=100, \n",
    "                                                field_width=15)\n",
    "        # results[difficulty].append((q, history))\n",
    "        # print(f\"Q: {q}\\nA: {history}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a0d407-6e71-4814-9460-b0bbc85af7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference_loop(tokenizer, model, \n",
    "                   \"Is the Moon Earth's satellite?\", \n",
    "                   max_new_tokens=50\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a672c5-1ba8-43b7-8e20-4b6aca205110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
