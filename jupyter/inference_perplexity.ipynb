{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85995f60-1839-4667-ac9e-d8c7d5ee3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig, Gemma3ForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19bf1bc0-cc18-4e59-9677-9ac0acdf582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "530c6411-4583-4cf8-94a0-3791fe7711b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_model(model_id=\"Qwen/Qwen3-0.6B\", dtype=torch.float16):\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_id,\n",
    "#         device_map=\"auto\",\n",
    "#         torch_dtype=dtype\n",
    "#     )\n",
    "#     model.eval()\n",
    "#     return tokenizer, model\n",
    "\n",
    "def load_model(\n",
    "    model_id=\"google/gemma-3-1b-it\",\n",
    "    dtype=torch.float16,\n",
    "    load_in_8bit=True,\n",
    "):\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=load_in_8bit)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = Gemma3ForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=dtype,\n",
    "        quantization_config=quantization_config if load_in_8bit else None,\n",
    "    )\n",
    "\n",
    "    # <end_of_turn> 토큰을 eos_token으로 등록 (토크나이저에 없으면 추가)\n",
    "    if '<end_of_turn>' not in tokenizer.get_vocab():\n",
    "        tokenizer.add_special_tokens({'eos_token': '<end_of_turn>'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    else:\n",
    "        # 이미 있으면 eos_token으로 지정만 해줌\n",
    "        tokenizer.eos_token = '<end_of_turn>'\n",
    "\n",
    "    model.eval()\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13c0c08d-9b7e-49eb-9361-ddd3911270f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_model()\n",
    "model.tokenizer = tokenizer  # decode에 사용하기 위해 tokenizer 주입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "837da4c7-d664-40a9-b3e7-0b6978c87139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_ansi(r, g, b):\n",
    "    return f\"\\033[38;2;{r};{g};{b}m\"\n",
    "\n",
    "def bg_rgb_ansi(r, g, b):\n",
    "    return f\"\\033[48;2;{r};{g};{b}m\"  # 배경색\n",
    "\n",
    "def color_by_prob(text, prob):\n",
    "    \"\"\"\n",
    "    확률(prob) 값에 따라 색상을 입힌 문자열 반환\n",
    "    높은 확률일수록 초록색, 낮을수록 빨간색 계열\n",
    "    \"\"\"\n",
    "    if prob >= 0.5:\n",
    "        color = \"\\033[92m\"  # 밝은 초록 (high prob)\n",
    "    elif prob >= 0.2:\n",
    "        color = \"\\033[93m\"  # 밝은 노랑 (medium prob)\n",
    "    else:\n",
    "        color = \"\\033[91m\"  # 밝은 빨강 (low prob)\n",
    "\n",
    "    reset = \"\\033[0m\"\n",
    "    return f\"{color}{text}{reset}\"\n",
    "\n",
    "def get_yellow_to_green_ansi(prob):\n",
    "    \"\"\"\n",
    "    확률에 따라 진한 노랑(255,255,0) → 초록(0,255,0) 색상 매핑\n",
    "    \"\"\"\n",
    "    steps = 10\n",
    "    index = min(int(prob * steps), steps - 1)\n",
    "\n",
    "    r_start, g, b = 255, 255, 0  # 시작 색 (노랑)\n",
    "    r_end = 0                   # 끝 색 (초록)\n",
    "    \n",
    "    # R을 선형적으로 감소\n",
    "    r = int(r_start - (r_start - r_end) * (index / (steps - 1)))\n",
    "    \n",
    "    return rgb_ansi(r, g, b)\n",
    "\n",
    "def get_yellow_to_green_bg_ansi(prob):\n",
    "    steps = 10\n",
    "    index = min(int(prob * steps), steps - 1)\n",
    "    r_start, g, b = 255, 255, 0\n",
    "    r = int(r_start - (r_start * index / (steps - 1)))\n",
    "    return bg_rgb_ansi(r, g, b)\n",
    "\n",
    "def get_yellow_to_green_bg_ansi(prob):\n",
    "    steps = 10\n",
    "    index = min(int(prob * steps), steps - 1)\n",
    "    r_start, g, b = 255, 255, 0\n",
    "    r = int(r_start - (r_start * index / (steps - 1)))\n",
    "    return bg_rgb_ansi(r, g, b)\n",
    "\n",
    "def get_yellow_to_brightblue_bg_ansi(prob):\n",
    "    \"\"\"\n",
    "    prob: 0.0(노랑) → 1.0(밝은 파랑) 배경색 10단계 반환\n",
    "    밝은 파랑: #6699FF (R=102,G=153,B=255)\n",
    "    노랑: #FFFF00 (R=255,G=255,B=0)\n",
    "    \"\"\"\n",
    "    steps = 10\n",
    "    index = min(int(prob * steps), steps - 1)\n",
    "    \n",
    "    r_start, g_start, b_start = 255, 255, 0       # 노랑\n",
    "    r_end, g_end, b_end = 102, 153, 255           # 밝은 파랑\n",
    "    \n",
    "    r = int(r_start + (r_end - r_start) * index / (steps - 1))\n",
    "    g = int(g_start + (g_end - g_start) * index / (steps - 1))\n",
    "    b = int(b_start + (b_end - b_start) * index / (steps - 1))\n",
    "    \n",
    "    return bg_rgb_ansi(r, g, b)\n",
    "\n",
    "def get_yellow_to_lightblue_bg_ansi(prob):\n",
    "    \"\"\"\n",
    "    prob: 0.0 (#E5E200 노랑) → 1.0 (#008FE6 파랑) 배경색 10단계 선형 보간\n",
    "    \"\"\"\n",
    "    steps = 10\n",
    "    index = min(int(prob * steps), steps - 1)\n",
    "\n",
    "    r_start, g_start, b_start = 229, 226, 0     # #E5E200 노랑\n",
    "    r_end, g_end, b_end = 0, 143, 230           # #008FE6 파랑\n",
    "\n",
    "    r = int(r_start + (r_end - r_start) * index / (steps - 1))\n",
    "    g = int(g_start + (g_end - g_start) * index / (steps - 1))\n",
    "    b = int(b_start + (b_end - b_start) * index / (steps - 1))\n",
    "\n",
    "    return bg_rgb_ansi(r, g, b)\n",
    "\n",
    "\n",
    "def bg_rgb_ansi(r, g, b):\n",
    "    return f\"\\033[48;2;{r};{g};{b}m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e99b176-8288-400f-b82d-f70789f36d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_tokens(top_tokens, field_width=22):\n",
    "    \"\"\"Top-5 토큰 정보를 가운데 정렬 + 배경색 노랑→초록, 글자색 기본 유지\"\"\"\n",
    "\n",
    "    formatted = []\n",
    "    for _, token_str, prob in top_tokens:\n",
    "        token_display = f\"{repr(token_str)} ({prob:.4f})\"\n",
    "        padded = f\"{token_display:^{field_width}}\"   # 먼저 순수 텍스트로 정렬\n",
    "        bg_color = get_yellow_to_lightblue_bg_ansi(prob) # 외부 정의 함수 호출\n",
    "        colored = f\"{bg_color}{padded}\\033[0m\"       # 색상 코드 감싸기\n",
    "        formatted.append(colored)\n",
    "\n",
    "    print(\"== Top-5 Tokens ==\")\n",
    "    print(\" | \".join(formatted))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50203565-f3f4-4dbc-9941-bd4679102e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token(model, input_ids):\n",
    "    \"\"\"다음 토큰과 top-5 후보 반환\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    next_token_logits = logits[0, -1, :]\n",
    "    probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "    # Top-5 토큰 추출\n",
    "    top_probs, top_ids = torch.topk(probs, k=5)\n",
    "    top_tokens = [(token_id.item(), token_str, top_probs[i].item())\n",
    "                  for i, token_id in enumerate(top_ids)\n",
    "                  for token_str in [model.tokenizer.decode(token_id)]]\n",
    "\n",
    "    return top_tokens\n",
    "\n",
    "def run_inference_loop(tokenizer, model, input_text, max_new_tokens=20):\n",
    "    \"\"\"전체 생성 루프 실행\"\"\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    generated = input_ids\n",
    "\n",
    "    print(f\"Input: {input_text}\\n\")\n",
    "\n",
    "    for step in range(max_new_tokens):\n",
    "        top_tokens = generate_next_token(model, generated)\n",
    "        print_top_tokens(top_tokens)\n",
    "\n",
    "        # 다음 토큰 선택 (top-1 기반, greedy)\n",
    "        next_token_id = torch.tensor([[top_tokens[0][0]]], device=model.device)\n",
    "        generated = torch.cat([generated, next_token_id], dim=1)\n",
    "\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    output_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    print(\"== Final Output ==\")\n",
    "    print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8727721-5b42-4eb5-ac17-ded8ee6791a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_from_logits(tokenizer, model, input_text):\n",
    "    \"\"\"logits를 사용하여 perplexity 직접 계산\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "        input_ids = inputs.input_ids.to(model.device)\n",
    "\n",
    "        # 모델 예측 logits\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        # 정답 label은 input_ids를 한 칸 오른쪽으로 이동\n",
    "        shift_logits = logits[:, :-1, :]         # [batch, seq-1, vocab]\n",
    "        shift_labels = input_ids[:, 1:]          # [batch, seq-1]\n",
    "\n",
    "        # CrossEntropy 계산: log_softmax 후 정답 토큰의 log prob 추출\n",
    "        log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "        shift_labels = shift_labels.unsqueeze(-1)  # [batch, seq-1, 1]\n",
    "\n",
    "        # gather로 정답 토큰의 log prob 추출\n",
    "        token_log_probs = log_probs.gather(dim=-1, index=shift_labels).squeeze(-1)  # [batch, seq-1]\n",
    "\n",
    "        # 평균 negative log likelihood\n",
    "        nll = -token_log_probs.mean()\n",
    "        perplexity = torch.exp(nll)\n",
    "\n",
    "        print(f\"NLL: {nll.item():.4f}\")\n",
    "        print(f\"Perplexity: {perplexity.item():.4f}\")\n",
    "        return perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4db778d-4fe2-4726-8e9d-3b902ea5f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_inference_loop(tokenizer, model, \"What is 2 + 2? Answer briefly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54674bf9-d390-44cc-ba10-b67bbfa97bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_self_confidence(tokenizer, model, input_text, max_new_tokens=20, field_width=20):\n",
    "    import math\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    generated = input_ids\n",
    "    log_probs = []\n",
    "    history = []\n",
    "\n",
    "    print(f\"Input: {input_text}\\n\")\n",
    "\n",
    "    for step in range(max_new_tokens):\n",
    "        outputs = model(generated)\n",
    "        logits = outputs.logits[:, -1, :]  # 마지막 토큰 logits\n",
    "\n",
    "        log_softmax = F.log_softmax(logits, dim=-1)\n",
    "        next_token_id = torch.argmax(log_softmax, dim=-1)\n",
    "        next_log_prob = log_softmax[0, next_token_id]\n",
    "        prob = next_log_prob.exp().item()\n",
    "\n",
    "        log_probs.append(next_log_prob.item())\n",
    "        avg_nll = -sum(log_probs) / len(log_probs)\n",
    "        avg_prob = math.exp(-avg_nll)\n",
    "\n",
    "        token_str = repr(tokenizer.decode([next_token_id.item()]))\n",
    "\n",
    "        # 배경색 적용\n",
    "        bg_color = get_yellow_to_lightblue_bg_ansi(avg_prob)\n",
    "        padded = f\"{token_str:^{field_width}}\"\n",
    "        colored_token = f\"{bg_color}{padded}\\033[0m\"\n",
    "\n",
    "        print(\n",
    "            f\"[{step:02d}] Token: {colored_token} | \"\n",
    "            f\"LogProb: {next_log_prob.item():>8.4f}  Prob: {prob:>7.4f}  | \"\n",
    "            f\"AvgNLL: {avg_nll:.4f}  PPL_Prob: {avg_prob:.4f}\"\n",
    "        )\n",
    "\n",
    "        history.append({\n",
    "            \"step\": step,\n",
    "            \"token_id\": next_token_id.item(),\n",
    "            \"token_str\": token_str,\n",
    "            \"log_prob\": next_log_prob.item(),\n",
    "            \"prob\": prob,\n",
    "            \"avg_nll\": avg_nll,\n",
    "            \"ppl_prob\": avg_prob,\n",
    "            \"colored_token\": colored_token,  # 기록도 색칠 토큰 포함\n",
    "        })\n",
    "\n",
    "        generated = torch.cat([generated, next_token_id.unsqueeze(0)], dim=1)\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    print(\"\\n== Self Perplexity ==\")\n",
    "    print(f\"Avg NLL: {avg_nll:.4f}\")\n",
    "    print(f\"Perplexity (self): {avg_prob:.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def print_generation_history(history, field_width=20):\n",
    "    print(\"\\n=== Generation History ===\")\n",
    "    for entry in history:\n",
    "        # 이미 색칠된 토큰을 기록에서 재사용 (있으면)\n",
    "        colored_token = entry.get(\"colored_token\")\n",
    "        if not colored_token:\n",
    "            # 없으면 새로 색칠 (fallback)\n",
    "            bg_color = get_yellow_to_lightblue_bg_ansi(entry['ppl_prob'])\n",
    "            padded = f\"{entry['token_str'] :^{field_width}}\"\n",
    "            colored_token = f\"{bg_color}{padded}\\033[0m\"\n",
    "\n",
    "        print(\n",
    "            f\"[{entry['step']:02d}] Token: {colored_token} | \"\n",
    "            f\"LogProb: {entry['log_prob']:8.4f}  Prob: {entry['prob']:7.4f}  | \"\n",
    "            f\"AvgNLL: {entry['avg_nll']:.4f}  PPL_Prob: {entry['ppl_prob']:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47ccaec4-bc19-4bd1-9bb3-14ee2198b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_with_self_confidence(tokenizer, model, \n",
    "#                               \"What is 4 + 4? Answer briefly.\", \n",
    "#                               max_new_tokens=50,\n",
    "#                               field_width=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d283f86-a8bd-4fb0-ae09-a74336e2c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = generate_with_self_confidence(tokenizer, model, \n",
    "                                        \"What is 2 + 2?\", \n",
    "                                        max_new_tokens=20,\n",
    "                                        field_width=15)\n",
    "# print_generation_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4254b48-5ec1-4662-b717-c99d7a9ac9bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Easy Questions ---\n",
      "\n",
      "Input: Is the Moon Earth's satellite?\n",
      "\n",
      "[00] Token: \u001b[48;2;25;152;204m    '\\n\\n'     \u001b[0m | LogProb:  -0.1418  Prob:  0.8677  | AvgNLL: 0.1418  PPL_Prob: 0.8678\n",
      "[01] Token: \u001b[48;2;127;189;102m     'The'     \u001b[0m | LogProb:  -1.4736  Prob:  0.2291  | AvgNLL: 0.8077  PPL_Prob: 0.4459\n",
      "[02] Token: \u001b[48;2;127;189;102m   ' answer'   \u001b[0m | LogProb:  -0.8120  Prob:  0.4438  | AvgNLL: 0.8092  PPL_Prob: 0.4452\n",
      "[03] Token: \u001b[48;2;101;179;127m     ' is'     \u001b[0m | LogProb:  -0.0242  Prob:  0.9761  | AvgNLL: 0.6129  PPL_Prob: 0.5418\n",
      "[04] Token: \u001b[48;2;101;179;127m     ' a'      \u001b[0m | LogProb:  -0.5020  Prob:  0.6055  | AvgNLL: 0.5907  PPL_Prob: 0.5539\n",
      "[05] Token: \u001b[48;2;76;170;153m ' resounding' \u001b[0m | LogProb:  -0.0490  Prob:  0.9521  | AvgNLL: 0.5004  PPL_Prob: 0.6063\n",
      "[06] Token: \u001b[48;2;76;170;153m     ' **'     \u001b[0m | LogProb:  -0.1910  Prob:  0.8262  | AvgNLL: 0.4562  PPL_Prob: 0.6337\n",
      "[07] Token: \u001b[48;2;76;170;153m     'no'      \u001b[0m | LogProb:  -0.5679  Prob:  0.5669  | AvgNLL: 0.4702  PPL_Prob: 0.6249\n",
      "[08] Token: \u001b[48;2;76;170;153m     '**.'     \u001b[0m | LogProb:  -0.0464  Prob:  0.9546  | AvgNLL: 0.4231  PPL_Prob: 0.6550\n",
      "[09] Token: \u001b[48;2;76;170;153m    '\\n\\n'     \u001b[0m | LogProb:  -0.3940  Prob:  0.6743  | AvgNLL: 0.4202  PPL_Prob: 0.6569\n",
      "[10] Token: \u001b[48;2;76;170;153m     'The'     \u001b[0m | LogProb:  -0.4980  Prob:  0.6079  | AvgNLL: 0.4273  PPL_Prob: 0.6523\n",
      "[11] Token: \u001b[48;2;76;170;153m    ' Moon'    \u001b[0m | LogProb:  -0.0023  Prob:  0.9976  | AvgNLL: 0.3919  PPL_Prob: 0.6758\n",
      "[12] Token: \u001b[48;2;76;170;153m     ' is'     \u001b[0m | LogProb:  -0.0177  Prob:  0.9824  | AvgNLL: 0.3631  PPL_Prob: 0.6955\n",
      "[13] Token: \u001b[48;2;50;161;178m     ' a'      \u001b[0m | LogProb:  -0.0067  Prob:  0.9932  | AvgNLL: 0.3376  PPL_Prob: 0.7135\n",
      "[14] Token: \u001b[48;2;50;161;178m  ' natural'   \u001b[0m | LogProb:  -0.0196  Prob:  0.9805  | AvgNLL: 0.3164  PPL_Prob: 0.7288\n",
      "[15] Token: \u001b[48;2;50;161;178m ' satellite'  \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2966  PPL_Prob: 0.7433\n",
      "[16] Token: \u001b[48;2;50;161;178m     ' of'     \u001b[0m | LogProb:  -0.1138  Prob:  0.8926  | AvgNLL: 0.2859  PPL_Prob: 0.7513\n",
      "[17] Token: \u001b[48;2;50;161;178m   ' Earth'    \u001b[0m | LogProb:  -0.0671  Prob:  0.9351  | AvgNLL: 0.2737  PPL_Prob: 0.7605\n",
      "[18] Token: \u001b[48;2;50;161;178m      '.'      \u001b[0m | LogProb:  -0.4888  Prob:  0.6133  | AvgNLL: 0.2851  PPL_Prob: 0.7520\n",
      "[19] Token: \u001b[48;2;50;161;178m     ' It'     \u001b[0m | LogProb:  -0.2554  Prob:  0.7744  | AvgNLL: 0.2836  PPL_Prob: 0.7531\n",
      "[20] Token: \u001b[48;2;50;161;178m   ' orbits'   \u001b[0m | LogProb:  -0.2539  Prob:  0.7759  | AvgNLL: 0.2822  PPL_Prob: 0.7542\n",
      "[21] Token: \u001b[48;2;50;161;178m    ' our'     \u001b[0m | LogProb:  -0.6699  Prob:  0.5117  | AvgNLL: 0.2998  PPL_Prob: 0.7410\n",
      "[22] Token: \u001b[48;2;50;161;178m   ' planet'   \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2867  PPL_Prob: 0.7507\n",
      "[23] Token: \u001b[48;2;50;161;178m      ','      \u001b[0m | LogProb:  -0.2173  Prob:  0.8047  | AvgNLL: 0.2839  PPL_Prob: 0.7529\n",
      "[24] Token: \u001b[48;2;50;161;178m    ' and'     \u001b[0m | LogProb:  -0.1223  Prob:  0.8848  | AvgNLL: 0.2774  PPL_Prob: 0.7578\n",
      "[25] Token: \u001b[48;2;50;161;178m     ' it'     \u001b[0m | LogProb:  -0.9097  Prob:  0.4026  | AvgNLL: 0.3017  PPL_Prob: 0.7396\n",
      "[26] Token: \u001b[48;2;50;161;178m      \"'\"      \u001b[0m | LogProb:  -0.4985  Prob:  0.6074  | AvgNLL: 0.3090  PPL_Prob: 0.7342\n",
      "[27] Token: \u001b[48;2;50;161;178m      's'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2980  PPL_Prob: 0.7423\n",
      "[28] Token: \u001b[48;2;50;161;178m     ' a'      \u001b[0m | LogProb:  -0.7681  Prob:  0.4639  | AvgNLL: 0.3142  PPL_Prob: 0.7304\n",
      "[29] Token: \u001b[48;2;76;170;153m' fascinating' \u001b[0m | LogProb:  -1.6104  Prob:  0.1998  | AvgNLL: 0.3574  PPL_Prob: 0.6995\n",
      "[30] Token: \u001b[48;2;76;170;153m   ' object'   \u001b[0m | LogProb:  -1.2666  Prob:  0.2817  | AvgNLL: 0.3867  PPL_Prob: 0.6793\n",
      "[31] Token: \u001b[48;2;76;170;153m    ' that'    \u001b[0m | LogProb:  -0.7129  Prob:  0.4902  | AvgNLL: 0.3969  PPL_Prob: 0.6724\n",
      "[32] Token: \u001b[48;2;76;170;153m    ' has'     \u001b[0m | LogProb:  -0.5391  Prob:  0.5835  | AvgNLL: 0.4012  PPL_Prob: 0.6695\n",
      "[33] Token: \u001b[48;2;76;170;153m  ' captured'  \u001b[0m | LogProb:  -0.8994  Prob:  0.4067  | AvgNLL: 0.4159  PPL_Prob: 0.6598\n",
      "[34] Token: \u001b[48;2;76;170;153m    ' the'     \u001b[0m | LogProb:  -0.9795  Prob:  0.3755  | AvgNLL: 0.4320  PPL_Prob: 0.6492\n",
      "[35] Token: \u001b[48;2;76;170;153m' imagination' \u001b[0m | LogProb:  -0.5288  Prob:  0.5894  | AvgNLL: 0.4347  PPL_Prob: 0.6475\n",
      "[36] Token: \u001b[48;2;76;170;153m     ' of'     \u001b[0m | LogProb:  -0.0871  Prob:  0.9165  | AvgNLL: 0.4253  PPL_Prob: 0.6536\n",
      "[37] Token: \u001b[48;2;76;170;153m   ' people'   \u001b[0m | LogProb:  -0.5918  Prob:  0.5532  | AvgNLL: 0.4296  PPL_Prob: 0.6507\n",
      "[38] Token: \u001b[48;2;76;170;153m    ' for'     \u001b[0m | LogProb:  -0.0164  Prob:  0.9839  | AvgNLL: 0.4191  PPL_Prob: 0.6577\n",
      "[39] Token: \u001b[48;2;76;170;153m ' centuries'  \u001b[0m | LogProb:  -0.7363  Prob:  0.4788  | AvgNLL: 0.4270  PPL_Prob: 0.6525\n",
      "[40] Token: \u001b[48;2;76;170;153m      '.'      \u001b[0m | LogProb:  -0.0240  Prob:  0.9761  | AvgNLL: 0.4172  PPL_Prob: 0.6589\n",
      "[41] Token: \u001b[48;2;76;170;153m    '\\n\\n'     \u001b[0m | LogProb:  -0.2283  Prob:  0.7959  | AvgNLL: 0.4127  PPL_Prob: 0.6619\n",
      "[42] Token: \u001b[48;2;76;170;153m    'Here'     \u001b[0m | LogProb:  -0.1898  Prob:  0.8271  | AvgNLL: 0.4075  PPL_Prob: 0.6653\n",
      "[43] Token: \u001b[48;2;76;170;153m      \"'\"      \u001b[0m | LogProb:  -0.1115  Prob:  0.8945  | AvgNLL: 0.4007  PPL_Prob: 0.6698\n",
      "[44] Token: \u001b[48;2;76;170;153m      's'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.3918  PPL_Prob: 0.6758\n",
      "[45] Token: \u001b[48;2;76;170;153m     ' a'      \u001b[0m | LogProb:  -0.0746  Prob:  0.9282  | AvgNLL: 0.3849  PPL_Prob: 0.6805\n",
      "[46] Token: \u001b[48;2;76;170;153m ' breakdown'  \u001b[0m | LogProb:  -0.3042  Prob:  0.7378  | AvgNLL: 0.3832  PPL_Prob: 0.6817\n",
      "[47] Token: \u001b[48;2;76;170;153m     ' of'     \u001b[0m | LogProb:  -0.2820  Prob:  0.7544  | AvgNLL: 0.3811  PPL_Prob: 0.6831\n",
      "[48] Token: \u001b[48;2;76;170;153m    ' why'     \u001b[0m | LogProb:  -0.0745  Prob:  0.9282  | AvgNLL: 0.3749  PPL_Prob: 0.6874\n",
      "[49] Token: \u001b[48;2;76;170;153m     ' it'     \u001b[0m | LogProb:  -0.4255  Prob:  0.6533  | AvgNLL: 0.3759  PPL_Prob: 0.6867\n",
      "[50] Token: \u001b[48;2;76;170;153m      \"'\"      \u001b[0m | LogProb:  -0.0630  Prob:  0.9390  | AvgNLL: 0.3697  PPL_Prob: 0.6909\n",
      "[51] Token: \u001b[48;2;76;170;153m      's'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.3626  PPL_Prob: 0.6958\n",
      "[52] Token: \u001b[48;2;76;170;153m    ' not'     \u001b[0m | LogProb:  -0.0813  Prob:  0.9219  | AvgNLL: 0.3573  PPL_Prob: 0.6995\n",
      "[53] Token: \u001b[48;2;50;161;178m   ' Earth'    \u001b[0m | LogProb:  -0.0067  Prob:  0.9932  | AvgNLL: 0.3508  PPL_Prob: 0.7041\n",
      "[54] Token: \u001b[48;2;50;161;178m      \"'\"      \u001b[0m | LogProb:  -0.0259  Prob:  0.9746  | AvgNLL: 0.3449  PPL_Prob: 0.7083\n",
      "[55] Token: \u001b[48;2;50;161;178m      's'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.3388  PPL_Prob: 0.7127\n",
      "[56] Token: \u001b[48;2;50;161;178m ' satellite'  \u001b[0m | LogProb:  -0.0122  Prob:  0.9878  | AvgNLL: 0.3330  PPL_Prob: 0.7167\n",
      "[57] Token: \u001b[48;2;50;161;178m      ':'      \u001b[0m | LogProb:  -0.0173  Prob:  0.9829  | AvgNLL: 0.3276  PPL_Prob: 0.7207\n",
      "[58] Token: \u001b[48;2;50;161;178m    '\\n\\n'     \u001b[0m | LogProb:  -0.0150  Prob:  0.9854  | AvgNLL: 0.3223  PPL_Prob: 0.7245\n",
      "[59] Token: \u001b[48;2;50;161;178m      '*'      \u001b[0m | LogProb:  -0.0006  Prob:  0.9995  | AvgNLL: 0.3169  PPL_Prob: 0.7284\n",
      "[60] Token: \u001b[48;2;50;161;178m     '   '     \u001b[0m | LogProb:  -0.3057  Prob:  0.7368  | AvgNLL: 0.3167  PPL_Prob: 0.7285\n",
      "[61] Token: \u001b[48;2;50;161;178m     '**'      \u001b[0m | LogProb:  -0.0016  Prob:  0.9985  | AvgNLL: 0.3117  PPL_Prob: 0.7322\n",
      "[62] Token: \u001b[48;2;50;161;178m    'Orbit'    \u001b[0m | LogProb:  -0.2622  Prob:  0.7695  | AvgNLL: 0.3109  PPL_Prob: 0.7328\n",
      "[63] Token: \u001b[48;2;50;161;178m     ':**'     \u001b[0m | LogProb:  -0.0393  Prob:  0.9614  | AvgNLL: 0.3066  PPL_Prob: 0.7359\n",
      "[64] Token: \u001b[48;2;50;161;178m    ' The'     \u001b[0m | LogProb:  -0.0051  Prob:  0.9946  | AvgNLL: 0.3020  PPL_Prob: 0.7393\n",
      "[65] Token: \u001b[48;2;50;161;178m    ' Moon'    \u001b[0m | LogProb:  -0.0019  Prob:  0.9980  | AvgNLL: 0.2974  PPL_Prob: 0.7427\n",
      "[66] Token: \u001b[48;2;50;161;178m   ' orbits'   \u001b[0m | LogProb:  -0.3928  Prob:  0.6753  | AvgNLL: 0.2989  PPL_Prob: 0.7417\n",
      "[67] Token: \u001b[48;2;50;161;178m   ' Earth'    \u001b[0m | LogProb:  -0.3069  Prob:  0.7358  | AvgNLL: 0.2990  PPL_Prob: 0.7416\n",
      "[68] Token: \u001b[48;2;50;161;178m      ','      \u001b[0m | LogProb:  -0.0230  Prob:  0.9771  | AvgNLL: 0.2950  PPL_Prob: 0.7445\n",
      "[69] Token: \u001b[48;2;50;161;178m   ' while'    \u001b[0m | LogProb:  -0.0768  Prob:  0.9263  | AvgNLL: 0.2919  PPL_Prob: 0.7469\n",
      "[70] Token: \u001b[48;2;50;161;178m   ' Earth'    \u001b[0m | LogProb:  -0.1440  Prob:  0.8657  | AvgNLL: 0.2898  PPL_Prob: 0.7484\n",
      "[71] Token: \u001b[48;2;50;161;178m   ' orbits'   \u001b[0m | LogProb:  -0.1879  Prob:  0.8286  | AvgNLL: 0.2884  PPL_Prob: 0.7495\n",
      "[72] Token: \u001b[48;2;50;161;178m    ' the'     \u001b[0m | LogProb:  -0.0016  Prob:  0.9985  | AvgNLL: 0.2844  PPL_Prob: 0.7524\n",
      "[73] Token: \u001b[48;2;50;161;178m    ' Sun'     \u001b[0m | LogProb:  -0.0226  Prob:  0.9775  | AvgNLL: 0.2809  PPL_Prob: 0.7551\n",
      "[74] Token: \u001b[48;2;50;161;178m      '.'      \u001b[0m | LogProb:  -0.0015  Prob:  0.9985  | AvgNLL: 0.2772  PPL_Prob: 0.7579\n",
      "[75] Token: \u001b[48;2;50;161;178m     '\\n'      \u001b[0m | LogProb:  -0.0270  Prob:  0.9731  | AvgNLL: 0.2739  PPL_Prob: 0.7604\n",
      "[76] Token: \u001b[48;2;50;161;178m      '*'      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.2703  PPL_Prob: 0.7631\n",
      "[77] Token: \u001b[48;2;50;161;178m     '   '     \u001b[0m | LogProb:  -0.0004  Prob:  0.9995  | AvgNLL: 0.2669  PPL_Prob: 0.7658\n",
      "[78] Token: \u001b[48;2;50;161;178m     '**'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2635  PPL_Prob: 0.7684\n",
      "[79] Token: \u001b[48;2;50;161;178m  'Formation'  \u001b[0m | LogProb:  -0.8799  Prob:  0.4148  | AvgNLL: 0.2712  PPL_Prob: 0.7625\n",
      "[80] Token: \u001b[48;2;50;161;178m     ':**'     \u001b[0m | LogProb:  -0.0004  Prob:  0.9995  | AvgNLL: 0.2679  PPL_Prob: 0.7650\n",
      "[81] Token: \u001b[48;2;50;161;178m    ' The'     \u001b[0m | LogProb:  -0.0596  Prob:  0.9419  | AvgNLL: 0.2653  PPL_Prob: 0.7670\n",
      "[82] Token: \u001b[48;2;50;161;178m    ' Moon'    \u001b[0m | LogProb:  -0.0018  Prob:  0.9980  | AvgNLL: 0.2621  PPL_Prob: 0.7694\n",
      "[83] Token: \u001b[48;2;50;161;178m   ' formed'   \u001b[0m | LogProb:  -0.0201  Prob:  0.9800  | AvgNLL: 0.2593  PPL_Prob: 0.7716\n",
      "[84] Token: \u001b[48;2;50;161;178m    ' from'    \u001b[0m | LogProb:  -0.4238  Prob:  0.6543  | AvgNLL: 0.2612  PPL_Prob: 0.7701\n",
      "[85] Token: \u001b[48;2;50;161;178m   ' debris'   \u001b[0m | LogProb:  -0.3606  Prob:  0.6973  | AvgNLL: 0.2624  PPL_Prob: 0.7692\n",
      "[86] Token: \u001b[48;2;50;161;178m    ' left'    \u001b[0m | LogProb:  -0.5762  Prob:  0.5620  | AvgNLL: 0.2660  PPL_Prob: 0.7665\n",
      "[87] Token: \u001b[48;2;50;161;178m    ' over'    \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.2629  PPL_Prob: 0.7688\n",
      "[88] Token: \u001b[48;2;50;161;178m   ' after'    \u001b[0m | LogProb:  -0.1801  Prob:  0.8354  | AvgNLL: 0.2620  PPL_Prob: 0.7695\n",
      "[89] Token: \u001b[48;2;50;161;178m     ' a'      \u001b[0m | LogProb:  -0.3450  Prob:  0.7080  | AvgNLL: 0.2629  PPL_Prob: 0.7688\n",
      "[90] Token: \u001b[48;2;50;161;178m   ' giant'    \u001b[0m | LogProb:  -0.3669  Prob:  0.6929  | AvgNLL: 0.2641  PPL_Prob: 0.7679\n",
      "[91] Token: \u001b[48;2;50;161;178m   ' impact'   \u001b[0m | LogProb:  -0.1007  Prob:  0.9043  | AvgNLL: 0.2623  PPL_Prob: 0.7693\n",
      "[92] Token: \u001b[48;2;50;161;178m  ' between'   \u001b[0m | LogProb:  -0.2096  Prob:  0.8110  | AvgNLL: 0.2617  PPL_Prob: 0.7697\n",
      "[93] Token: \u001b[48;2;50;161;178m   ' Earth'    \u001b[0m | LogProb:  -0.0878  Prob:  0.9160  | AvgNLL: 0.2599  PPL_Prob: 0.7711\n",
      "[94] Token: \u001b[48;2;50;161;178m    ' and'     \u001b[0m | LogProb:  -0.0009  Prob:  0.9990  | AvgNLL: 0.2572  PPL_Prob: 0.7732\n",
      "[95] Token: \u001b[48;2;50;161;178m     ' a'      \u001b[0m | LogProb:  -0.0207  Prob:  0.9795  | AvgNLL: 0.2547  PPL_Prob: 0.7752\n",
      "[96] Token: \u001b[48;2;50;161;178m    ' Mars'    \u001b[0m | LogProb:  -0.0005  Prob:  0.9995  | AvgNLL: 0.2521  PPL_Prob: 0.7772\n",
      "[97] Token: \u001b[48;2;50;161;178m      '-'      \u001b[0m | LogProb:  -0.0003  Prob:  0.9995  | AvgNLL: 0.2495  PPL_Prob: 0.7792\n",
      "[98] Token: \u001b[48;2;50;161;178m    'sized'    \u001b[0m | LogProb:  -0.0065  Prob:  0.9937  | AvgNLL: 0.2470  PPL_Prob: 0.7811\n",
      "[99] Token: \u001b[48;2;50;161;178m   ' object'   \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2446  PPL_Prob: 0.7830\n",
      "\n",
      "== Self Perplexity ==\n",
      "Avg NLL: 0.2446\n",
      "Perplexity (self): 0.7830\n",
      "Input: What is 3 + 4?\n",
      "\n",
      "[00] Token: \u001b[48;2;0;143;230m    '\\n\\n'     \u001b[0m | LogProb:  -0.0691  Prob:  0.9331  | AvgNLL: 0.0691  PPL_Prob: 0.9332\n",
      "[01] Token: \u001b[48;2;0;143;230m      '3'      \u001b[0m | LogProb:  -0.0786  Prob:  0.9243  | AvgNLL: 0.0738  PPL_Prob: 0.9288\n",
      "[02] Token: \u001b[48;2;0;143;230m     ' +'      \u001b[0m | LogProb:  -0.0025  Prob:  0.9976  | AvgNLL: 0.0500  PPL_Prob: 0.9512\n",
      "[03] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0375  PPL_Prob: 0.9632\n",
      "[04] Token: \u001b[48;2;0;143;230m      '4'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0300  PPL_Prob: 0.9704\n",
      "[05] Token: \u001b[48;2;0;143;230m     ' ='      \u001b[0m | LogProb:  -0.0005  Prob:  0.9995  | AvgNLL: 0.0251  PPL_Prob: 0.9752\n",
      "[06] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0215  PPL_Prob: 0.9787\n",
      "[07] Token: \u001b[48;2;0;143;230m      '7'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0188  PPL_Prob: 0.9813\n",
      "[08] Token: \u001b[48;2;0;143;230m    '\\n\\n'     \u001b[0m | LogProb:  -0.0102  Prob:  0.9897  | AvgNLL: 0.0179  PPL_Prob: 0.9823\n",
      "[09] Token: \u001b[48;2;0;143;230m     'So'      \u001b[0m | LogProb:  -0.4150  Prob:  0.6602  | AvgNLL: 0.0576  PPL_Prob: 0.9440\n",
      "[10] Token: \u001b[48;2;0;143;230m    ' the'     \u001b[0m | LogProb:  -0.0920  Prob:  0.9121  | AvgNLL: 0.0607  PPL_Prob: 0.9411\n",
      "[11] Token: \u001b[48;2;0;143;230m   ' answer'   \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0557  PPL_Prob: 0.9459\n",
      "[12] Token: \u001b[48;2;0;143;230m     ' is'     \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.0514  PPL_Prob: 0.9499\n",
      "[13] Token: \u001b[48;2;0;143;230m      ' '      \u001b[0m | LogProb:  -0.0624  Prob:  0.9395  | AvgNLL: 0.0522  PPL_Prob: 0.9492\n",
      "[14] Token: \u001b[48;2;0;143;230m      '7'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.0487  PPL_Prob: 0.9525\n",
      "[15] Token: \u001b[48;2;0;143;230m      '.'      \u001b[0m | LogProb:  -0.0076  Prob:  0.9922  | AvgNLL: 0.0461  PPL_Prob: 0.9549\n",
      "[16] Token: \u001b[48;2;0;143;230m     '\\n'      \u001b[0m | LogProb:  -0.5781  Prob:  0.5610  | AvgNLL: 0.0774  PPL_Prob: 0.9255\n",
      "[17] Token: \u001b[48;2;0;143;230m'<end_of_turn>'\u001b[0m | LogProb:  -0.0387  Prob:  0.9619  | AvgNLL: 0.0753  PPL_Prob: 0.9275\n",
      "\n",
      "== Self Perplexity ==\n",
      "Avg NLL: 0.0753\n",
      "Perplexity (self): 0.9275\n",
      "Input: Is water H2O?\n",
      "\n",
      "[00] Token: \u001b[48;2;152;198;76m    '\\n\\n'     \u001b[0m | LogProb:  -1.0957  Prob:  0.3342  | AvgNLL: 1.0957  PPL_Prob: 0.3343\n",
      "[01] Token: \u001b[48;2;101;179;127m     'Yes'     \u001b[0m | LogProb:  -0.0751  Prob:  0.9277  | AvgNLL: 0.5854  PPL_Prob: 0.5569\n",
      "[02] Token: \u001b[48;2;101;179;127m      ','      \u001b[0m | LogProb:  -0.4468  Prob:  0.6396  | AvgNLL: 0.5392  PPL_Prob: 0.5832\n",
      "[03] Token: \u001b[48;2;76;170;153m   ' water'    \u001b[0m | LogProb:  -0.3401  Prob:  0.7119  | AvgNLL: 0.4894  PPL_Prob: 0.6130\n",
      "[04] Token: \u001b[48;2;76;170;153m     ' is'     \u001b[0m | LogProb:  -0.1483  Prob:  0.8623  | AvgNLL: 0.4212  PPL_Prob: 0.6563\n",
      "[05] Token: \u001b[48;2;101;179;127m     ' H'      \u001b[0m | LogProb:  -0.9907  Prob:  0.3713  | AvgNLL: 0.5161  PPL_Prob: 0.5968\n",
      "[06] Token: \u001b[48;2;76;170;153m      '2'      \u001b[0m | LogProb:  -0.0166  Prob:  0.9834  | AvgNLL: 0.4448  PPL_Prob: 0.6410\n",
      "[07] Token: \u001b[48;2;76;170;153m      'O'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.3892  PPL_Prob: 0.6776\n",
      "[08] Token: \u001b[48;2;76;170;153m      '.'      \u001b[0m | LogProb:  -0.1187  Prob:  0.8882  | AvgNLL: 0.3591  PPL_Prob: 0.6983\n",
      "[09] Token: \u001b[48;2;50;161;178m     ' It'     \u001b[0m | LogProb:  -0.3103  Prob:  0.7334  | AvgNLL: 0.3542  PPL_Prob: 0.7017\n",
      "[10] Token: \u001b[48;2;50;161;178m      \"'\"      \u001b[0m | LogProb:  -0.3091  Prob:  0.7339  | AvgNLL: 0.3501  PPL_Prob: 0.7046\n",
      "[11] Token: \u001b[48;2;50;161;178m      's'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.3210  PPL_Prob: 0.7255\n",
      "[12] Token: \u001b[48;2;50;161;178m     ' a'      \u001b[0m | LogProb:  -0.2578  Prob:  0.7729  | AvgNLL: 0.3161  PPL_Prob: 0.7290\n",
      "[13] Token: \u001b[48;2;76;170;153m   ' simple'   \u001b[0m | LogProb:  -1.4092  Prob:  0.2444  | AvgNLL: 0.3942  PPL_Prob: 0.6742\n",
      "[14] Token: \u001b[48;2;76;170;153m      ','      \u001b[0m | LogProb:  -1.0459  Prob:  0.3513  | AvgNLL: 0.4376  PPL_Prob: 0.6456\n",
      "[15] Token: \u001b[48;2;76;170;153m' fundamental' \u001b[0m | LogProb:  -0.1869  Prob:  0.8296  | AvgNLL: 0.4220  PPL_Prob: 0.6558\n",
      "[16] Token: \u001b[48;2;76;170;153m  ' molecule'  \u001b[0m | LogProb:  -0.7056  Prob:  0.4939  | AvgNLL: 0.4386  PPL_Prob: 0.6449\n",
      "[17] Token: \u001b[48;2;76;170;153m      '.'      \u001b[0m | LogProb:  -0.7227  Prob:  0.4854  | AvgNLL: 0.4544  PPL_Prob: 0.6348\n",
      "[18] Token: \u001b[48;2;76;170;153m    '\\n\\n'     \u001b[0m | LogProb:  -0.8315  Prob:  0.4353  | AvgNLL: 0.4743  PPL_Prob: 0.6223\n",
      "[19] Token: \u001b[48;2;76;170;153m'<end_of_turn>'\u001b[0m | LogProb:  -1.0840  Prob:  0.3381  | AvgNLL: 0.5047  PPL_Prob: 0.6037\n",
      "\n",
      "== Self Perplexity ==\n",
      "Avg NLL: 0.5047\n",
      "Perplexity (self): 0.6037\n",
      "Input: Is the Sun a star?\n",
      "\n",
      "[00] Token: \u001b[48;2;50;161;178m    '\\n\\n'     \u001b[0m | LogProb:  -0.2756  Prob:  0.7593  | AvgNLL: 0.2756  PPL_Prob: 0.7591\n",
      "[01] Token: \u001b[48;2;50;161;178m     'Yes'     \u001b[0m | LogProb:  -0.2734  Prob:  0.7607  | AvgNLL: 0.2745  PPL_Prob: 0.7599\n",
      "[02] Token: \u001b[48;2;50;161;178m      ','      \u001b[0m | LogProb:  -0.2849  Prob:  0.7520  | AvgNLL: 0.2780  PPL_Prob: 0.7573\n",
      "[03] Token: \u001b[48;2;50;161;178m    ' the'     \u001b[0m | LogProb:  -0.1316  Prob:  0.8765  | AvgNLL: 0.2414  PPL_Prob: 0.7855\n",
      "[04] Token: \u001b[48;2;25;152;204m    ' Sun'     \u001b[0m | LogProb:  -0.0023  Prob:  0.9976  | AvgNLL: 0.1936  PPL_Prob: 0.8240\n",
      "[05] Token: \u001b[48;2;25;152;204m     ' is'     \u001b[0m | LogProb:  -0.0171  Prob:  0.9829  | AvgNLL: 0.1642  PPL_Prob: 0.8486\n",
      "[06] Token: \u001b[48;2;50;161;178m     ' a'      \u001b[0m | LogProb:  -0.8145  Prob:  0.4429  | AvgNLL: 0.2571  PPL_Prob: 0.7733\n",
      "[07] Token: \u001b[48;2;50;161;178m    ' star'    \u001b[0m | LogProb:  -0.0026  Prob:  0.9976  | AvgNLL: 0.2253  PPL_Prob: 0.7983\n",
      "[08] Token: \u001b[48;2;25;152;204m      '.'      \u001b[0m | LogProb:  -0.0984  Prob:  0.9062  | AvgNLL: 0.2112  PPL_Prob: 0.8096\n",
      "[09] Token: \u001b[48;2;25;152;204m     ' It'     \u001b[0m | LogProb:  -0.1486  Prob:  0.8618  | AvgNLL: 0.2049  PPL_Prob: 0.8147\n",
      "[10] Token: \u001b[48;2;25;152;204m      \"'\"      \u001b[0m | LogProb:  -0.3799  Prob:  0.6841  | AvgNLL: 0.2208  PPL_Prob: 0.8019\n",
      "[11] Token: \u001b[48;2;25;152;204m      's'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2024  PPL_Prob: 0.8168\n",
      "[12] Token: \u001b[48;2;25;152;204m     ' a'      \u001b[0m | LogProb:  -0.0091  Prob:  0.9907  | AvgNLL: 0.1875  PPL_Prob: 0.8290\n",
      "[13] Token: \u001b[48;2;50;161;178m  ' massive'   \u001b[0m | LogProb:  -0.8491  Prob:  0.4277  | AvgNLL: 0.2348  PPL_Prob: 0.7907\n",
      "[14] Token: \u001b[48;2;25;152;204m      ','      \u001b[0m | LogProb:  -0.0415  Prob:  0.9595  | AvgNLL: 0.2219  PPL_Prob: 0.8010\n",
      "[15] Token: \u001b[48;2;25;152;204m  ' luminous'  \u001b[0m | LogProb:  -0.0167  Prob:  0.9834  | AvgNLL: 0.2091  PPL_Prob: 0.8113\n",
      "[16] Token: \u001b[48;2;50;161;178m   ' sphere'   \u001b[0m | LogProb:  -0.6479  Prob:  0.5229  | AvgNLL: 0.2349  PPL_Prob: 0.7906\n",
      "[17] Token: \u001b[48;2;25;152;204m     ' of'     \u001b[0m | LogProb:  -0.0009  Prob:  0.9990  | AvgNLL: 0.2219  PPL_Prob: 0.8010\n",
      "[18] Token: \u001b[48;2;25;152;204m   ' plasma'   \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.2102  PPL_Prob: 0.8104\n",
      "[19] Token: \u001b[48;2;25;152;204m    ' held'    \u001b[0m | LogProb:  -0.0141  Prob:  0.9858  | AvgNLL: 0.2004  PPL_Prob: 0.8184\n",
      "[20] Token: \u001b[48;2;25;152;204m  ' together'  \u001b[0m | LogProb:  -0.0004  Prob:  0.9995  | AvgNLL: 0.1909  PPL_Prob: 0.8262\n",
      "[21] Token: \u001b[48;2;25;152;204m     ' by'     \u001b[0m | LogProb:  -0.0004  Prob:  0.9995  | AvgNLL: 0.1822  PPL_Prob: 0.8334\n",
      "[22] Token: \u001b[48;2;25;152;204m    ' its'     \u001b[0m | LogProb:  -0.0732  Prob:  0.9292  | AvgNLL: 0.1775  PPL_Prob: 0.8374\n",
      "[23] Token: \u001b[48;2;25;152;204m    ' own'     \u001b[0m | LogProb:  -0.0008  Prob:  0.9990  | AvgNLL: 0.1701  PPL_Prob: 0.8435\n",
      "[24] Token: \u001b[48;2;25;152;204m  ' gravity'   \u001b[0m | LogProb:  -0.0005  Prob:  0.9995  | AvgNLL: 0.1634  PPL_Prob: 0.8493\n",
      "[25] Token: \u001b[48;2;25;152;204m      '.'      \u001b[0m | LogProb:  -0.1213  Prob:  0.8857  | AvgNLL: 0.1617  PPL_Prob: 0.8507\n",
      "[26] Token: \u001b[48;2;25;152;204m     ' It'     \u001b[0m | LogProb:  -0.5112  Prob:  0.5996  | AvgNLL: 0.1747  PPL_Prob: 0.8397\n",
      "[27] Token: \u001b[48;2;25;152;204m  ' produces'  \u001b[0m | LogProb:  -1.0811  Prob:  0.3394  | AvgNLL: 0.2071  PPL_Prob: 0.8130\n",
      "[28] Token: \u001b[48;2;25;152;204m   ' light'    \u001b[0m | LogProb:  -0.2544  Prob:  0.7754  | AvgNLL: 0.2087  PPL_Prob: 0.8117\n",
      "[29] Token: \u001b[48;2;25;152;204m    ' and'     \u001b[0m | LogProb:  -0.0014  Prob:  0.9985  | AvgNLL: 0.2018  PPL_Prob: 0.8173\n",
      "[30] Token: \u001b[48;2;25;152;204m    ' heat'    \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.1953  PPL_Prob: 0.8226\n",
      "[31] Token: \u001b[48;2;25;152;204m  ' through'   \u001b[0m | LogProb:  -0.0024  Prob:  0.9976  | AvgNLL: 0.1892  PPL_Prob: 0.8276\n",
      "[32] Token: \u001b[48;2;25;152;204m  ' nuclear'   \u001b[0m | LogProb:  -0.0061  Prob:  0.9941  | AvgNLL: 0.1837  PPL_Prob: 0.8322\n",
      "[33] Token: \u001b[48;2;25;152;204m   ' fusion'   \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.1783  PPL_Prob: 0.8367\n",
      "[34] Token: \u001b[48;2;25;152;204m     ' in'     \u001b[0m | LogProb:  -0.7300  Prob:  0.4819  | AvgNLL: 0.1940  PPL_Prob: 0.8236\n",
      "[35] Token: \u001b[48;2;25;152;204m    ' its'     \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.1887  PPL_Prob: 0.8281\n",
      "[36] Token: \u001b[48;2;25;152;204m    ' core'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.1836  PPL_Prob: 0.8323\n",
      "[37] Token: \u001b[48;2;25;152;204m      ','      \u001b[0m | LogProb:  -0.6060  Prob:  0.5454  | AvgNLL: 0.1947  PPL_Prob: 0.8231\n",
      "[38] Token: \u001b[48;2;25;152;204m   ' which'    \u001b[0m | LogProb:  -0.6938  Prob:  0.4998  | AvgNLL: 0.2075  PPL_Prob: 0.8126\n",
      "[39] Token: \u001b[48;2;25;152;204m     ' is'     \u001b[0m | LogProb:  -0.0818  Prob:  0.9214  | AvgNLL: 0.2043  PPL_Prob: 0.8152\n",
      "[40] Token: \u001b[48;2;25;152;204m     ' a'      \u001b[0m | LogProb:  -0.9336  Prob:  0.3931  | AvgNLL: 0.2221  PPL_Prob: 0.8008\n",
      "[41] Token: \u001b[48;2;25;152;204m' fundamental' \u001b[0m | LogProb:  -0.1777  Prob:  0.8374  | AvgNLL: 0.2211  PPL_Prob: 0.8017\n",
      "[42] Token: \u001b[48;2;25;152;204m' characteristic'\u001b[0m | LogProb:  -0.0632  Prob:  0.9390  | AvgNLL: 0.2174  PPL_Prob: 0.8046\n",
      "[43] Token: \u001b[48;2;25;152;204m     ' of'     \u001b[0m | LogProb:  -0.0017  Prob:  0.9980  | AvgNLL: 0.2125  PPL_Prob: 0.8086\n",
      "[44] Token: \u001b[48;2;25;152;204m   ' stars'    \u001b[0m | LogProb:  -0.0118  Prob:  0.9883  | AvgNLL: 0.2080  PPL_Prob: 0.8122\n",
      "[45] Token: \u001b[48;2;25;152;204m      '.'      \u001b[0m | LogProb:  -0.0152  Prob:  0.9849  | AvgNLL: 0.2038  PPL_Prob: 0.8156\n",
      "[46] Token: \u001b[48;2;25;152;204m    '\\n\\n'     \u001b[0m | LogProb:  -0.1786  Prob:  0.8364  | AvgNLL: 0.2033  PPL_Prob: 0.8160\n",
      "[47] Token: \u001b[48;2;25;152;204m    'Here'     \u001b[0m | LogProb:  -0.2113  Prob:  0.8096  | AvgNLL: 0.2035  PPL_Prob: 0.8159\n",
      "[48] Token: \u001b[48;2;25;152;204m      \"'\"      \u001b[0m | LogProb:  -0.0793  Prob:  0.9238  | AvgNLL: 0.2009  PPL_Prob: 0.8180\n",
      "[49] Token: \u001b[48;2;25;152;204m      's'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.1969  PPL_Prob: 0.8213\n",
      "[50] Token: \u001b[48;2;25;152;204m     ' a'      \u001b[0m | LogProb:  -0.0076  Prob:  0.9927  | AvgNLL: 0.1932  PPL_Prob: 0.8243\n",
      "[51] Token: \u001b[48;2;25;152;204m ' breakdown'  \u001b[0m | LogProb:  -0.2810  Prob:  0.7549  | AvgNLL: 0.1949  PPL_Prob: 0.8229\n",
      "[52] Token: \u001b[48;2;25;152;204m     ' of'     \u001b[0m | LogProb:  -0.5850  Prob:  0.5571  | AvgNLL: 0.2023  PPL_Prob: 0.8169\n",
      "[53] Token: \u001b[48;2;25;152;204m    ' why'     \u001b[0m | LogProb:  -0.0659  Prob:  0.9360  | AvgNLL: 0.1997  PPL_Prob: 0.8190\n",
      "[54] Token: \u001b[48;2;25;152;204m     ' it'     \u001b[0m | LogProb:  -0.4084  Prob:  0.6646  | AvgNLL: 0.2035  PPL_Prob: 0.8159\n",
      "[55] Token: \u001b[48;2;25;152;204m      \"'\"      \u001b[0m | LogProb:  -0.3503  Prob:  0.7046  | AvgNLL: 0.2061  PPL_Prob: 0.8137\n",
      "[56] Token: \u001b[48;2;25;152;204m      's'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2025  PPL_Prob: 0.8167\n",
      "[57] Token: \u001b[48;2;25;152;204m     ' a'      \u001b[0m | LogProb:  -0.4114  Prob:  0.6626  | AvgNLL: 0.2061  PPL_Prob: 0.8137\n",
      "[58] Token: \u001b[48;2;25;152;204m    ' star'    \u001b[0m | LogProb:  -0.0013  Prob:  0.9985  | AvgNLL: 0.2027  PPL_Prob: 0.8166\n",
      "[59] Token: \u001b[48;2;25;152;204m      ':'      \u001b[0m | LogProb:  -0.0424  Prob:  0.9585  | AvgNLL: 0.2000  PPL_Prob: 0.8187\n",
      "[60] Token: \u001b[48;2;25;152;204m    '\\n\\n'     \u001b[0m | LogProb:  -0.0603  Prob:  0.9414  | AvgNLL: 0.1977  PPL_Prob: 0.8206\n",
      "[61] Token: \u001b[48;2;25;152;204m      '*'      \u001b[0m | LogProb:  -0.0003  Prob:  0.9995  | AvgNLL: 0.1945  PPL_Prob: 0.8232\n",
      "[62] Token: \u001b[48;2;25;152;204m     ' **'     \u001b[0m | LogProb:  -0.6699  Prob:  0.5117  | AvgNLL: 0.2021  PPL_Prob: 0.8170\n",
      "[63] Token: \u001b[48;2;25;152;204m   'Nuclear'   \u001b[0m | LogProb:  -0.4641  Prob:  0.6289  | AvgNLL: 0.2062  PPL_Prob: 0.8137\n",
      "[64] Token: \u001b[48;2;25;152;204m   ' Fusion'   \u001b[0m | LogProb:  -0.0011  Prob:  0.9990  | AvgNLL: 0.2030  PPL_Prob: 0.8163\n",
      "[65] Token: \u001b[48;2;25;152;204m     ':**'     \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.1999  PPL_Prob: 0.8188\n",
      "[66] Token: \u001b[48;2;25;152;204m   ' Stars'    \u001b[0m | LogProb:  -0.5376  Prob:  0.5840  | AvgNLL: 0.2050  PPL_Prob: 0.8147\n",
      "[67] Token: \u001b[48;2;25;152;204m  ' generate'  \u001b[0m | LogProb:  -0.0606  Prob:  0.9414  | AvgNLL: 0.2028  PPL_Prob: 0.8164\n",
      "[68] Token: \u001b[48;2;25;152;204m   ' energy'   \u001b[0m | LogProb:  -0.0139  Prob:  0.9863  | AvgNLL: 0.2001  PPL_Prob: 0.8186\n",
      "[69] Token: \u001b[48;2;25;152;204m     ' by'     \u001b[0m | LogProb:  -0.1196  Prob:  0.8872  | AvgNLL: 0.1990  PPL_Prob: 0.8196\n",
      "[70] Token: \u001b[48;2;25;152;204m   ' fusing'   \u001b[0m | LogProb:  -0.0493  Prob:  0.9521  | AvgNLL: 0.1968  PPL_Prob: 0.8213\n",
      "[71] Token: \u001b[48;2;25;152;204m  ' hydrogen'  \u001b[0m | LogProb:  -0.3403  Prob:  0.7114  | AvgNLL: 0.1988  PPL_Prob: 0.8197\n",
      "[72] Token: \u001b[48;2;25;152;204m   ' atoms'    \u001b[0m | LogProb:  -0.4746  Prob:  0.6221  | AvgNLL: 0.2026  PPL_Prob: 0.8166\n",
      "[73] Token: \u001b[48;2;25;152;204m    ' into'    \u001b[0m | LogProb:  -0.0004  Prob:  0.9995  | AvgNLL: 0.1999  PPL_Prob: 0.8188\n",
      "[74] Token: \u001b[48;2;25;152;204m   ' helium'   \u001b[0m | LogProb:  -0.0022  Prob:  0.9980  | AvgNLL: 0.1972  PPL_Prob: 0.8210\n",
      "[75] Token: \u001b[48;2;25;152;204m   ' atoms'    \u001b[0m | LogProb:  -0.7422  Prob:  0.4761  | AvgNLL: 0.2044  PPL_Prob: 0.8151\n",
      "[76] Token: \u001b[48;2;25;152;204m      ','      \u001b[0m | LogProb:  -0.9800  Prob:  0.3752  | AvgNLL: 0.2145  PPL_Prob: 0.8070\n",
      "[77] Token: \u001b[48;2;25;152;204m ' releasing'  \u001b[0m | LogProb:  -0.0046  Prob:  0.9956  | AvgNLL: 0.2118  PPL_Prob: 0.8091\n",
      "[78] Token: \u001b[48;2;25;152;204m ' tremendous' \u001b[0m | LogProb:  -0.6162  Prob:  0.5400  | AvgNLL: 0.2169  PPL_Prob: 0.8050\n",
      "[79] Token: \u001b[48;2;25;152;204m  ' amounts'   \u001b[0m | LogProb:  -0.0280  Prob:  0.9722  | AvgNLL: 0.2146  PPL_Prob: 0.8069\n",
      "[80] Token: \u001b[48;2;25;152;204m     ' of'     \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.2119  PPL_Prob: 0.8090\n",
      "[81] Token: \u001b[48;2;25;152;204m   ' energy'   \u001b[0m | LogProb:  -0.3000  Prob:  0.7407  | AvgNLL: 0.2130  PPL_Prob: 0.8082\n",
      "[82] Token: \u001b[48;2;25;152;204m     ' in'     \u001b[0m | LogProb:  -0.0172  Prob:  0.9829  | AvgNLL: 0.2106  PPL_Prob: 0.8101\n",
      "[83] Token: \u001b[48;2;25;152;204m    ' the'     \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.2081  PPL_Prob: 0.8121\n",
      "[84] Token: \u001b[48;2;25;152;204m  ' process'   \u001b[0m | LogProb:  -0.0021  Prob:  0.9980  | AvgNLL: 0.2057  PPL_Prob: 0.8141\n",
      "[85] Token: \u001b[48;2;25;152;204m      '.'      \u001b[0m | LogProb:  -0.0063  Prob:  0.9937  | AvgNLL: 0.2034  PPL_Prob: 0.8160\n",
      "[86] Token: \u001b[48;2;25;152;204m     '\\n'      \u001b[0m | LogProb:  -0.0773  Prob:  0.9258  | AvgNLL: 0.2019  PPL_Prob: 0.8172\n",
      "[87] Token: \u001b[48;2;25;152;204m      '*'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.1996  PPL_Prob: 0.8190\n",
      "[88] Token: \u001b[48;2;25;152;204m     ' **'     \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.1974  PPL_Prob: 0.8209\n",
      "[89] Token: \u001b[48;2;25;152;204m    'Mass'     \u001b[0m | LogProb:  -0.7559  Prob:  0.4697  | AvgNLL: 0.2036  PPL_Prob: 0.8158\n",
      "[90] Token: \u001b[48;2;25;152;204m     'ive'     \u001b[0m | LogProb:  -0.0043  Prob:  0.9956  | AvgNLL: 0.2014  PPL_Prob: 0.8176\n",
      "[91] Token: \u001b[48;2;25;152;204m    ' Size'    \u001b[0m | LogProb:  -0.0013  Prob:  0.9985  | AvgNLL: 0.1992  PPL_Prob: 0.8194\n",
      "[92] Token: \u001b[48;2;25;152;204m     ':**'     \u001b[0m | LogProb:  -0.0592  Prob:  0.9424  | AvgNLL: 0.1977  PPL_Prob: 0.8206\n",
      "[93] Token: \u001b[48;2;25;152;204m   ' Stars'    \u001b[0m | LogProb:  -0.1764  Prob:  0.8384  | AvgNLL: 0.1975  PPL_Prob: 0.8208\n",
      "[94] Token: \u001b[48;2;25;152;204m    ' are'     \u001b[0m | LogProb:  -0.1284  Prob:  0.8794  | AvgNLL: 0.1968  PPL_Prob: 0.8214\n",
      "[95] Token: \u001b[48;2;25;152;204m ' incredibly' \u001b[0m | LogProb:  -0.2932  Prob:  0.7461  | AvgNLL: 0.1978  PPL_Prob: 0.8206\n",
      "[96] Token: \u001b[48;2;25;152;204m   ' large'    \u001b[0m | LogProb:  -0.1473  Prob:  0.8628  | AvgNLL: 0.1973  PPL_Prob: 0.8210\n",
      "[97] Token: \u001b[48;2;25;152;204m  ' compared'  \u001b[0m | LogProb:  -0.7686  Prob:  0.4636  | AvgNLL: 0.2031  PPL_Prob: 0.8162\n",
      "[98] Token: \u001b[48;2;25;152;204m     ' to'     \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2010  PPL_Prob: 0.8179\n",
      "[99] Token: \u001b[48;2;25;152;204m  ' planets'   \u001b[0m | LogProb:  -0.6899  Prob:  0.5015  | AvgNLL: 0.2059  PPL_Prob: 0.8139\n",
      "\n",
      "== Self Perplexity ==\n",
      "Avg NLL: 0.2059\n",
      "Perplexity (self): 0.8139\n",
      "Input: Are cats mammals?\n",
      "\n",
      "[00] Token: \u001b[48;2;76;170;153m    '\\n\\n'     \u001b[0m | LogProb:  -0.4697  Prob:  0.6250  | AvgNLL: 0.4697  PPL_Prob: 0.6252\n",
      "[01] Token: \u001b[48;2;101;179;127m     'The'     \u001b[0m | LogProb:  -0.7832  Prob:  0.4570  | AvgNLL: 0.6265  PPL_Prob: 0.5345\n",
      "[02] Token: \u001b[48;2;127;189;102m   ' answer'   \u001b[0m | LogProb:  -0.9268  Prob:  0.3958  | AvgNLL: 0.7266  PPL_Prob: 0.4836\n",
      "[03] Token: \u001b[48;2;127;189;102m     ' is'     \u001b[0m | LogProb:  -0.6675  Prob:  0.5132  | AvgNLL: 0.7118  PPL_Prob: 0.4908\n",
      "[04] Token: \u001b[48;2;101;179;127m     ' a'      \u001b[0m | LogProb:  -0.1493  Prob:  0.8613  | AvgNLL: 0.5993  PPL_Prob: 0.5492\n",
      "[05] Token: \u001b[48;2;101;179;127m    ' bit'     \u001b[0m | LogProb:  -0.2698  Prob:  0.7637  | AvgNLL: 0.5444  PPL_Prob: 0.5802\n",
      "[06] Token: \u001b[48;2;101;179;127m  ' complex'   \u001b[0m | LogProb:  -0.8589  Prob:  0.4236  | AvgNLL: 0.5893  PPL_Prob: 0.5547\n",
      "[07] Token: \u001b[48;2;101;179;127m      ','      \u001b[0m | LogProb:  -0.6602  Prob:  0.5166  | AvgNLL: 0.5982  PPL_Prob: 0.5498\n",
      "[08] Token: \u001b[48;2;101;179;127m    ' but'     \u001b[0m | LogProb:  -0.0941  Prob:  0.9102  | AvgNLL: 0.5422  PPL_Prob: 0.5815\n",
      "[09] Token: \u001b[48;2;101;179;127m ' generally'  \u001b[0m | LogProb:  -0.2947  Prob:  0.7446  | AvgNLL: 0.5174  PPL_Prob: 0.5961\n",
      "[10] Token: \u001b[48;2;76;170;153m      ','      \u001b[0m | LogProb:  -0.2786  Prob:  0.7568  | AvgNLL: 0.4957  PPL_Prob: 0.6091\n",
      "[11] Token: \u001b[48;2;101;179;127m    ' cats'    \u001b[0m | LogProb:  -0.7397  Prob:  0.4773  | AvgNLL: 0.5160  PPL_Prob: 0.5969\n",
      "[12] Token: \u001b[48;2;76;170;153m    ' are'     \u001b[0m | LogProb:  -0.0687  Prob:  0.9336  | AvgNLL: 0.4816  PPL_Prob: 0.6178\n",
      "[13] Token: \u001b[48;2;76;170;153m ' considered' \u001b[0m | LogProb:  -0.7476  Prob:  0.4736  | AvgNLL: 0.5006  PPL_Prob: 0.6062\n",
      "[14] Token: \u001b[48;2;101;179;127m  ' mammals'   \u001b[0m | LogProb:  -0.7642  Prob:  0.4658  | AvgNLL: 0.5182  PPL_Prob: 0.5956\n",
      "[15] Token: \u001b[48;2;76;170;153m      '.'      \u001b[0m | LogProb:  -0.0208  Prob:  0.9795  | AvgNLL: 0.4871  PPL_Prob: 0.6144\n",
      "[16] Token: \u001b[48;2;76;170;153m    ' Here'    \u001b[0m | LogProb:  -0.1307  Prob:  0.8774  | AvgNLL: 0.4661  PPL_Prob: 0.6274\n",
      "[17] Token: \u001b[48;2;76;170;153m      \"'\"      \u001b[0m | LogProb:  -0.1774  Prob:  0.8374  | AvgNLL: 0.4501  PPL_Prob: 0.6376\n",
      "[18] Token: \u001b[48;2;76;170;153m      's'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.4264  PPL_Prob: 0.6529\n",
      "[19] Token: \u001b[48;2;76;170;153m     ' a'      \u001b[0m | LogProb:  -0.2632  Prob:  0.7686  | AvgNLL: 0.4182  PPL_Prob: 0.6582\n",
      "[20] Token: \u001b[48;2;76;170;153m ' breakdown'  \u001b[0m | LogProb:  -0.0010  Prob:  0.9990  | AvgNLL: 0.3984  PPL_Prob: 0.6714\n",
      "[21] Token: \u001b[48;2;76;170;153m      ':'      \u001b[0m | LogProb:  -0.3306  Prob:  0.7188  | AvgNLL: 0.3953  PPL_Prob: 0.6735\n",
      "[22] Token: \u001b[48;2;76;170;153m    '\\n\\n'     \u001b[0m | LogProb:  -0.0005  Prob:  0.9995  | AvgNLL: 0.3781  PPL_Prob: 0.6851\n",
      "[23] Token: \u001b[48;2;76;170;153m      '*'      \u001b[0m | LogProb:  -0.0874  Prob:  0.9165  | AvgNLL: 0.3660  PPL_Prob: 0.6935\n",
      "[24] Token: \u001b[48;2;76;170;153m     '   '     \u001b[0m | LogProb:  -0.5361  Prob:  0.5850  | AvgNLL: 0.3728  PPL_Prob: 0.6888\n",
      "[25] Token: \u001b[48;2;76;170;153m     '**'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.3585  PPL_Prob: 0.6987\n",
      "[26] Token: \u001b[48;2;50;161;178m    'Mamm'     \u001b[0m | LogProb:  -0.0102  Prob:  0.9897  | AvgNLL: 0.3456  PPL_Prob: 0.7078\n",
      "[27] Token: \u001b[48;2;50;161;178m     'als'     \u001b[0m | LogProb:  -0.2003  Prob:  0.8184  | AvgNLL: 0.3404  PPL_Prob: 0.7115\n",
      "[28] Token: \u001b[48;2;76;170;153m   ' share'    \u001b[0m | LogProb:  -0.9521  Prob:  0.3860  | AvgNLL: 0.3615  PPL_Prob: 0.6966\n",
      "[29] Token: \u001b[48;2;50;161;178m    ' key'     \u001b[0m | LogProb:  -0.0317  Prob:  0.9688  | AvgNLL: 0.3505  PPL_Prob: 0.7043\n",
      "[30] Token: \u001b[48;2;50;161;178m' characteristics'\u001b[0m | LogProb:  -0.0100  Prob:  0.9902  | AvgNLL: 0.3395  PPL_Prob: 0.7121\n",
      "[31] Token: \u001b[48;2;50;161;178m     ':**'     \u001b[0m | LogProb:  -0.0100  Prob:  0.9902  | AvgNLL: 0.3292  PPL_Prob: 0.7195\n",
      "[32] Token: \u001b[48;2;50;161;178m     '\\n'      \u001b[0m | LogProb:  -0.8188  Prob:  0.4409  | AvgNLL: 0.3441  PPL_Prob: 0.7089\n",
      "[33] Token: \u001b[48;2;50;161;178m    '    '     \u001b[0m | LogProb:  -0.0005  Prob:  0.9995  | AvgNLL: 0.3339  PPL_Prob: 0.7161\n",
      "[34] Token: \u001b[48;2;50;161;178m      '*'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.3244  PPL_Prob: 0.7230\n",
      "[35] Token: \u001b[48;2;50;161;178m     '   '     \u001b[0m | LogProb:  -0.0007  Prob:  0.9995  | AvgNLL: 0.3154  PPL_Prob: 0.7295\n",
      "[36] Token: \u001b[48;2;50;161;178m    'They'     \u001b[0m | LogProb:  -0.3044  Prob:  0.7373  | AvgNLL: 0.3151  PPL_Prob: 0.7297\n",
      "[37] Token: \u001b[48;2;50;161;178m    ' are'     \u001b[0m | LogProb:  -0.4114  Prob:  0.6626  | AvgNLL: 0.3177  PPL_Prob: 0.7279\n",
      "[38] Token: \u001b[48;2;50;161;178m    ' warm'    \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.3095  PPL_Prob: 0.7338\n",
      "[39] Token: \u001b[48;2;50;161;178m      '-'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.3018  PPL_Prob: 0.7395\n",
      "[40] Token: \u001b[48;2;50;161;178m   'blooded'   \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2944  PPL_Prob: 0.7450\n",
      "[41] Token: \u001b[48;2;50;161;178m     ' ('      \u001b[0m | LogProb:  -0.1761  Prob:  0.8384  | AvgNLL: 0.2916  PPL_Prob: 0.7471\n",
      "[42] Token: \u001b[48;2;50;161;178m    'they'     \u001b[0m | LogProb:  -1.0020  Prob:  0.3672  | AvgNLL: 0.3081  PPL_Prob: 0.7348\n",
      "[43] Token: \u001b[48;2;50;161;178m  ' maintain'  \u001b[0m | LogProb:  -0.3062  Prob:  0.7363  | AvgNLL: 0.3081  PPL_Prob: 0.7349\n",
      "[44] Token: \u001b[48;2;50;161;178m     ' a'      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.3012  PPL_Prob: 0.7399\n",
      "[45] Token: \u001b[48;2;50;161;178m  ' constant'  \u001b[0m | LogProb:  -0.5254  Prob:  0.5913  | AvgNLL: 0.3061  PPL_Prob: 0.7363\n",
      "[46] Token: \u001b[48;2;50;161;178m  ' internal'  \u001b[0m | LogProb:  -0.0071  Prob:  0.9927  | AvgNLL: 0.2997  PPL_Prob: 0.7410\n",
      "[47] Token: \u001b[48;2;50;161;178m    ' body'    \u001b[0m | LogProb:  -0.0392  Prob:  0.9614  | AvgNLL: 0.2943  PPL_Prob: 0.7450\n",
      "[48] Token: \u001b[48;2;50;161;178m' temperature' \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2883  PPL_Prob: 0.7495\n",
      "[49] Token: \u001b[48;2;50;161;178m      ')'      \u001b[0m | LogProb:  -0.6279  Prob:  0.5337  | AvgNLL: 0.2951  PPL_Prob: 0.7445\n",
      "[50] Token: \u001b[48;2;50;161;178m     '\\n'      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.2893  PPL_Prob: 0.7488\n",
      "[51] Token: \u001b[48;2;50;161;178m    '    '     \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2838  PPL_Prob: 0.7530\n",
      "[52] Token: \u001b[48;2;50;161;178m      '*'      \u001b[0m | LogProb:   0.0000  Prob:  1.0000  | AvgNLL: 0.2784  PPL_Prob: 0.7570\n",
      "[53] Token: \u001b[48;2;50;161;178m     '   '     \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2732  PPL_Prob: 0.7609\n",
      "[54] Token: \u001b[48;2;50;161;178m    'They'     \u001b[0m | LogProb:  -0.0083  Prob:  0.9917  | AvgNLL: 0.2684  PPL_Prob: 0.7646\n",
      "[55] Token: \u001b[48;2;50;161;178m    ' have'    \u001b[0m | LogProb:  -0.1423  Prob:  0.8672  | AvgNLL: 0.2662  PPL_Prob: 0.7663\n",
      "[56] Token: \u001b[48;2;50;161;178m    ' fur'     \u001b[0m | LogProb:  -0.6792  Prob:  0.5068  | AvgNLL: 0.2734  PPL_Prob: 0.7608\n",
      "[57] Token: \u001b[48;2;50;161;178m     ' or'     \u001b[0m | LogProb:  -0.0013  Prob:  0.9985  | AvgNLL: 0.2687  PPL_Prob: 0.7643\n",
      "[58] Token: \u001b[48;2;50;161;178m    ' hair'    \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.2642  PPL_Prob: 0.7678\n",
      "[59] Token: \u001b[48;2;50;161;178m     '\\n'      \u001b[0m | LogProb:  -0.0368  Prob:  0.9639  | AvgNLL: 0.2604  PPL_Prob: 0.7708\n",
      "[60] Token: \u001b[48;2;50;161;178m    '    '     \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2561  PPL_Prob: 0.7740\n",
      "[61] Token: \u001b[48;2;50;161;178m      '*'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2520  PPL_Prob: 0.7773\n",
      "[62] Token: \u001b[48;2;50;161;178m     '   '     \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2480  PPL_Prob: 0.7804\n",
      "[63] Token: \u001b[48;2;50;161;178m    'They'     \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.2441  PPL_Prob: 0.7834\n",
      "[64] Token: \u001b[48;2;50;161;178m    ' give'    \u001b[0m | LogProb:  -0.3486  Prob:  0.7056  | AvgNLL: 0.2457  PPL_Prob: 0.7821\n",
      "[65] Token: \u001b[48;2;50;161;178m   ' birth'    \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.2420  PPL_Prob: 0.7851\n",
      "[66] Token: \u001b[48;2;50;161;178m     ' to'     \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2384  PPL_Prob: 0.7879\n",
      "[67] Token: \u001b[48;2;50;161;178m    ' live'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2349  PPL_Prob: 0.7907\n",
      "[68] Token: \u001b[48;2;50;161;178m   ' young'    \u001b[0m | LogProb:  -0.0006  Prob:  0.9995  | AvgNLL: 0.2315  PPL_Prob: 0.7933\n",
      "[69] Token: \u001b[48;2;50;161;178m     ' ('      \u001b[0m | LogProb:  -0.1957  Prob:  0.8223  | AvgNLL: 0.2310  PPL_Prob: 0.7938\n",
      "[70] Token: \u001b[48;2;50;161;178m   'except'    \u001b[0m | LogProb:  -0.6147  Prob:  0.5410  | AvgNLL: 0.2364  PPL_Prob: 0.7895\n",
      "[71] Token: \u001b[48;2;50;161;178m    ' for'     \u001b[0m | LogProb:  -0.0021  Prob:  0.9980  | AvgNLL: 0.2331  PPL_Prob: 0.7920\n",
      "[72] Token: \u001b[48;2;50;161;178m   ' monot'    \u001b[0m | LogProb:  -0.0215  Prob:  0.9785  | AvgNLL: 0.2302  PPL_Prob: 0.7943\n",
      "[73] Token: \u001b[48;2;50;161;178m     'rem'     \u001b[0m | LogProb:  -0.0190  Prob:  0.9810  | AvgNLL: 0.2274  PPL_Prob: 0.7966\n",
      "[74] Token: \u001b[48;2;50;161;178m     'es'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2243  PPL_Prob: 0.7990\n",
      "[75] Token: \u001b[48;2;50;161;178m    ' like'    \u001b[0m | LogProb:  -0.6060  Prob:  0.5454  | AvgNLL: 0.2294  PPL_Prob: 0.7950\n",
      "[76] Token: \u001b[48;2;50;161;178m    ' the'     \u001b[0m | LogProb:  -0.6489  Prob:  0.5225  | AvgNLL: 0.2348  PPL_Prob: 0.7907\n",
      "[77] Token: \u001b[48;2;50;161;178m    ' ech'     \u001b[0m | LogProb:  -0.6797  Prob:  0.5068  | AvgNLL: 0.2405  PPL_Prob: 0.7862\n",
      "[78] Token: \u001b[48;2;50;161;178m     'id'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2375  PPL_Prob: 0.7886\n",
      "[79] Token: \u001b[48;2;50;161;178m     'na'      \u001b[0m | LogProb:  -0.0026  Prob:  0.9976  | AvgNLL: 0.2345  PPL_Prob: 0.7909\n",
      "[80] Token: \u001b[48;2;50;161;178m    ' and'     \u001b[0m | LogProb:  -0.0125  Prob:  0.9878  | AvgNLL: 0.2318  PPL_Prob: 0.7931\n",
      "[81] Token: \u001b[48;2;50;161;178m    ' plat'    \u001b[0m | LogProb:  -0.0003  Prob:  0.9995  | AvgNLL: 0.2290  PPL_Prob: 0.7953\n",
      "[82] Token: \u001b[48;2;50;161;178m     'yp'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2262  PPL_Prob: 0.7975\n",
      "[83] Token: \u001b[48;2;50;161;178m     'us'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2235  PPL_Prob: 0.7997\n",
      "[84] Token: \u001b[48;2;25;152;204m      ')'      \u001b[0m | LogProb:  -0.0875  Prob:  0.9160  | AvgNLL: 0.2219  PPL_Prob: 0.8010\n",
      "[85] Token: \u001b[48;2;25;152;204m     '\\n'      \u001b[0m | LogProb:  -0.0022  Prob:  0.9980  | AvgNLL: 0.2194  PPL_Prob: 0.8030\n",
      "[86] Token: \u001b[48;2;25;152;204m    '    '     \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.2168  PPL_Prob: 0.8051\n",
      "[87] Token: \u001b[48;2;25;152;204m      '*'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2144  PPL_Prob: 0.8070\n",
      "[88] Token: \u001b[48;2;25;152;204m     '   '     \u001b[0m | LogProb:  -0.0007  Prob:  0.9990  | AvgNLL: 0.2120  PPL_Prob: 0.8090\n",
      "[89] Token: \u001b[48;2;25;152;204m    'They'     \u001b[0m | LogProb:  -0.0003  Prob:  0.9995  | AvgNLL: 0.2096  PPL_Prob: 0.8109\n",
      "[90] Token: \u001b[48;2;25;152;204m   ' nurse'    \u001b[0m | LogProb:  -0.2072  Prob:  0.8130  | AvgNLL: 0.2096  PPL_Prob: 0.8109\n",
      "[91] Token: \u001b[48;2;25;152;204m   ' their'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2073  PPL_Prob: 0.8128\n",
      "[92] Token: \u001b[48;2;25;152;204m   ' young'    \u001b[0m | LogProb:  -0.0125  Prob:  0.9878  | AvgNLL: 0.2052  PPL_Prob: 0.8145\n",
      "[93] Token: \u001b[48;2;25;152;204m    ' with'    \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.2031  PPL_Prob: 0.8162\n",
      "[94] Token: \u001b[48;2;25;152;204m    ' milk'    \u001b[0m | LogProb:  -0.0003  Prob:  0.9995  | AvgNLL: 0.2009  PPL_Prob: 0.8180\n",
      "[95] Token: \u001b[48;2;25;152;204m     '\\n'      \u001b[0m | LogProb:  -0.5029  Prob:  0.6050  | AvgNLL: 0.2041  PPL_Prob: 0.8154\n",
      "[96] Token: \u001b[48;2;25;152;204m    '    '     \u001b[0m | LogProb:  -0.0088  Prob:  0.9912  | AvgNLL: 0.2020  PPL_Prob: 0.8171\n",
      "[97] Token: \u001b[48;2;25;152;204m      '*'      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.2000  PPL_Prob: 0.8187\n",
      "[98] Token: \u001b[48;2;25;152;204m     '   '     \u001b[0m | LogProb:  -0.0033  Prob:  0.9966  | AvgNLL: 0.1980  PPL_Prob: 0.8204\n",
      "[99] Token: \u001b[48;2;25;152;204m    'They'     \u001b[0m | LogProb:  -0.0013  Prob:  0.9985  | AvgNLL: 0.1960  PPL_Prob: 0.8220\n",
      "\n",
      "== Self Perplexity ==\n",
      "Avg NLL: 0.1960\n",
      "Perplexity (self): 0.8220\n",
      "\n",
      "--- Tricky Questions ---\n",
      "\n",
      "Input: If you say “I am lying,” are you telling the truth?\n",
      "\n",
      "[00] Token: \u001b[48;2;25;152;204m    '\\n\\n'     \u001b[0m | LogProb:  -0.1346  Prob:  0.8740  | AvgNLL: 0.1346  PPL_Prob: 0.8740\n",
      "[01] Token: \u001b[48;2;101;179;127m    'This'     \u001b[0m | LogProb:  -1.0508  Prob:  0.3496  | AvgNLL: 0.5927  PPL_Prob: 0.5528\n",
      "[02] Token: \u001b[48;2;76;170;153m     ' is'     \u001b[0m | LogProb:  -0.0059  Prob:  0.9941  | AvgNLL: 0.3971  PPL_Prob: 0.6723\n",
      "[03] Token: \u001b[48;2;50;161;178m     ' a'      \u001b[0m | LogProb:  -0.0200  Prob:  0.9800  | AvgNLL: 0.3028  PPL_Prob: 0.7387\n",
      "[04] Token: \u001b[48;2;50;161;178m  ' classic'   \u001b[0m | LogProb:  -0.0489  Prob:  0.9521  | AvgNLL: 0.2520  PPL_Prob: 0.7772\n",
      "[05] Token: \u001b[48;2;50;161;178m' philosophical'\u001b[0m | LogProb:  -0.8018  Prob:  0.4485  | AvgNLL: 0.3437  PPL_Prob: 0.7092\n",
      "[06] Token: \u001b[48;2;76;170;153m  ' thought'   \u001b[0m | LogProb:  -0.4612  Prob:  0.6304  | AvgNLL: 0.3605  PPL_Prob: 0.6974\n",
      "[07] Token: \u001b[48;2;50;161;178m ' experiment' \u001b[0m | LogProb:  -0.0017  Prob:  0.9985  | AvgNLL: 0.3156  PPL_Prob: 0.7293\n",
      "[08] Token: \u001b[48;2;76;170;153m      ','      \u001b[0m | LogProb:  -0.6919  Prob:  0.5005  | AvgNLL: 0.3574  PPL_Prob: 0.6995\n",
      "[09] Token: \u001b[48;2;76;170;153m   ' often'    \u001b[0m | LogProb:  -0.4124  Prob:  0.6621  | AvgNLL: 0.3629  PPL_Prob: 0.6956\n",
      "[10] Token: \u001b[48;2;76;170;153m   ' called'   \u001b[0m | LogProb:  -1.0166  Prob:  0.3618  | AvgNLL: 0.4223  PPL_Prob: 0.6555\n",
      "[11] Token: \u001b[48;2;76;170;153m    ' the'     \u001b[0m | LogProb:  -0.0052  Prob:  0.9946  | AvgNLL: 0.3876  PPL_Prob: 0.6787\n",
      "[12] Token: \u001b[48;2;76;170;153m     ' Li'     \u001b[0m | LogProb:  -0.3530  Prob:  0.7026  | AvgNLL: 0.3849  PPL_Prob: 0.6805\n",
      "[13] Token: \u001b[48;2;76;170;153m     'ar'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.3574  PPL_Prob: 0.6995\n",
      "[14] Token: \u001b[48;2;50;161;178m  ' Paradox'   \u001b[0m | LogProb:  -0.0079  Prob:  0.9922  | AvgNLL: 0.3341  PPL_Prob: 0.7160\n",
      "[15] Token: \u001b[48;2;50;161;178m      '.'      \u001b[0m | LogProb:  -0.0018  Prob:  0.9980  | AvgNLL: 0.3134  PPL_Prob: 0.7310\n",
      "[16] Token: \u001b[48;2;50;161;178m     ' It'     \u001b[0m | LogProb:  -0.6318  Prob:  0.5317  | AvgNLL: 0.3321  PPL_Prob: 0.7174\n",
      "[17] Token: \u001b[48;2;76;170;153m ' highlights' \u001b[0m | LogProb:  -0.8540  Prob:  0.4258  | AvgNLL: 0.3611  PPL_Prob: 0.6969\n",
      "[18] Token: \u001b[48;2;76;170;153m    ' the'     \u001b[0m | LogProb:  -0.5112  Prob:  0.5996  | AvgNLL: 0.3690  PPL_Prob: 0.6914\n",
      "[19] Token: \u001b[48;2;76;170;153m ' challenges' \u001b[0m | LogProb:  -1.5449  Prob:  0.2134  | AvgNLL: 0.4278  PPL_Prob: 0.6520\n",
      "[20] Token: \u001b[48;2;76;170;153m     ' of'     \u001b[0m | LogProb:  -0.2330  Prob:  0.7920  | AvgNLL: 0.4185  PPL_Prob: 0.6580\n",
      "[21] Token: \u001b[48;2;76;170;153m  ' defining'  \u001b[0m | LogProb:  -0.7290  Prob:  0.4824  | AvgNLL: 0.4326  PPL_Prob: 0.6488\n",
      "[22] Token: \u001b[48;2;76;170;153m   ' truth'    \u001b[0m | LogProb:  -0.0042  Prob:  0.9956  | AvgNLL: 0.4140  PPL_Prob: 0.6610\n",
      "[23] Token: \u001b[48;2;76;170;153m    ' and'     \u001b[0m | LogProb:  -0.1727  Prob:  0.8413  | AvgNLL: 0.4039  PPL_Prob: 0.6677\n",
      "[24] Token: \u001b[48;2;76;170;153m' consistency' \u001b[0m | LogProb:  -1.0029  Prob:  0.3667  | AvgNLL: 0.4279  PPL_Prob: 0.6519\n",
      "[25] Token: \u001b[48;2;76;170;153m      '.'      \u001b[0m | LogProb:  -0.1881  Prob:  0.8286  | AvgNLL: 0.4187  PPL_Prob: 0.6579\n",
      "[26] Token: \u001b[48;2;76;170;153m    '\\n\\n'     \u001b[0m | LogProb:  -0.6060  Prob:  0.5454  | AvgNLL: 0.4256  PPL_Prob: 0.6534\n",
      "[27] Token: \u001b[48;2;76;170;153m    'Here'     \u001b[0m | LogProb:  -0.1897  Prob:  0.8271  | AvgNLL: 0.4172  PPL_Prob: 0.6589\n",
      "[28] Token: \u001b[48;2;76;170;153m      \"'\"      \u001b[0m | LogProb:  -0.1908  Prob:  0.8262  | AvgNLL: 0.4094  PPL_Prob: 0.6641\n",
      "[29] Token: \u001b[48;2;76;170;153m      's'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.3957  PPL_Prob: 0.6732\n",
      "[30] Token: \u001b[48;2;76;170;153m     ' a'      \u001b[0m | LogProb:  -0.9185  Prob:  0.3992  | AvgNLL: 0.4126  PPL_Prob: 0.6619\n",
      "[31] Token: \u001b[48;2;76;170;153m ' breakdown'  \u001b[0m | LogProb:  -0.0007  Prob:  0.9995  | AvgNLL: 0.3997  PPL_Prob: 0.6705\n",
      "[32] Token: \u001b[48;2;76;170;153m     ' of'     \u001b[0m | LogProb:  -0.0925  Prob:  0.9116  | AvgNLL: 0.3904  PPL_Prob: 0.6768\n",
      "[33] Token: \u001b[48;2;76;170;153m    ' the'     \u001b[0m | LogProb:  -0.3181  Prob:  0.7275  | AvgNLL: 0.3883  PPL_Prob: 0.6782\n",
      "[34] Token: \u001b[48;2;76;170;153m  ' paradox'   \u001b[0m | LogProb:  -0.5107  Prob:  0.6001  | AvgNLL: 0.3918  PPL_Prob: 0.6758\n",
      "[35] Token: \u001b[48;2;76;170;153m      ':'      \u001b[0m | LogProb:  -0.1140  Prob:  0.8921  | AvgNLL: 0.3841  PPL_Prob: 0.6811\n",
      "[36] Token: \u001b[48;2;76;170;153m    '\\n\\n'     \u001b[0m | LogProb:  -0.0022  Prob:  0.9980  | AvgNLL: 0.3738  PPL_Prob: 0.6881\n",
      "[37] Token: \u001b[48;2;76;170;153m      '*'      \u001b[0m | LogProb:  -0.0163  Prob:  0.9839  | AvgNLL: 0.3643  PPL_Prob: 0.6947\n",
      "[38] Token: \u001b[48;2;50;161;178m     ' **'     \u001b[0m | LogProb:  -0.0365  Prob:  0.9644  | AvgNLL: 0.3559  PPL_Prob: 0.7005\n",
      "[39] Token: \u001b[48;2;76;170;153m     'If'      \u001b[0m | LogProb:  -0.5244  Prob:  0.5918  | AvgNLL: 0.3601  PPL_Prob: 0.6976\n",
      "[40] Token: \u001b[48;2;50;161;178m    ' you'     \u001b[0m | LogProb:  -0.0055  Prob:  0.9946  | AvgNLL: 0.3515  PPL_Prob: 0.7036\n",
      "[41] Token: \u001b[48;2;50;161;178m    ' say'     \u001b[0m | LogProb:  -0.1923  Prob:  0.8252  | AvgNLL: 0.3477  PPL_Prob: 0.7063\n",
      "[42] Token: \u001b[48;2;50;161;178m     ' \"'      \u001b[0m | LogProb:  -0.0277  Prob:  0.9727  | AvgNLL: 0.3403  PPL_Prob: 0.7116\n",
      "[43] Token: \u001b[48;2;50;161;178m      'I'      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.3325  PPL_Prob: 0.7171\n",
      "[44] Token: \u001b[48;2;50;161;178m     ' am'     \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.3251  PPL_Prob: 0.7224\n",
      "[45] Token: \u001b[48;2;50;161;178m   ' lying'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.3181  PPL_Prob: 0.7275\n",
      "[46] Token: \u001b[48;2;50;161;178m     ',\"'      \u001b[0m | LogProb:  -0.2988  Prob:  0.7417  | AvgNLL: 0.3177  PPL_Prob: 0.7278\n",
      "[47] Token: \u001b[48;2;50;161;178m     '**'      \u001b[0m | LogProb:  -0.0462  Prob:  0.9551  | AvgNLL: 0.3120  PPL_Prob: 0.7320\n",
      "[48] Token: \u001b[48;2;50;161;178m    ' then'    \u001b[0m | LogProb:  -0.7715  Prob:  0.4624  | AvgNLL: 0.3214  PPL_Prob: 0.7251\n",
      "[49] Token: \u001b[48;2;50;161;178m    ' you'     \u001b[0m | LogProb:  -0.6177  Prob:  0.5391  | AvgNLL: 0.3273  PPL_Prob: 0.7209\n",
      "[50] Token: \u001b[48;2;50;161;178m    ' are'     \u001b[0m | LogProb:  -0.6851  Prob:  0.5039  | AvgNLL: 0.3343  PPL_Prob: 0.7158\n",
      "[51] Token: \u001b[48;2;50;161;178m  ' claiming'  \u001b[0m | LogProb:  -0.9976  Prob:  0.3689  | AvgNLL: 0.3471  PPL_Prob: 0.7067\n",
      "[52] Token: \u001b[48;2;50;161;178m    ' that'    \u001b[0m | LogProb:  -0.6782  Prob:  0.5073  | AvgNLL: 0.3533  PPL_Prob: 0.7023\n",
      "[53] Token: \u001b[48;2;50;161;178m    ' you'     \u001b[0m | LogProb:  -0.1329  Prob:  0.8755  | AvgNLL: 0.3493  PPL_Prob: 0.7052\n",
      "[54] Token: \u001b[48;2;50;161;178m    ' are'     \u001b[0m | LogProb:  -0.4143  Prob:  0.6606  | AvgNLL: 0.3504  PPL_Prob: 0.7044\n",
      "[55] Token: \u001b[48;2;50;161;178m    ' not'     \u001b[0m | LogProb:  -0.5278  Prob:  0.5898  | AvgNLL: 0.3536  PPL_Prob: 0.7022\n",
      "[56] Token: \u001b[48;2;50;161;178m   ' lying'    \u001b[0m | LogProb:  -0.0817  Prob:  0.9214  | AvgNLL: 0.3488  PPL_Prob: 0.7055\n",
      "[57] Token: \u001b[48;2;50;161;178m      '.'      \u001b[0m | LogProb:  -0.0303  Prob:  0.9702  | AvgNLL: 0.3433  PPL_Prob: 0.7094\n",
      "[58] Token: \u001b[48;2;50;161;178m     '  '      \u001b[0m | LogProb:  -0.9775  Prob:  0.3762  | AvgNLL: 0.3541  PPL_Prob: 0.7018\n",
      "[59] Token: \u001b[48;2;76;170;153m     'But'     \u001b[0m | LogProb:  -0.7842  Prob:  0.4565  | AvgNLL: 0.3613  PPL_Prob: 0.6968\n",
      "[60] Token: \u001b[48;2;76;170;153m     ' if'     \u001b[0m | LogProb:  -0.0820  Prob:  0.9214  | AvgNLL: 0.3567  PPL_Prob: 0.7000\n",
      "[61] Token: \u001b[48;2;50;161;178m    ' you'     \u001b[0m | LogProb:  -0.0102  Prob:  0.9897  | AvgNLL: 0.3511  PPL_Prob: 0.7039\n",
      "[62] Token: \u001b[48;2;50;161;178m    ' are'     \u001b[0m | LogProb:  -0.3191  Prob:  0.7271  | AvgNLL: 0.3506  PPL_Prob: 0.7043\n",
      "[63] Token: \u001b[48;2;50;161;178m    ' not'     \u001b[0m | LogProb:  -0.0155  Prob:  0.9844  | AvgNLL: 0.3453  PPL_Prob: 0.7080\n",
      "[64] Token: \u001b[48;2;50;161;178m   ' lying'    \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.3400  PPL_Prob: 0.7117\n",
      "[65] Token: \u001b[48;2;50;161;178m      ','      \u001b[0m | LogProb:  -0.0726  Prob:  0.9302  | AvgNLL: 0.3360  PPL_Prob: 0.7146\n",
      "[66] Token: \u001b[48;2;50;161;178m    ' then'    \u001b[0m | LogProb:  -0.0039  Prob:  0.9961  | AvgNLL: 0.3310  PPL_Prob: 0.7182\n",
      "[67] Token: \u001b[48;2;50;161;178m    ' you'     \u001b[0m | LogProb:  -0.1758  Prob:  0.8389  | AvgNLL: 0.3287  PPL_Prob: 0.7198\n",
      "[68] Token: \u001b[48;2;50;161;178m     ' *'      \u001b[0m | LogProb:  -0.1248  Prob:  0.8828  | AvgNLL: 0.3258  PPL_Prob: 0.7220\n",
      "[69] Token: \u001b[48;2;50;161;178m     'are'     \u001b[0m | LogProb:  -0.0303  Prob:  0.9702  | AvgNLL: 0.3216  PPL_Prob: 0.7250\n",
      "[70] Token: \u001b[48;2;50;161;178m      '*'      \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.3170  PPL_Prob: 0.7283\n",
      "[71] Token: \u001b[48;2;50;161;178m   ' lying'    \u001b[0m | LogProb:  -0.0071  Prob:  0.9927  | AvgNLL: 0.3127  PPL_Prob: 0.7314\n",
      "[72] Token: \u001b[48;2;50;161;178m      '.'      \u001b[0m | LogProb:  -0.0568  Prob:  0.9448  | AvgNLL: 0.3092  PPL_Prob: 0.7340\n",
      "[73] Token: \u001b[48;2;50;161;178m    '\\n\\n'     \u001b[0m | LogProb:  -1.0068  Prob:  0.3655  | AvgNLL: 0.3187  PPL_Prob: 0.7271\n",
      "[74] Token: \u001b[48;2;50;161;178m      '*'      \u001b[0m | LogProb:  -0.0402  Prob:  0.9604  | AvgNLL: 0.3149  PPL_Prob: 0.7298\n",
      "[75] Token: \u001b[48;2;50;161;178m     ' **'     \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.3108  PPL_Prob: 0.7329\n",
      "[76] Token: \u001b[48;2;50;161;178m     'If'      \u001b[0m | LogProb:  -0.0272  Prob:  0.9731  | AvgNLL: 0.3071  PPL_Prob: 0.7356\n",
      "[77] Token: \u001b[48;2;50;161;178m    ' you'     \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.3032  PPL_Prob: 0.7385\n",
      "[78] Token: \u001b[48;2;50;161;178m    ' say'     \u001b[0m | LogProb:  -0.0008  Prob:  0.9990  | AvgNLL: 0.2994  PPL_Prob: 0.7413\n",
      "[79] Token: \u001b[48;2;50;161;178m     ' \"'      \u001b[0m | LogProb:  -0.0218  Prob:  0.9785  | AvgNLL: 0.2959  PPL_Prob: 0.7439\n",
      "[80] Token: \u001b[48;2;50;161;178m      'I'      \u001b[0m | LogProb:  -0.0972  Prob:  0.9072  | AvgNLL: 0.2934  PPL_Prob: 0.7457\n",
      "[81] Token: \u001b[48;2;50;161;178m     ' am'     \u001b[0m | LogProb:  -0.0012  Prob:  0.9990  | AvgNLL: 0.2899  PPL_Prob: 0.7484\n",
      "[82] Token: \u001b[48;2;50;161;178m    ' not'     \u001b[0m | LogProb:  -0.1393  Prob:  0.8701  | AvgNLL: 0.2881  PPL_Prob: 0.7497\n",
      "[83] Token: \u001b[48;2;50;161;178m   ' lying'    \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2846  PPL_Prob: 0.7523\n",
      "[84] Token: \u001b[48;2;50;161;178m     ',\"'      \u001b[0m | LogProb:  -0.0046  Prob:  0.9956  | AvgNLL: 0.2813  PPL_Prob: 0.7548\n",
      "[85] Token: \u001b[48;2;50;161;178m     '**'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2781  PPL_Prob: 0.7572\n",
      "[86] Token: \u001b[48;2;50;161;178m    ' then'    \u001b[0m | LogProb:  -0.0026  Prob:  0.9976  | AvgNLL: 0.2749  PPL_Prob: 0.7597\n",
      "[87] Token: \u001b[48;2;50;161;178m    ' you'     \u001b[0m | LogProb:  -0.0010  Prob:  0.9990  | AvgNLL: 0.2718  PPL_Prob: 0.7620\n",
      "[88] Token: \u001b[48;2;50;161;178m    ' are'     \u001b[0m | LogProb:  -0.0151  Prob:  0.9849  | AvgNLL: 0.2689  PPL_Prob: 0.7642\n",
      "[89] Token: \u001b[48;2;50;161;178m  ' claiming'  \u001b[0m | LogProb:  -0.0010  Prob:  0.9990  | AvgNLL: 0.2659  PPL_Prob: 0.7665\n",
      "[90] Token: \u001b[48;2;50;161;178m    ' that'    \u001b[0m | LogProb:  -0.0241  Prob:  0.9761  | AvgNLL: 0.2633  PPL_Prob: 0.7685\n",
      "[91] Token: \u001b[48;2;50;161;178m    ' you'     \u001b[0m | LogProb:  -0.0010  Prob:  0.9990  | AvgNLL: 0.2604  PPL_Prob: 0.7707\n",
      "[92] Token: \u001b[48;2;50;161;178m     ' *'      \u001b[0m | LogProb:  -0.4822  Prob:  0.6177  | AvgNLL: 0.2628  PPL_Prob: 0.7689\n",
      "[93] Token: \u001b[48;2;50;161;178m     'are'     \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.2600  PPL_Prob: 0.7710\n",
      "[94] Token: \u001b[48;2;50;161;178m      '*'      \u001b[0m | LogProb:  -0.0004  Prob:  0.9995  | AvgNLL: 0.2573  PPL_Prob: 0.7732\n",
      "[95] Token: \u001b[48;2;50;161;178m   ' lying'    \u001b[0m | LogProb:  -0.0009  Prob:  0.9990  | AvgNLL: 0.2546  PPL_Prob: 0.7752\n",
      "[96] Token: \u001b[48;2;50;161;178m      '.'      \u001b[0m | LogProb:  -0.0058  Prob:  0.9941  | AvgNLL: 0.2520  PPL_Prob: 0.7772\n",
      "[97] Token: \u001b[48;2;50;161;178m    ' But'     \u001b[0m | LogProb:  -0.1838  Prob:  0.8320  | AvgNLL: 0.2513  PPL_Prob: 0.7778\n",
      "[98] Token: \u001b[48;2;50;161;178m     ' if'     \u001b[0m | LogProb:  -0.0058  Prob:  0.9941  | AvgNLL: 0.2489  PPL_Prob: 0.7797\n",
      "[99] Token: \u001b[48;2;50;161;178m    ' you'     \u001b[0m | LogProb:  -0.0003  Prob:  0.9995  | AvgNLL: 0.2464  PPL_Prob: 0.7816\n",
      "\n",
      "== Self Perplexity ==\n",
      "Avg NLL: 0.2464\n",
      "Perplexity (self): 0.7816\n",
      "Input: If you are your brother's sibling, who are you?\n",
      "\n",
      "[00] Token: \u001b[48;2;0;143;230m    '\\n\\n'     \u001b[0m | LogProb:  -0.0308  Prob:  0.9697  | AvgNLL: 0.0308  PPL_Prob: 0.9697\n",
      "[01] Token: \u001b[48;2;0;143;230m    'This'     \u001b[0m | LogProb:  -0.0586  Prob:  0.9429  | AvgNLL: 0.0447  PPL_Prob: 0.9563\n",
      "[02] Token: \u001b[48;2;0;143;230m     ' is'     \u001b[0m | LogProb:  -0.0899  Prob:  0.9141  | AvgNLL: 0.0598  PPL_Prob: 0.9420\n",
      "[03] Token: \u001b[48;2;0;143;230m     ' a'      \u001b[0m | LogProb:  -0.0899  Prob:  0.9141  | AvgNLL: 0.0673  PPL_Prob: 0.9349\n",
      "[04] Token: \u001b[48;2;76;170;153m   ' famous'   \u001b[0m | LogProb:  -1.7090  Prob:  0.1810  | AvgNLL: 0.3956  PPL_Prob: 0.6733\n",
      "[05] Token: \u001b[48;2;101;179;127m   ' quote'    \u001b[0m | LogProb:  -1.1416  Prob:  0.3193  | AvgNLL: 0.5200  PPL_Prob: 0.5945\n",
      "[06] Token: \u001b[48;2;76;170;153m    ' from'    \u001b[0m | LogProb:  -0.3633  Prob:  0.6953  | AvgNLL: 0.4976  PPL_Prob: 0.6080\n",
      "[07] Token: \u001b[48;2;76;170;153m    ' the'     \u001b[0m | LogProb:  -0.1425  Prob:  0.8672  | AvgNLL: 0.4532  PPL_Prob: 0.6356\n",
      "[08] Token: \u001b[48;2;101;179;127m    ' book'    \u001b[0m | LogProb:  -1.7285  Prob:  0.1775  | AvgNLL: 0.5949  PPL_Prob: 0.5516\n",
      "[09] Token: \u001b[48;2;101;179;127m     ' *'      \u001b[0m | LogProb:  -0.7563  Prob:  0.4695  | AvgNLL: 0.6110  PPL_Prob: 0.5428\n",
      "[10] Token: \u001b[48;2;101;179;127m     'The'     \u001b[0m | LogProb:  -0.2085  Prob:  0.8120  | AvgNLL: 0.5744  PPL_Prob: 0.5630\n",
      "[11] Token: \u001b[48;2;101;179;127m  ' Brothers'  \u001b[0m | LogProb:  -1.5117  Prob:  0.2206  | AvgNLL: 0.6525  PPL_Prob: 0.5207\n",
      "[12] Token: \u001b[48;2;101;179;127m     '*.'      \u001b[0m | LogProb:  -0.0798  Prob:  0.9233  | AvgNLL: 0.6085  PPL_Prob: 0.5442\n",
      "[13] Token: \u001b[48;2;101;179;127m     ' It'     \u001b[0m | LogProb:  -0.1637  Prob:  0.8491  | AvgNLL: 0.5767  PPL_Prob: 0.5617\n",
      "[14] Token: \u001b[48;2;101;179;127m      \"'\"      \u001b[0m | LogProb:  -0.5850  Prob:  0.5571  | AvgNLL: 0.5773  PPL_Prob: 0.5614\n",
      "[15] Token: \u001b[48;2;101;179;127m      's'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.5412  PPL_Prob: 0.5821\n",
      "[16] Token: \u001b[48;2;101;179;127m     ' a'      \u001b[0m | LogProb:  -0.1289  Prob:  0.8789  | AvgNLL: 0.5169  PPL_Prob: 0.5963\n",
      "[17] Token: \u001b[48;2;101;179;127m  ' profound'  \u001b[0m | LogProb:  -1.7871  Prob:  0.1675  | AvgNLL: 0.5875  PPL_Prob: 0.5557\n",
      "[18] Token: \u001b[48;2;101;179;127m  ' question'  \u001b[0m | LogProb:  -0.7695  Prob:  0.4631  | AvgNLL: 0.5971  PPL_Prob: 0.5504\n",
      "[19] Token: \u001b[48;2;101;179;127m    ' that'    \u001b[0m | LogProb:  -0.4116  Prob:  0.6626  | AvgNLL: 0.5878  PPL_Prob: 0.5555\n",
      "[20] Token: \u001b[48;2;101;179;127m  ' explores'  \u001b[0m | LogProb:  -0.7061  Prob:  0.4937  | AvgNLL: 0.5934  PPL_Prob: 0.5524\n",
      "[21] Token: \u001b[48;2;101;179;127m    ' the'     \u001b[0m | LogProb:  -0.2300  Prob:  0.7944  | AvgNLL: 0.5769  PPL_Prob: 0.5616\n",
      "[22] Token: \u001b[48;2;101;179;127m' complexities'\u001b[0m | LogProb:  -0.3755  Prob:  0.6870  | AvgNLL: 0.5682  PPL_Prob: 0.5666\n",
      "[23] Token: \u001b[48;2;101;179;127m     ' of'     \u001b[0m | LogProb:  -0.0018  Prob:  0.9980  | AvgNLL: 0.5446  PPL_Prob: 0.5801\n",
      "[24] Token: \u001b[48;2;101;179;127m   ' family'   \u001b[0m | LogProb:  -0.1405  Prob:  0.8691  | AvgNLL: 0.5284  PPL_Prob: 0.5895\n",
      "[25] Token: \u001b[48;2;101;179;127m' relationships'\u001b[0m | LogProb:  -0.1525  Prob:  0.8584  | AvgNLL: 0.5139  PPL_Prob: 0.5981\n",
      "[26] Token: \u001b[48;2;76;170;153m    ' and'     \u001b[0m | LogProb:  -0.3335  Prob:  0.7163  | AvgNLL: 0.5073  PPL_Prob: 0.6021\n",
      "[27] Token: \u001b[48;2;76;170;153m    ' the'     \u001b[0m | LogProb:  -0.2319  Prob:  0.7930  | AvgNLL: 0.4974  PPL_Prob: 0.6081\n",
      "[28] Token: \u001b[48;2;101;179;127m   ' often'    \u001b[0m | LogProb:  -1.3750  Prob:  0.2529  | AvgNLL: 0.5277  PPL_Prob: 0.5900\n",
      "[29] Token: \u001b[48;2;101;179;127m      '-'      \u001b[0m | LogProb:  -0.0891  Prob:  0.9150  | AvgNLL: 0.5131  PPL_Prob: 0.5987\n",
      "[30] Token: \u001b[48;2;76;170;153m     'un'      \u001b[0m | LogProb:  -0.2896  Prob:  0.7485  | AvgNLL: 0.5059  PPL_Prob: 0.6030\n",
      "[31] Token: \u001b[48;2;101;179;127m   'spoken'    \u001b[0m | LogProb:  -1.0732  Prob:  0.3418  | AvgNLL: 0.5236  PPL_Prob: 0.5924\n",
      "[32] Token: \u001b[48;2;101;179;127m  ' dynamics'  \u001b[0m | LogProb:  -0.6099  Prob:  0.5435  | AvgNLL: 0.5262  PPL_Prob: 0.5908\n",
      "[33] Token: \u001b[48;2;101;179;127m   ' within'   \u001b[0m | LogProb:  -0.2361  Prob:  0.7896  | AvgNLL: 0.5177  PPL_Prob: 0.5959\n",
      "[34] Token: \u001b[48;2;76;170;153m    ' them'    \u001b[0m | LogProb:  -0.0218  Prob:  0.9785  | AvgNLL: 0.5035  PPL_Prob: 0.6044\n",
      "[35] Token: \u001b[48;2;76;170;153m      '.'      \u001b[0m | LogProb:  -0.0006  Prob:  0.9995  | AvgNLL: 0.4895  PPL_Prob: 0.6129\n",
      "[36] Token: \u001b[48;2;76;170;153m    '\\n\\n'     \u001b[0m | LogProb:  -0.8828  Prob:  0.4136  | AvgNLL: 0.5002  PPL_Prob: 0.6064\n",
      "[37] Token: \u001b[48;2;76;170;153m    'Here'     \u001b[0m | LogProb:  -0.4878  Prob:  0.6138  | AvgNLL: 0.4998  PPL_Prob: 0.6066\n",
      "[38] Token: \u001b[48;2;76;170;153m      \"'\"      \u001b[0m | LogProb:  -0.1843  Prob:  0.8315  | AvgNLL: 0.4917  PPL_Prob: 0.6116\n",
      "[39] Token: \u001b[48;2;76;170;153m      's'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.4795  PPL_Prob: 0.6191\n",
      "[40] Token: \u001b[48;2;76;170;153m     ' a'      \u001b[0m | LogProb:  -0.0027  Prob:  0.9971  | AvgNLL: 0.4678  PPL_Prob: 0.6264\n",
      "[41] Token: \u001b[48;2;76;170;153m ' breakdown'  \u001b[0m | LogProb:  -0.0158  Prob:  0.9844  | AvgNLL: 0.4571  PPL_Prob: 0.6331\n",
      "[42] Token: \u001b[48;2;76;170;153m     ' of'     \u001b[0m | LogProb:  -0.0087  Prob:  0.9912  | AvgNLL: 0.4466  PPL_Prob: 0.6398\n",
      "[43] Token: \u001b[48;2;76;170;153m    ' what'    \u001b[0m | LogProb:  -0.5469  Prob:  0.5786  | AvgNLL: 0.4489  PPL_Prob: 0.6383\n",
      "[44] Token: \u001b[48;2;76;170;153m     ' it'     \u001b[0m | LogProb:  -0.3928  Prob:  0.6753  | AvgNLL: 0.4477  PPL_Prob: 0.6391\n",
      "[45] Token: \u001b[48;2;76;170;153m   ' means'    \u001b[0m | LogProb:  -0.3342  Prob:  0.7158  | AvgNLL: 0.4452  PPL_Prob: 0.6407\n",
      "[46] Token: \u001b[48;2;76;170;153m      ':'      \u001b[0m | LogProb:  -0.3752  Prob:  0.6870  | AvgNLL: 0.4437  PPL_Prob: 0.6417\n",
      "[47] Token: \u001b[48;2;76;170;153m    '\\n\\n'     \u001b[0m | LogProb:  -0.0004  Prob:  0.9995  | AvgNLL: 0.4345  PPL_Prob: 0.6476\n",
      "[48] Token: \u001b[48;2;76;170;153m      '*'      \u001b[0m | LogProb:  -0.0028  Prob:  0.9971  | AvgNLL: 0.4257  PPL_Prob: 0.6533\n",
      "[49] Token: \u001b[48;2;76;170;153m     ' **'     \u001b[0m | LogProb:  -0.2891  Prob:  0.7490  | AvgNLL: 0.4229  PPL_Prob: 0.6551\n",
      "[50] Token: \u001b[48;2;76;170;153m     'The'     \u001b[0m | LogProb:  -0.1752  Prob:  0.8394  | AvgNLL: 0.4181  PPL_Prob: 0.6583\n",
      "[51] Token: \u001b[48;2;76;170;153m  ' Brothers'  \u001b[0m | LogProb:  -0.1229  Prob:  0.8843  | AvgNLL: 0.4124  PPL_Prob: 0.6621\n",
      "[52] Token: \u001b[48;2;76;170;153m     ':**'     \u001b[0m | LogProb:  -0.0861  Prob:  0.9175  | AvgNLL: 0.4062  PPL_Prob: 0.6661\n",
      "[53] Token: \u001b[48;2;76;170;153m    ' The'     \u001b[0m | LogProb:  -0.8657  Prob:  0.4207  | AvgNLL: 0.4148  PPL_Prob: 0.6605\n",
      "[54] Token: \u001b[48;2;76;170;153m   ' story'    \u001b[0m | LogProb:  -0.8652  Prob:  0.4209  | AvgNLL: 0.4229  PPL_Prob: 0.6551\n",
      "[55] Token: \u001b[48;2;76;170;153m  ' centers'   \u001b[0m | LogProb:  -0.6621  Prob:  0.5156  | AvgNLL: 0.4272  PPL_Prob: 0.6523\n",
      "[56] Token: \u001b[48;2;76;170;153m   ' around'   \u001b[0m | LogProb:  -0.3442  Prob:  0.7090  | AvgNLL: 0.4258  PPL_Prob: 0.6533\n",
      "[57] Token: \u001b[48;2;76;170;153m    ' two'     \u001b[0m | LogProb:  -0.4175  Prob:  0.6587  | AvgNLL: 0.4256  PPL_Prob: 0.6534\n",
      "[58] Token: \u001b[48;2;76;170;153m  ' brothers'  \u001b[0m | LogProb:  -0.0109  Prob:  0.9893  | AvgNLL: 0.4186  PPL_Prob: 0.6580\n",
      "[59] Token: \u001b[48;2;76;170;153m      ','      \u001b[0m | LogProb:  -0.0668  Prob:  0.9355  | AvgNLL: 0.4127  PPL_Prob: 0.6618\n",
      "[60] Token: \u001b[48;2;76;170;153m     ' a'      \u001b[0m | LogProb:  -2.8965  Prob:  0.0552  | AvgNLL: 0.4534  PPL_Prob: 0.6354\n",
      "[61] Token: \u001b[48;2;76;170;153m  ' brother'   \u001b[0m | LogProb:  -1.2295  Prob:  0.2925  | AvgNLL: 0.4660  PPL_Prob: 0.6275\n",
      "[62] Token: \u001b[48;2;76;170;153m    ' and'     \u001b[0m | LogProb:  -0.1549  Prob:  0.8564  | AvgNLL: 0.4610  PPL_Prob: 0.6306\n",
      "[63] Token: \u001b[48;2;76;170;153m    ' his'     \u001b[0m | LogProb:  -0.6689  Prob:  0.5122  | AvgNLL: 0.4643  PPL_Prob: 0.6286\n",
      "[64] Token: \u001b[48;2;76;170;153m  ' sibling'   \u001b[0m | LogProb:  -0.4287  Prob:  0.6514  | AvgNLL: 0.4637  PPL_Prob: 0.6289\n",
      "[65] Token: \u001b[48;2;76;170;153m      '.'      \u001b[0m | LogProb:  -0.2355  Prob:  0.7900  | AvgNLL: 0.4603  PPL_Prob: 0.6311\n",
      "[66] Token: \u001b[48;2;76;170;153m     '\\n'      \u001b[0m | LogProb:  -0.3579  Prob:  0.6992  | AvgNLL: 0.4587  PPL_Prob: 0.6321\n",
      "[67] Token: \u001b[48;2;76;170;153m      '*'      \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.4520  PPL_Prob: 0.6364\n",
      "[68] Token: \u001b[48;2;76;170;153m     ' **'     \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.4454  PPL_Prob: 0.6405\n",
      "[69] Token: \u001b[48;2;76;170;153m      '\"'      \u001b[0m | LogProb:  -0.3748  Prob:  0.6875  | AvgNLL: 0.4444  PPL_Prob: 0.6412\n",
      "[70] Token: \u001b[48;2;76;170;153m     'If'      \u001b[0m | LogProb:  -0.3196  Prob:  0.7266  | AvgNLL: 0.4427  PPL_Prob: 0.6423\n",
      "[71] Token: \u001b[48;2;76;170;153m    ' you'     \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.4365  PPL_Prob: 0.6463\n",
      "[72] Token: \u001b[48;2;76;170;153m    ' are'     \u001b[0m | LogProb:  -0.0003  Prob:  0.9995  | AvgNLL: 0.4305  PPL_Prob: 0.6502\n",
      "[73] Token: \u001b[48;2;76;170;153m    ' your'    \u001b[0m | LogProb:  -0.0063  Prob:  0.9937  | AvgNLL: 0.4248  PPL_Prob: 0.6539\n",
      "[74] Token: \u001b[48;2;76;170;153m  ' brother'   \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.4192  PPL_Prob: 0.6576\n",
      "[75] Token: \u001b[48;2;76;170;153m      \"'\"      \u001b[0m | LogProb:  -0.0013  Prob:  0.9985  | AvgNLL: 0.4137  PPL_Prob: 0.6612\n",
      "[76] Token: \u001b[48;2;76;170;153m      's'      \u001b[0m | LogProb:   0.0000  Prob:  1.0000  | AvgNLL: 0.4083  PPL_Prob: 0.6648\n",
      "[77] Token: \u001b[48;2;76;170;153m  ' sibling'   \u001b[0m | LogProb:  -0.0002  Prob:  1.0000  | AvgNLL: 0.4031  PPL_Prob: 0.6683\n",
      "[78] Token: \u001b[48;2;76;170;153m    '...\"'     \u001b[0m | LogProb:  -0.3650  Prob:  0.6943  | AvgNLL: 0.4026  PPL_Prob: 0.6686\n",
      "[79] Token: \u001b[48;2;76;170;153m     '**'      \u001b[0m | LogProb:  -0.5728  Prob:  0.5640  | AvgNLL: 0.4047  PPL_Prob: 0.6672\n",
      "[80] Token: \u001b[48;2;76;170;153m    ' This'    \u001b[0m | LogProb:  -0.4973  Prob:  0.6084  | AvgNLL: 0.4058  PPL_Prob: 0.6664\n",
      "[81] Token: \u001b[48;2;76;170;153m     ' is'     \u001b[0m | LogProb:  -0.2295  Prob:  0.7949  | AvgNLL: 0.4037  PPL_Prob: 0.6679\n",
      "[82] Token: \u001b[48;2;76;170;153m    ' the'     \u001b[0m | LogProb:  -0.0079  Prob:  0.9922  | AvgNLL: 0.3989  PPL_Prob: 0.6710\n",
      "[83] Token: \u001b[48;2;76;170;153m    ' core'    \u001b[0m | LogProb:  -0.1232  Prob:  0.8843  | AvgNLL: 0.3956  PPL_Prob: 0.6732\n",
      "[84] Token: \u001b[48;2;76;170;153m     ' of'     \u001b[0m | LogProb:  -0.0210  Prob:  0.9790  | AvgNLL: 0.3912  PPL_Prob: 0.6762\n",
      "[85] Token: \u001b[48;2;76;170;153m    ' the'     \u001b[0m | LogProb:  -0.0001  Prob:  1.0000  | AvgNLL: 0.3867  PPL_Prob: 0.6793\n",
      "[86] Token: \u001b[48;2;76;170;153m  ' question'  \u001b[0m | LogProb:  -0.2900  Prob:  0.7480  | AvgNLL: 0.3856  PPL_Prob: 0.6801\n",
      "[87] Token: \u001b[48;2;76;170;153m      '.'      \u001b[0m | LogProb:  -0.0253  Prob:  0.9751  | AvgNLL: 0.3815  PPL_Prob: 0.6829\n",
      "[88] Token: \u001b[48;2;76;170;153m     ' It'     \u001b[0m | LogProb:  -0.1559  Prob:  0.8555  | AvgNLL: 0.3789  PPL_Prob: 0.6846\n",
      "[89] Token: \u001b[48;2;76;170;153m      \"'\"      \u001b[0m | LogProb:  -1.3271  Prob:  0.2651  | AvgNLL: 0.3895  PPL_Prob: 0.6774\n",
      "[90] Token: \u001b[48;2;76;170;153m      's'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.3852  PPL_Prob: 0.6803\n",
      "[91] Token: \u001b[48;2;76;170;153m     ' a'      \u001b[0m | LogProb:  -0.6250  Prob:  0.5352  | AvgNLL: 0.3878  PPL_Prob: 0.6785\n",
      "[92] Token: \u001b[48;2;76;170;153m' metaphorical'\u001b[0m | LogProb:  -1.9092  Prob:  0.1482  | AvgNLL: 0.4042  PPL_Prob: 0.6675\n",
      "[93] Token: \u001b[48;2;76;170;153m ' statement'  \u001b[0m | LogProb:  -0.2710  Prob:  0.7627  | AvgNLL: 0.4027  PPL_Prob: 0.6685\n",
      "[94] Token: \u001b[48;2;76;170;153m   ' about'    \u001b[0m | LogProb:  -0.8022  Prob:  0.4482  | AvgNLL: 0.4070  PPL_Prob: 0.6657\n",
      "[95] Token: \u001b[48;2;76;170;153m    ' how'     \u001b[0m | LogProb:  -0.7246  Prob:  0.4846  | AvgNLL: 0.4103  PPL_Prob: 0.6635\n",
      "[96] Token: \u001b[48;2;76;170;153m     ' we'     \u001b[0m | LogProb:  -0.2832  Prob:  0.7534  | AvgNLL: 0.4090  PPL_Prob: 0.6643\n",
      "[97] Token: \u001b[48;2;76;170;153m  ' perceive'  \u001b[0m | LogProb:  -1.3535  Prob:  0.2583  | AvgNLL: 0.4186  PPL_Prob: 0.6580\n",
      "[98] Token: \u001b[48;2;76;170;153m ' ourselves'  \u001b[0m | LogProb:  -0.2739  Prob:  0.7603  | AvgNLL: 0.4171  PPL_Prob: 0.6589\n",
      "[99] Token: \u001b[48;2;76;170;153m     ' in'     \u001b[0m | LogProb:  -0.8320  Prob:  0.4351  | AvgNLL: 0.4213  PPL_Prob: 0.6562\n",
      "\n",
      "== Self Perplexity ==\n",
      "Avg NLL: 0.4213\n",
      "Perplexity (self): 0.6562\n",
      "Input: In a dark room with a candle, a lantern, and a match, which do you light first?\n",
      "\n",
      "[00] Token: \u001b[48;2;0;143;230m    '\\n\\n'     \u001b[0m | LogProb:  -0.0982  Prob:  0.9062  | AvgNLL: 0.0982  PPL_Prob: 0.9065\n",
      "[01] Token: \u001b[48;2;50;161;178m    'This'     \u001b[0m | LogProb:  -0.5073  Prob:  0.6021  | AvgNLL: 0.3028  PPL_Prob: 0.7388\n",
      "[02] Token: \u001b[48;2;25;152;204m     ' is'     \u001b[0m | LogProb:  -0.0054  Prob:  0.9946  | AvgNLL: 0.2036  PPL_Prob: 0.8157\n",
      "[03] Token: \u001b[48;2;25;152;204m     ' a'      \u001b[0m | LogProb:  -0.0071  Prob:  0.9927  | AvgNLL: 0.1545  PPL_Prob: 0.8568\n",
      "[04] Token: \u001b[48;2;25;152;204m  ' classic'   \u001b[0m | LogProb:  -0.0404  Prob:  0.9604  | AvgNLL: 0.1317  PPL_Prob: 0.8766\n",
      "[05] Token: \u001b[48;2;25;152;204m   ' riddle'   \u001b[0m | LogProb:  -0.0031  Prob:  0.9971  | AvgNLL: 0.1103  PPL_Prob: 0.8956\n",
      "[06] Token: \u001b[48;2;25;152;204m      '!'      \u001b[0m | LogProb:  -0.3235  Prob:  0.7236  | AvgNLL: 0.1407  PPL_Prob: 0.8687\n",
      "[07] Token: \u001b[48;2;25;152;204m    ' The'     \u001b[0m | LogProb:  -0.0371  Prob:  0.9634  | AvgNLL: 0.1278  PPL_Prob: 0.8801\n",
      "[08] Token: \u001b[48;2;25;152;204m   ' answer'   \u001b[0m | LogProb:  -0.0005  Prob:  0.9995  | AvgNLL: 0.1136  PPL_Prob: 0.8926\n",
      "[09] Token: \u001b[48;2;0;143;230m     ' is'     \u001b[0m | LogProb:  -0.0004  Prob:  0.9995  | AvgNLL: 0.1023  PPL_Prob: 0.9028\n",
      "[10] Token: \u001b[48;2;0;143;230m    ' the'     \u001b[0m | LogProb:  -0.0111  Prob:  0.9888  | AvgNLL: 0.0940  PPL_Prob: 0.9103\n",
      "[11] Token: \u001b[48;2;0;143;230m   ' match'    \u001b[0m | LogProb:  -0.0022  Prob:  0.9976  | AvgNLL: 0.0864  PPL_Prob: 0.9173\n",
      "[12] Token: \u001b[48;2;0;143;230m      '.'      \u001b[0m | LogProb:  -0.0010  Prob:  0.9990  | AvgNLL: 0.0798  PPL_Prob: 0.9233\n",
      "[13] Token: \u001b[48;2;25;152;204m    '\\n\\n'     \u001b[0m | LogProb:  -0.6919  Prob:  0.5005  | AvgNLL: 0.1235  PPL_Prob: 0.8838\n",
      "[14] Token: \u001b[48;2;25;152;204m     'Let'     \u001b[0m | LogProb:  -1.1309  Prob:  0.3228  | AvgNLL: 0.1907  PPL_Prob: 0.8264\n",
      "[15] Token: \u001b[48;2;25;152;204m     ' me'     \u001b[0m | LogProb:  -0.0402  Prob:  0.9604  | AvgNLL: 0.1813  PPL_Prob: 0.8342\n",
      "[16] Token: \u001b[48;2;25;152;204m    ' know'    \u001b[0m | LogProb:  -0.0007  Prob:  0.9995  | AvgNLL: 0.1706  PPL_Prob: 0.8431\n",
      "[17] Token: \u001b[48;2;25;152;204m     ' if'     \u001b[0m | LogProb:  -0.0050  Prob:  0.9951  | AvgNLL: 0.1614  PPL_Prob: 0.8509\n",
      "[18] Token: \u001b[48;2;25;152;204m    ' you'     \u001b[0m | LogProb:  -0.0006  Prob:  0.9995  | AvgNLL: 0.1530  PPL_Prob: 0.8582\n",
      "[19] Token: \u001b[48;2;25;152;204m      \"'\"      \u001b[0m | LogProb:  -0.0999  Prob:  0.9048  | AvgNLL: 0.1503  PPL_Prob: 0.8604\n",
      "[20] Token: \u001b[48;2;25;152;204m      'd'      \u001b[0m | LogProb:  -0.0000  Prob:  1.0000  | AvgNLL: 0.1432  PPL_Prob: 0.8666\n",
      "[21] Token: \u001b[48;2;25;152;204m    ' like'    \u001b[0m | LogProb:  -0.0003  Prob:  0.9995  | AvgNLL: 0.1367  PPL_Prob: 0.8723\n",
      "[22] Token: \u001b[48;2;25;152;204m     ' to'     \u001b[0m | LogProb:  -0.0370  Prob:  0.9639  | AvgNLL: 0.1323  PPL_Prob: 0.8760\n",
      "[23] Token: \u001b[48;2;25;152;204m    ' try'     \u001b[0m | LogProb:  -0.0022  Prob:  0.9980  | AvgNLL: 0.1269  PPL_Prob: 0.8808\n",
      "[24] Token: \u001b[48;2;25;152;204m  ' another'   \u001b[0m | LogProb:  -0.0003  Prob:  0.9995  | AvgNLL: 0.1218  PPL_Prob: 0.8853\n",
      "[25] Token: \u001b[48;2;25;152;204m   ' riddle'   \u001b[0m | LogProb:  -0.0657  Prob:  0.9365  | AvgNLL: 0.1197  PPL_Prob: 0.8872\n",
      "[26] Token: \u001b[48;2;25;152;204m      '!'      \u001b[0m | LogProb:  -0.0012  Prob:  0.9990  | AvgNLL: 0.1153  PPL_Prob: 0.8911\n",
      "[27] Token: \u001b[48;2;25;152;204m'<end_of_turn>'\u001b[0m | LogProb:  -0.0420  Prob:  0.9590  | AvgNLL: 0.1127  PPL_Prob: 0.8934\n",
      "\n",
      "== Self Perplexity ==\n",
      "Avg NLL: 0.1127\n",
      "Perplexity (self): 0.8934\n"
     ]
    }
   ],
   "source": [
    "questions = {\n",
    "    \"Easy\": [\n",
    "        \"Is the Moon Earth's satellite?\",\n",
    "        \"What is 3 + 4?\",\n",
    "        \"Is water H2O?\",\n",
    "        \"Is the Sun a star?\",\n",
    "        \"Are cats mammals?\"\n",
    "    ],\n",
    "    \"Tricky\": [\n",
    "        \"If you say “I am lying,” are you telling the truth?\",\n",
    "        # \"How many months have 28 days?\",\n",
    "        \"If you are your brother's sibling, who are you?\",\n",
    "        # \"If a clock is one hour fast after one hour, what time is it now?\",\n",
    "        \"In a dark room with a candle, a lantern, and a match, which do you light first?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for difficulty, qs in questions.items():\n",
    "    print(f\"\\n--- {difficulty} Questions ---\\n\")\n",
    "    results[difficulty] = []\n",
    "    for q in qs:\n",
    "        history = generate_with_self_confidence(tokenizer, model, \n",
    "                                                q, \n",
    "                                                max_new_tokens=100, \n",
    "                                                field_width=15)\n",
    "        # results[difficulty].append((q, history))\n",
    "        # print(f\"Q: {q}\\nA: {history}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40a0d407-6e71-4814-9460-b0bbc85af7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Is the Moon Earth's satellite?\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;25;152;204m   '\\n\\n' (0.8677)    \u001b[0m | \u001b[48;2;229;226;0m    '\\n' (0.0900)     \u001b[0m | \u001b[48;2;229;226;0m     ' ' (0.0100)     \u001b[0m | \u001b[48;2;229;226;0m   ' The' (0.0039)    \u001b[0m | \u001b[48;2;229;226;0m    ' Or' (0.0023)    \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;178;207;51m    'The' (0.2291)    \u001b[0m | \u001b[48;2;203;216;25m    '**' (0.1959)     \u001b[0m | \u001b[48;2;203;216;25m    'No' (0.1870)     \u001b[0m | \u001b[48;2;203;216;25m    'Yes' (0.1625)    \u001b[0m | \u001b[48;2;229;226;0m   'This' (0.0421)    \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;127;189;102m  ' answer' (0.4441)  \u001b[0m | \u001b[48;2;152;198;76m   ' Moon' (0.3682)   \u001b[0m | \u001b[48;2;229;226;0m  ' short' (0.0461)   \u001b[0m | \u001b[48;2;229;226;0m  ' Earth' (0.0245)   \u001b[0m | \u001b[48;2;229;226;0m   ' moon' (0.0161)   \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;0;143;230m    ' is' (0.9761)    \u001b[0m | \u001b[48;2;229;226;0m    ' to' (0.0206)    \u001b[0m | \u001b[48;2;229;226;0m   ' isn' (0.0013)    \u001b[0m | \u001b[48;2;229;226;0m     ',' (0.0006)     \u001b[0m | \u001b[48;2;229;226;0m     ':' (0.0004)     \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;76;170;153m    ' a' (0.6055)     \u001b[0m | \u001b[48;2;203;216;25m   ' yes' (0.1555)    \u001b[0m | \u001b[48;2;229;226;0m     ':' (0.0758)     \u001b[0m | \u001b[48;2;229;226;0m    ' no' (0.0659)    \u001b[0m | \u001b[48;2;229;226;0m     ',' (0.0202)     \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;0;143;230m' resounding' (0.9521)\u001b[0m | \u001b[48;2;229;226;0m' definitive' (0.0191)\u001b[0m | \u001b[48;2;229;226;0m ' definite' (0.0064) \u001b[0m | \u001b[48;2;229;226;0m  ' clear' (0.0041)   \u001b[0m | \u001b[48;2;229;226;0m   ' bit' (0.0040)    \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;25;152;204m    ' **' (0.8262)    \u001b[0m | \u001b[48;2;229;226;0m   ' yes' (0.0473)    \u001b[0m | \u001b[48;2;229;226;0m    ' no' (0.0266)    \u001b[0m | \u001b[48;2;229;226;0m   ' YES' (0.0253)    \u001b[0m | \u001b[48;2;229;226;0m    ' *' (0.0220)     \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;101;179;127m    'no' (0.5669)     \u001b[0m | \u001b[48;2;127;189;102m    'yes' (0.4277)    \u001b[0m | \u001b[48;2;229;226;0m    'No' (0.0019)     \u001b[0m | \u001b[48;2;229;226;0m    'Yes' (0.0011)    \u001b[0m | \u001b[48;2;229;226;0m    'YES' (0.0006)    \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;0;143;230m    '**.' (0.9546)    \u001b[0m | \u001b[48;2;229;226;0m    '**' (0.0159)     \u001b[0m | \u001b[48;2;229;226;0m    '.**' (0.0136)    \u001b[0m | \u001b[48;2;229;226;0m     '!' (0.0074)     \u001b[0m | \u001b[48;2;229;226;0m    '**,' (0.0052)    \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;76;170;153m   '\\n\\n' (0.6743)    \u001b[0m | \u001b[48;2;178;207;51m   ' The' (0.2993)    \u001b[0m | \u001b[48;2;229;226;0m     ' ' (0.0073)     \u001b[0m | \u001b[48;2;229;226;0m    '  ' (0.0073)     \u001b[0m | \u001b[48;2;229;226;0m  ' Earth' (0.0037)   \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;76;170;153m    'The' (0.6079)    \u001b[0m | \u001b[48;2;152;198;76m   'Here' (0.3462)    \u001b[0m | \u001b[48;2;229;226;0m     '*' (0.0134)     \u001b[0m | \u001b[48;2;229;226;0m   'While' (0.0115)   \u001b[0m | \u001b[48;2;229;226;0m   'Earth' (0.0030)   \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;0;143;230m   ' Moon' (0.9976)   \u001b[0m | \u001b[48;2;229;226;0m   ' moon' (0.0010)   \u001b[0m | \u001b[48;2;229;226;0m  ' Earth' (0.0007)   \u001b[0m | \u001b[48;2;229;226;0m   ' Sun' (0.0004)    \u001b[0m | \u001b[48;2;229;226;0m ' current' (0.0000)  \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;0;143;230m    ' is' (0.9824)    \u001b[0m | \u001b[48;2;229;226;0m  ' orbits' (0.0109)  \u001b[0m | \u001b[48;2;229;226;0m     ',' (0.0021)     \u001b[0m | \u001b[48;2;229;226;0m   ' isn' (0.0016)    \u001b[0m | \u001b[48;2;229;226;0m    ' *' (0.0004)     \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;0;143;230m    ' a' (0.9932)     \u001b[0m | \u001b[48;2;229;226;0m  ' Earth' (0.0033)   \u001b[0m | \u001b[48;2;229;226;0m   ' not' (0.0013)    \u001b[0m | \u001b[48;2;229;226;0m   ' the' (0.0003)    \u001b[0m | \u001b[48;2;229;226;0m ' actually' (0.0003) \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;0;143;230m ' natural' (0.9805)  \u001b[0m | \u001b[48;2;229;226;0m' satellite' (0.0112) \u001b[0m | \u001b[48;2;229;226;0m' naturally' (0.0080) \u001b[0m | \u001b[48;2;229;226;0m    ' *' (0.0001)     \u001b[0m | \u001b[48;2;229;226;0m ' Natural' (0.0000)  \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;0;143;230m' satellite' (1.0000) \u001b[0m | \u001b[48;2;229;226;0m' celestial' (0.0000) \u001b[0m | \u001b[48;2;229;226;0m  ' object' (0.0000)  \u001b[0m | \u001b[48;2;229;226;0m     ',' (0.0000)     \u001b[0m | \u001b[48;2;229;226;0m   ' body' (0.0000)   \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;25;152;204m    ' of' (0.8926)    \u001b[0m | \u001b[48;2;229;226;0m   ' that' (0.0667)   \u001b[0m | \u001b[48;2;229;226;0m     ',' (0.0156)     \u001b[0m | \u001b[48;2;229;226;0m ' orbiting' (0.0144) \u001b[0m | \u001b[48;2;229;226;0m     '.' (0.0039)     \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;0;143;230m  ' Earth' (0.9351)   \u001b[0m | \u001b[48;2;229;226;0m   ' the' (0.0646)    \u001b[0m | \u001b[48;2;229;226;0m    ' *' (0.0001)     \u001b[0m | \u001b[48;2;229;226;0m    ' **' (0.0001)    \u001b[0m | \u001b[48;2;229;226;0m   ' our' (0.0000)    \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;76;170;153m     '.' (0.6133)     \u001b[0m | \u001b[48;2;152;198;76m     ',' (0.3550)     \u001b[0m | \u001b[48;2;229;226;0m   ' and' (0.0265)    \u001b[0m | \u001b[48;2;229;226;0m   ' that' (0.0020)   \u001b[0m | \u001b[48;2;229;226;0m    ' –' (0.0010)     \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;50;161;178m    ' It' (0.7744)    \u001b[0m | \u001b[48;2;203;216;25m   '\\n\\n' (0.1099)    \u001b[0m | \u001b[48;2;229;226;0m    '  ' (0.0310)     \u001b[0m | \u001b[48;2;229;226;0m  ' Earth' (0.0300)   \u001b[0m | \u001b[48;2;229;226;0m   ' Here' (0.0099)   \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;50;161;178m  ' orbits' (0.7759)  \u001b[0m | \u001b[48;2;203;216;25m  ' doesn' (0.1266)   \u001b[0m | \u001b[48;2;229;226;0m     \"'\" (0.0438)     \u001b[0m | \u001b[48;2;229;226;0m  ' formed' (0.0120)  \u001b[0m | \u001b[48;2;229;226;0m    ' is' (0.0116)    \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;101;179;127m   ' our' (0.5117)    \u001b[0m | \u001b[48;2;152;198;76m   ' the' (0.3057)    \u001b[0m | \u001b[48;2;229;226;0m  ' Earth' (0.0917)   \u001b[0m | \u001b[48;2;229;226;0m  ' around' (0.0862)  \u001b[0m | \u001b[48;2;229;226;0m   ' and' (0.0011)    \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;0;143;230m  ' planet' (1.0000)  \u001b[0m | \u001b[48;2;229;226;0m' celestial' (0.0000) \u001b[0m | \u001b[48;2;229;226;0m   ' home' (0.0000)   \u001b[0m | \u001b[48;2;229;226;0m  ' Planet' (0.0000)  \u001b[0m | \u001b[48;2;229;226;0m   '<pad>' (0.0000)   \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;25;152;204m     ',' (0.8047)     \u001b[0m | \u001b[48;2;229;226;0m   ' and' (0.0809)    \u001b[0m | \u001b[48;2;229;226;0m  ' every' (0.0630)   \u001b[0m | \u001b[48;2;229;226;0m     '.' (0.0259)     \u001b[0m | \u001b[48;2;229;226;0m  ' around' (0.0128)  \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;25;152;204m   ' and' (0.8848)    \u001b[0m | \u001b[48;2;229;226;0m   ' but' (0.0225)    \u001b[0m | \u001b[48;2;229;226;0m' constantly' (0.0088)\u001b[0m | \u001b[48;2;229;226;0m ' creating' (0.0078) \u001b[0m | \u001b[48;2;229;226;0m' providing' (0.0072) \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;127;189;102m    ' it' (0.4028)    \u001b[0m | \u001b[48;2;203;216;25m    ' is' (0.1602)    \u001b[0m | \u001b[48;2;203;216;25m    ' we' (0.1459)    \u001b[0m | \u001b[48;2;203;216;25m   ' its' (0.1328)    \u001b[0m | \u001b[48;2;229;226;0m   ' the' (0.0386)    \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;76;170;153m     \"'\" (0.6074)     \u001b[0m | \u001b[48;2;203;216;25m     '’' (0.1159)     \u001b[0m | \u001b[48;2;203;216;25m    ' is' (0.1072)    \u001b[0m | \u001b[48;2;229;226;0m   ' was' (0.0547)    \u001b[0m | \u001b[48;2;229;226;0m  ' doesn' (0.0440)   \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;0;143;230m     's' (1.0000)     \u001b[0m | \u001b[48;2;229;226;0m    'll' (0.0000)     \u001b[0m | \u001b[48;2;229;226;0m     't' (0.0000)     \u001b[0m | \u001b[48;2;229;226;0m    ' is' (0.0000)    \u001b[0m | \u001b[48;2;229;226;0m    ' s' (0.0000)     \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;127;189;102m    ' a' (0.4639)     \u001b[0m | \u001b[48;2;203;216;25m' constantly' (0.1136)\u001b[0m | \u001b[48;2;229;226;0m   ' the' (0.0831)    \u001b[0m | \u001b[48;2;229;226;0m ' captured' (0.0769) \u001b[0m | \u001b[48;2;229;226;0m   ' held' (0.0580)   \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;203;216;25m' fascinating' (0.1998)\u001b[0m | \u001b[48;2;203;216;25m   ' key' (0.1395)    \u001b[0m | \u001b[48;2;203;216;25m' relatively' (0.1005)\u001b[0m | \u001b[48;2;229;226;0m ' captured' (0.0929) \u001b[0m | \u001b[48;2;229;226;0m  ' large' (0.0581)   \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;178;207;51m  ' object' (0.2820)  \u001b[0m | \u001b[48;2;178;207;51m   ' and' (0.2776)    \u001b[0m | \u001b[48;2;229;226;0m  ' place' (0.0860)   \u001b[0m | \u001b[48;2;229;226;0m   ' part' (0.0564)   \u001b[0m | \u001b[48;2;229;226;0m ' feature' (0.0446)  \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;127;189;102m   ' that' (0.4902)   \u001b[0m | \u001b[48;2;178;207;51m     ',' (0.2927)     \u001b[0m | \u001b[48;2;229;226;0m   ' with' (0.0559)   \u001b[0m | \u001b[48;2;229;226;0m    ' in' (0.0541)    \u001b[0m | \u001b[48;2;229;226;0m    ' to' (0.0229)    \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;101;179;127m   ' has' (0.5830)    \u001b[0m | \u001b[48;2;203;216;25m     \"'\" (0.1475)     \u001b[0m | \u001b[48;2;229;226;0m    ' we' (0.0718)    \u001b[0m | \u001b[48;2;229;226;0m  ' plays' (0.0644)   \u001b[0m | \u001b[48;2;229;226;0m     '’' (0.0281)     \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;127;189;102m ' captured' (0.4070) \u001b[0m | \u001b[48;2;178;207;51m   ' been' (0.2754)   \u001b[0m | \u001b[48;2;203;216;25m' captivated' (0.1952)\u001b[0m | \u001b[48;2;229;226;0m  ' shaped' (0.0435)  \u001b[0m | \u001b[48;2;229;226;0m  ' played' (0.0319)  \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;152;198;76m   ' the' (0.3755)    \u001b[0m | \u001b[48;2;178;207;51m ' humanity' (0.2140) \u001b[0m | \u001b[48;2;203;216;25m  ' human' (0.1774)   \u001b[0m | \u001b[48;2;203;216;25m   ' our' (0.1565)    \u001b[0m | \u001b[48;2;229;226;0m  ' humans' (0.0567)  \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;101;179;127m' imagination' (0.5894)\u001b[0m | \u001b[48;2;152;198;76m' imaginations' (0.3154)\u001b[0m | \u001b[48;2;229;226;0m  ' human' (0.0903)   \u001b[0m | \u001b[48;2;229;226;0m' attention' (0.0032) \u001b[0m | \u001b[48;2;229;226;0m  ' minds' (0.0008)   \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;0;143;230m    ' of' (0.9165)    \u001b[0m | \u001b[48;2;229;226;0m   ' for' (0.0814)    \u001b[0m | \u001b[48;2;229;226;0m   ' and' (0.0017)    \u001b[0m | \u001b[48;2;229;226;0m  ' since' (0.0002)   \u001b[0m | \u001b[48;2;229;226;0m' throughout' (0.0000)\u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;101;179;127m  ' people' (0.5532)  \u001b[0m | \u001b[48;2;152;198;76m  ' humans' (0.3572)  \u001b[0m | \u001b[48;2;229;226;0m ' humanity' (0.0848) \u001b[0m | \u001b[48;2;229;226;0m' humankind' (0.0026) \u001b[0m | \u001b[48;2;229;226;0m' civilizations' (0.0004)\u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;0;143;230m   ' for' (0.9839)    \u001b[0m | \u001b[48;2;229;226;0m  ' around' (0.0104)  \u001b[0m | \u001b[48;2;229;226;0m' worldwide' (0.0027) \u001b[0m | \u001b[48;2;229;226;0m  ' across' (0.0022)  \u001b[0m | \u001b[48;2;229;226;0m  ' since' (0.0004)   \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;127;189;102m' centuries' (0.4790) \u001b[0m | \u001b[48;2;127;189;102m' millennia' (0.4790) \u001b[0m | \u001b[48;2;229;226;0m' thousands' (0.0381) \u001b[0m | \u001b[48;2;229;226;0m' countless' (0.0014) \u001b[0m | \u001b[48;2;229;226;0m ' millions' (0.0010) \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;0;143;230m     '.' (0.9761)     \u001b[0m | \u001b[48;2;229;226;0m     '!' (0.0199)     \u001b[0m | \u001b[48;2;229;226;0m     ',' (0.0030)     \u001b[0m | \u001b[48;2;229;226;0m   ' due' (0.0001)    \u001b[0m | \u001b[48;2;229;226;0m   ' and' (0.0001)    \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;50;161;178m   '\\n\\n' (0.7959)    \u001b[0m | \u001b[48;2;229;226;0m ' However' (0.0595)  \u001b[0m | \u001b[48;2;229;226;0m     ' ' (0.0422)     \u001b[0m | \u001b[48;2;229;226;0m    ' It' (0.0226)    \u001b[0m | \u001b[48;2;229;226;0m    '\\n' (0.0206)     \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;25;152;204m   'Here' (0.8271)    \u001b[0m | \u001b[48;2;229;226;0m  'However' (0.0459)  \u001b[0m | \u001b[48;2;229;226;0m    'The' (0.0210)    \u001b[0m | \u001b[48;2;229;226;0m    'Let' (0.0204)    \u001b[0m | \u001b[48;2;229;226;0m   'Earth' (0.0180)   \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;25;152;204m     \"'\" (0.8945)     \u001b[0m | \u001b[48;2;229;226;0m     '’' (0.0928)     \u001b[0m | \u001b[48;2;229;226;0m   ' are' (0.0086)    \u001b[0m | \u001b[48;2;229;226;0m    ' is' (0.0039)    \u001b[0m | \u001b[48;2;229;226;0m   ' some' (0.0001)   \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;0;143;230m     's' (1.0000)     \u001b[0m | \u001b[48;2;229;226;0m    're' (0.0000)     \u001b[0m | \u001b[48;2;229;226;0m   ' are' (0.0000)    \u001b[0m | \u001b[48;2;229;226;0m    ' is' (0.0000)    \u001b[0m | \u001b[48;2;229;226;0m    ' s' (0.0000)     \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;0;143;230m    ' a' (0.9282)     \u001b[0m | \u001b[48;2;229;226;0m   ' why' (0.0515)    \u001b[0m | \u001b[48;2;229;226;0m   ' some' (0.0155)   \u001b[0m | \u001b[48;2;229;226;0m   ' the' (0.0021)    \u001b[0m | \u001b[48;2;229;226;0m   ' more' (0.0012)   \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;50;161;178m' breakdown' (0.7378) \u001b[0m | \u001b[48;2;203;216;25m  ' quick' (0.1168)   \u001b[0m | \u001b[48;2;229;226;0m   ' more' (0.0596)   \u001b[0m | \u001b[48;2;229;226;0m  ' little' (0.0286)  \u001b[0m | \u001b[48;2;229;226;0m   ' bit' (0.0252)    \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;50;161;178m    ' of' (0.7544)    \u001b[0m | \u001b[48;2;178;207;51m     ':' (0.2449)     \u001b[0m | \u001b[48;2;229;226;0m    ' to' (0.0004)    \u001b[0m | \u001b[48;2;229;226;0m   ' why' (0.0002)    \u001b[0m | \u001b[48;2;229;226;0m   ' for' (0.0000)    \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;0;143;230m   ' why' (0.9282)    \u001b[0m | \u001b[48;2;229;226;0m   ' the' (0.0558)    \u001b[0m | \u001b[48;2;229;226;0m   ' what' (0.0071)   \u001b[0m | \u001b[48;2;229;226;0m   ' key' (0.0029)    \u001b[0m | \u001b[48;2;229;226;0m   ' how' (0.0022)    \u001b[0m\n",
      "\n",
      "== Top-5 Tokens ==\n",
      "\u001b[48;2;76;170;153m    ' it' (0.6533)    \u001b[0m | \u001b[48;2;152;198;76m   ' the' (0.3040)    \u001b[0m | \u001b[48;2;229;226;0m   ' this' (0.0098)   \u001b[0m | \u001b[48;2;229;226;0m     ':' (0.0098)     \u001b[0m | \u001b[48;2;229;226;0m    ' we' (0.0079)    \u001b[0m\n",
      "\n",
      "== Final Output ==\n",
      "Is the Moon Earth's satellite?\n",
      "\n",
      "The answer is a resounding **no**.\n",
      "\n",
      "The Moon is a natural satellite of Earth. It orbits our planet, and it's a fascinating object that has captured the imagination of people for centuries.\n",
      "\n",
      "Here's a breakdown of why it\n"
     ]
    }
   ],
   "source": [
    "run_inference_loop(tokenizer, model, \n",
    "                   \"Is the Moon Earth's satellite?\", \n",
    "                   max_new_tokens=50\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a672c5-1ba8-43b7-8e20-4b6aca205110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
