{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85995f60-1839-4667-ac9e-d8c7d5ee3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19bf1bc0-cc18-4e59-9677-9ac0acdf582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "530c6411-4583-4cf8-94a0-3791fe7711b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_id=\"Qwen/Qwen3-0.6B\", dtype=torch.float16):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=dtype\n",
    "    )\n",
    "    model.eval()\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13c0c08d-9b7e-49eb-9361-ddd3911270f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_model()\n",
    "model.tokenizer = tokenizer  # decode에 사용하기 위해 tokenizer 주입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "837da4c7-d664-40a9-b3e7-0b6978c87139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_ansi(r, g, b):\n",
    "    return f\"\\033[38;2;{r};{g};{b}m\"\n",
    "\n",
    "def bg_rgb_ansi(r, g, b):\n",
    "    return f\"\\033[48;2;{r};{g};{b}m\"  # 배경색\n",
    "\n",
    "def color_by_prob(text, prob):\n",
    "    \"\"\"\n",
    "    확률(prob) 값에 따라 색상을 입힌 문자열 반환\n",
    "    높은 확률일수록 초록색, 낮을수록 빨간색 계열\n",
    "    \"\"\"\n",
    "    if prob >= 0.5:\n",
    "        color = \"\\033[92m\"  # 밝은 초록 (high prob)\n",
    "    elif prob >= 0.2:\n",
    "        color = \"\\033[93m\"  # 밝은 노랑 (medium prob)\n",
    "    else:\n",
    "        color = \"\\033[91m\"  # 밝은 빨강 (low prob)\n",
    "\n",
    "    reset = \"\\033[0m\"\n",
    "    return f\"{color}{text}{reset}\"\n",
    "\n",
    "def get_yellow_to_green_ansi(prob):\n",
    "    \"\"\"\n",
    "    확률에 따라 진한 노랑(255,255,0) → 초록(0,255,0) 색상 매핑\n",
    "    \"\"\"\n",
    "    steps = 10\n",
    "    index = min(int(prob * steps), steps - 1)\n",
    "\n",
    "    r_start, g, b = 255, 255, 0  # 시작 색 (노랑)\n",
    "    r_end = 0                   # 끝 색 (초록)\n",
    "    \n",
    "    # R을 선형적으로 감소\n",
    "    r = int(r_start - (r_start - r_end) * (index / (steps - 1)))\n",
    "    \n",
    "    return rgb_ansi(r, g, b)\n",
    "\n",
    "def get_yellow_to_green_bg_ansi(prob):\n",
    "    steps = 10\n",
    "    index = min(int(prob * steps), steps - 1)\n",
    "    r_start, g, b = 255, 255, 0\n",
    "    r = int(r_start - (r_start * index / (steps - 1)))\n",
    "    return bg_rgb_ansi(r, g, b)\n",
    "\n",
    "def get_yellow_to_green_bg_ansi(prob):\n",
    "    steps = 10\n",
    "    index = min(int(prob * steps), steps - 1)\n",
    "    r_start, g, b = 255, 255, 0\n",
    "    r = int(r_start - (r_start * index / (steps - 1)))\n",
    "    return bg_rgb_ansi(r, g, b)\n",
    "\n",
    "def get_yellow_to_brightblue_bg_ansi(prob):\n",
    "    \"\"\"\n",
    "    prob: 0.0(노랑) → 1.0(밝은 파랑) 배경색 10단계 반환\n",
    "    밝은 파랑: #6699FF (R=102,G=153,B=255)\n",
    "    노랑: #FFFF00 (R=255,G=255,B=0)\n",
    "    \"\"\"\n",
    "    steps = 10\n",
    "    index = min(int(prob * steps), steps - 1)\n",
    "    \n",
    "    r_start, g_start, b_start = 255, 255, 0       # 노랑\n",
    "    r_end, g_end, b_end = 102, 153, 255           # 밝은 파랑\n",
    "    \n",
    "    r = int(r_start + (r_end - r_start) * index / (steps - 1))\n",
    "    g = int(g_start + (g_end - g_start) * index / (steps - 1))\n",
    "    b = int(b_start + (b_end - b_start) * index / (steps - 1))\n",
    "    \n",
    "    return bg_rgb_ansi(r, g, b)\n",
    "\n",
    "def get_yellow_to_lightblue_bg_ansi(prob):\n",
    "    \"\"\"\n",
    "    prob: 0.0 (#E5E200 노랑) → 1.0 (#008FE6 파랑) 배경색 10단계 선형 보간\n",
    "    \"\"\"\n",
    "    steps = 10\n",
    "    index = min(int(prob * steps), steps - 1)\n",
    "\n",
    "    r_start, g_start, b_start = 229, 226, 0     # #E5E200 노랑\n",
    "    r_end, g_end, b_end = 0, 143, 230           # #008FE6 파랑\n",
    "\n",
    "    r = int(r_start + (r_end - r_start) * index / (steps - 1))\n",
    "    g = int(g_start + (g_end - g_start) * index / (steps - 1))\n",
    "    b = int(b_start + (b_end - b_start) * index / (steps - 1))\n",
    "\n",
    "    return bg_rgb_ansi(r, g, b)\n",
    "\n",
    "\n",
    "def bg_rgb_ansi(r, g, b):\n",
    "    return f\"\\033[48;2;{r};{g};{b}m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e99b176-8288-400f-b82d-f70789f36d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_tokens(top_tokens, field_width=20):\n",
    "    \"\"\"Top-5 토큰 정보를 가운데 정렬 + 배경색 노랑→초록, 글자색 기본 유지\"\"\"\n",
    "\n",
    "    formatted = []\n",
    "    for _, token_str, prob in top_tokens:\n",
    "        token_display = f\"{repr(token_str)} ({prob:.4f})\"\n",
    "        padded = f\"{token_display:^{field_width}}\"   # 먼저 순수 텍스트로 정렬\n",
    "        bg_color = get_yellow_to_lightblue_bg_ansi(prob) # 외부 정의 함수 호출\n",
    "        colored = f\"{bg_color}{padded}\\033[0m\"       # 색상 코드 감싸기\n",
    "        formatted.append(colored)\n",
    "\n",
    "    print(\"== Top-5 Tokens ==\")\n",
    "    print(\" | \".join(formatted))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50203565-f3f4-4dbc-9941-bd4679102e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token(model, input_ids):\n",
    "    \"\"\"다음 토큰과 top-5 후보 반환\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    next_token_logits = logits[0, -1, :]\n",
    "    probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "    # Top-5 토큰 추출\n",
    "    top_probs, top_ids = torch.topk(probs, k=5)\n",
    "    top_tokens = [(token_id.item(), token_str, top_probs[i].item())\n",
    "                  for i, token_id in enumerate(top_ids)\n",
    "                  for token_str in [model.tokenizer.decode(token_id)]]\n",
    "\n",
    "    return top_tokens\n",
    "\n",
    "def run_inference_loop(tokenizer, model, input_text, max_new_tokens=20):\n",
    "    \"\"\"전체 생성 루프 실행\"\"\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    generated = input_ids\n",
    "\n",
    "    print(f\"Input: {input_text}\\n\")\n",
    "\n",
    "    for step in range(max_new_tokens):\n",
    "        top_tokens = generate_next_token(model, generated)\n",
    "        print_top_tokens(top_tokens)\n",
    "\n",
    "        # 다음 토큰 선택 (top-1 기반, greedy)\n",
    "        next_token_id = torch.tensor([[top_tokens[0][0]]], device=model.device)\n",
    "        generated = torch.cat([generated, next_token_id], dim=1)\n",
    "\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    output_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    print(\"== Final Output ==\")\n",
    "    print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8727721-5b42-4eb5-ac17-ded8ee6791a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_from_logits(tokenizer, model, input_text):\n",
    "    \"\"\"logits를 사용하여 perplexity 직접 계산\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "        input_ids = inputs.input_ids.to(model.device)\n",
    "\n",
    "        # 모델 예측 logits\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        # 정답 label은 input_ids를 한 칸 오른쪽으로 이동\n",
    "        shift_logits = logits[:, :-1, :]         # [batch, seq-1, vocab]\n",
    "        shift_labels = input_ids[:, 1:]          # [batch, seq-1]\n",
    "\n",
    "        # CrossEntropy 계산: log_softmax 후 정답 토큰의 log prob 추출\n",
    "        log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "        shift_labels = shift_labels.unsqueeze(-1)  # [batch, seq-1, 1]\n",
    "\n",
    "        # gather로 정답 토큰의 log prob 추출\n",
    "        token_log_probs = log_probs.gather(dim=-1, index=shift_labels).squeeze(-1)  # [batch, seq-1]\n",
    "\n",
    "        # 평균 negative log likelihood\n",
    "        nll = -token_log_probs.mean()\n",
    "        perplexity = torch.exp(nll)\n",
    "\n",
    "        print(f\"NLL: {nll.item():.4f}\")\n",
    "        print(f\"Perplexity: {perplexity.item():.4f}\")\n",
    "        return perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c4db778d-4fe2-4726-8e9d-3b902ea5f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_inference_loop(tokenizer, model, \"What is 2 + 2? Answer briefly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54674bf9-d390-44cc-ba10-b67bbfa97bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_self_confidence(tokenizer, model, input_text, max_new_tokens=20, field_width=20):\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    generated = input_ids\n",
    "    log_probs = []\n",
    "\n",
    "    print(f\"Input: {input_text}\\n\")\n",
    "\n",
    "    for step in range(max_new_tokens):\n",
    "        outputs = model(generated)\n",
    "        logits = outputs.logits[:, -1, :]  # 마지막 토큰의 logits\n",
    "\n",
    "        log_softmax = F.log_softmax(logits, dim=-1)\n",
    "        next_token_id = torch.argmax(log_softmax, dim=-1)\n",
    "        next_log_prob = log_softmax[0, next_token_id]\n",
    "        prob = next_log_prob.exp().item()\n",
    "\n",
    "        log_probs.append(next_log_prob.item())\n",
    "        avg_nll = -sum(log_probs) / len(log_probs)\n",
    "        avg_prob = math.exp(-avg_nll)\n",
    "\n",
    "        # 토큰 문자열 (repr로 이스케이프 처리)\n",
    "        token_str = repr(tokenizer.decode([next_token_id.item()]))\n",
    "\n",
    "        # 배경색 코드 생성 (PPL_Prob 즉 avg_prob 기준)\n",
    "        bg_color = get_yellow_to_lightblue_bg_ansi(avg_prob)\n",
    "\n",
    "        # 토큰 문자열에 색상과 정렬 적용\n",
    "        padded = f\"{token_str:^{field_width}}\"\n",
    "        colored_token = f\"{bg_color}{padded}\\033[0m\"\n",
    "\n",
    "        print(\n",
    "            f\"[{step:02d}] Token: {colored_token} | \"\n",
    "            f\"LogProb: {next_log_prob.item():>8.4f}  Prob: {prob:>7.4f}  | \"\n",
    "            f\"AvgNLL: {avg_nll:.4f}  PPL_Prob: {avg_prob:.4f}\"\n",
    "        )\n",
    "\n",
    "        generated = torch.cat([generated, next_token_id.unsqueeze(0)], dim=1)\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    print(\"\\n== Self Perplexity ==\")\n",
    "    print(f\"Avg NLL: {avg_nll:.4f}\")\n",
    "    print(f\"Perplexity (self): {avg_prob:.4f}\")\n",
    "\n",
    "    return avg_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d9f86ef7-3942-4d1c-b51c-0b38724942bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: What is 2 + 2? Answer briefly.\n",
      "\n",
      "[00] Token: \u001b[48;2;178;207;51m     ' '      \u001b[0m | LogProb:  -1.5977  Prob:  0.2024  | AvgNLL: 1.5977  PPL_Prob: 0.2024\n",
      "[01] Token: \u001b[48;2;152;198;76m     '2'      \u001b[0m | LogProb:  -0.3132  Prob:  0.7310  | AvgNLL: 0.9554  PPL_Prob: 0.3846\n",
      "[02] Token: \u001b[48;2;127;189;102m     ' +'     \u001b[0m | LogProb:  -0.2515  Prob:  0.7778  | AvgNLL: 0.7208  PPL_Prob: 0.4864\n",
      "[03] Token: \u001b[48;2;101;179;127m     ' '      \u001b[0m | LogProb:  -0.0073  Prob:  0.9927  | AvgNLL: 0.5424  PPL_Prob: 0.5814\n",
      "[04] Token: \u001b[48;2;76;170;153m     '2'      \u001b[0m | LogProb:  -0.0457  Prob:  0.9551  | AvgNLL: 0.4431  PPL_Prob: 0.6421\n",
      "[05] Token: \u001b[48;2;101;179;127m    ' is'     \u001b[0m | LogProb:  -0.9570  Prob:  0.3840  | AvgNLL: 0.5287  PPL_Prob: 0.5894\n",
      "[06] Token: \u001b[48;2;76;170;153m     ' '      \u001b[0m | LogProb:  -0.1348  Prob:  0.8740  | AvgNLL: 0.4724  PPL_Prob: 0.6235\n",
      "[07] Token: \u001b[48;2;76;170;153m     '4'      \u001b[0m | LogProb:  -0.0252  Prob:  0.9751  | AvgNLL: 0.4165  PPL_Prob: 0.6593\n",
      "[08] Token: \u001b[48;2;76;170;153m   '.\\n\\n'    \u001b[0m | LogProb:  -0.8896  Prob:  0.4109  | AvgNLL: 0.4691  PPL_Prob: 0.6256\n",
      "[09] Token: \u001b[48;2;101;179;127m     '**'     \u001b[0m | LogProb:  -1.9297  Prob:  0.1451  | AvgNLL: 0.6152  PPL_Prob: 0.5406\n",
      "[10] Token: \u001b[48;2;101;179;127m  'Question'  \u001b[0m | LogProb:  -1.3789  Prob:  0.2520  | AvgNLL: 0.6846  PPL_Prob: 0.5043\n",
      "[11] Token: \u001b[48;2;101;179;127m    '**\\n'    \u001b[0m | LogProb:  -0.5205  Prob:  0.5942  | AvgNLL: 0.6709  PPL_Prob: 0.5112\n",
      "[12] Token: \u001b[48;2;101;179;127m    'What'    \u001b[0m | LogProb:  -0.6787  Prob:  0.5073  | AvgNLL: 0.6715  PPL_Prob: 0.5109\n",
      "[13] Token: \u001b[48;2;101;179;127m    ' is'     \u001b[0m | LogProb:  -0.1539  Prob:  0.8574  | AvgNLL: 0.6345  PPL_Prob: 0.5302\n",
      "[14] Token: \u001b[48;2;101;179;127m     ' '      \u001b[0m | LogProb:  -0.3198  Prob:  0.7261  | AvgNLL: 0.6136  PPL_Prob: 0.5414\n",
      "[15] Token: \u001b[48;2;101;179;127m     '2'      \u001b[0m | LogProb:  -0.5024  Prob:  0.6050  | AvgNLL: 0.6066  PPL_Prob: 0.5452\n",
      "[16] Token: \u001b[48;2;101;179;127m     ' +'     \u001b[0m | LogProb:  -0.1947  Prob:  0.8232  | AvgNLL: 0.5824  PPL_Prob: 0.5586\n",
      "[17] Token: \u001b[48;2;101;179;127m     ' '      \u001b[0m | LogProb:  -0.0108  Prob:  0.9893  | AvgNLL: 0.5506  PPL_Prob: 0.5766\n",
      "[18] Token: \u001b[48;2;101;179;127m     '2'      \u001b[0m | LogProb:  -0.1311  Prob:  0.8770  | AvgNLL: 0.5286  PPL_Prob: 0.5895\n",
      "[19] Token: \u001b[48;2;101;179;127m     ' +'     \u001b[0m | LogProb:  -0.3838  Prob:  0.6812  | AvgNLL: 0.5213  PPL_Prob: 0.5937\n",
      "\n",
      "== Self Perplexity ==\n",
      "Avg NLL: 0.5213\n",
      "Perplexity (self): 0.5937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5937385863244323"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_with_self_confidence(tokenizer, model, \"What is 2 + 2? Answer briefly.\", \n",
    "                              max_new_tokens=20,\n",
    "                              field_width=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47ccaec4-bc19-4bd1-9bb3-14ee2198b03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: What is 4 + 4? Answer briefly.\n",
      "\n",
      "[00] Token: \u001b[48;2;178;207;51m     ' '      \u001b[0m | LogProb:  -1.3672  Prob:  0.2549  | AvgNLL: 1.3672  PPL_Prob: 0.2548\n",
      "[01] Token: \u001b[48;2;127;189;102m     '4'      \u001b[0m | LogProb:  -0.3164  Prob:  0.7290  | AvgNLL: 0.8418  PPL_Prob: 0.4309\n",
      "[02] Token: \u001b[48;2;101;179;127m     ' +'     \u001b[0m | LogProb:  -0.2198  Prob:  0.8027  | AvgNLL: 0.6345  PPL_Prob: 0.5302\n",
      "[03] Token: \u001b[48;2;76;170;153m     ' '      \u001b[0m | LogProb:  -0.0069  Prob:  0.9932  | AvgNLL: 0.4776  PPL_Prob: 0.6203\n",
      "[04] Token: \u001b[48;2;76;170;153m     '4'      \u001b[0m | LogProb:  -0.0273  Prob:  0.9731  | AvgNLL: 0.3875  PPL_Prob: 0.6787\n",
      "[05] Token: \u001b[48;2;76;170;153m    ' is'     \u001b[0m | LogProb:  -0.6938  Prob:  0.4998  | AvgNLL: 0.4386  PPL_Prob: 0.6449\n",
      "[06] Token: \u001b[48;2;76;170;153m     ' '      \u001b[0m | LogProb:  -0.1329  Prob:  0.8755  | AvgNLL: 0.3949  PPL_Prob: 0.6737\n",
      "[07] Token: \u001b[48;2;50;161;178m     '8'      \u001b[0m | LogProb:  -0.0122  Prob:  0.9878  | AvgNLL: 0.3471  PPL_Prob: 0.7067\n",
      "[08] Token: \u001b[48;2;76;170;153m   '.\\n\\n'    \u001b[0m | LogProb:  -0.8340  Prob:  0.4343  | AvgNLL: 0.4012  PPL_Prob: 0.6695\n",
      "[09] Token: \u001b[48;2;101;179;127m     '**'     \u001b[0m | LogProb:  -1.5771  Prob:  0.2065  | AvgNLL: 0.5188  PPL_Prob: 0.5952\n",
      "[10] Token: \u001b[48;2;101;179;127m  'Question'  \u001b[0m | LogProb:  -1.7207  Prob:  0.1790  | AvgNLL: 0.6280  PPL_Prob: 0.5336\n",
      "[11] Token: \u001b[48;2;101;179;127m    '**\\n'    \u001b[0m | LogProb:  -0.5767  Prob:  0.5620  | AvgNLL: 0.6238  PPL_Prob: 0.5359\n",
      "[12] Token: \u001b[48;2;101;179;127m    'What'    \u001b[0m | LogProb:  -0.6318  Prob:  0.5317  | AvgNLL: 0.6244  PPL_Prob: 0.5356\n",
      "[13] Token: \u001b[48;2;101;179;127m    ' is'     \u001b[0m | LogProb:  -0.1345  Prob:  0.8740  | AvgNLL: 0.5894  PPL_Prob: 0.5547\n",
      "[14] Token: \u001b[48;2;101;179;127m     ' '      \u001b[0m | LogProb:  -0.2998  Prob:  0.7407  | AvgNLL: 0.5701  PPL_Prob: 0.5655\n",
      "[15] Token: \u001b[48;2;101;179;127m     '4'      \u001b[0m | LogProb:  -0.6367  Prob:  0.5288  | AvgNLL: 0.5743  PPL_Prob: 0.5631\n",
      "[16] Token: \u001b[48;2;101;179;127m     ' +'     \u001b[0m | LogProb:  -0.1578  Prob:  0.8540  | AvgNLL: 0.5498  PPL_Prob: 0.5771\n",
      "[17] Token: \u001b[48;2;101;179;127m     ' '      \u001b[0m | LogProb:  -0.0064  Prob:  0.9937  | AvgNLL: 0.5196  PPL_Prob: 0.5948\n",
      "[18] Token: \u001b[48;2;76;170;153m     '4'      \u001b[0m | LogProb:  -0.1542  Prob:  0.8569  | AvgNLL: 0.5003  PPL_Prob: 0.6063\n",
      "[19] Token: \u001b[48;2;76;170;153m     ' +'     \u001b[0m | LogProb:  -0.6367  Prob:  0.5288  | AvgNLL: 0.5072  PPL_Prob: 0.6022\n",
      "\n",
      "== Self Perplexity ==\n",
      "Avg NLL: 0.5072\n",
      "Perplexity (self): 0.6022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6022058247076545"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_with_self_confidence(tokenizer, model, \"What is 4 + 4? Answer briefly.\", \n",
    "                              max_new_tokens=20,\n",
    "                              field_width=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d283f86-a8bd-4fb0-ae09-a74336e2c041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
